from fastapi import FastAPI, APIRouter, HTTPException, BackgroundTasks, Depends, Request
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from starlette.middleware.sessions import SessionMiddleware
from dotenv import load_dotenv
from starlette.middleware.cors import CORSMiddleware
from motor.motor_asyncio import AsyncIOMotorClient
import os
import logging
from pathlib import Path
from pydantic import BaseModel, Field, EmailStr
from typing import List, Optional, Dict, Any
import uuid
import hashlib
import secrets
from datetime import datetime, timedelta
import asyncio
import httpx
import json
import base64
from io import BytesIO
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.utils import PlotlyJSONEncoder
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import letter, A4
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
from reportlab.lib import colors
import openpyxl
from openpyxl.styles import Font, Alignment, PatternFill

# Phase 4: Stripe Integration
from emergentintegrations.payments.stripe.checkout import StripeCheckout, CheckoutSessionResponse, CheckoutStatusResponse, CheckoutSessionRequest

# Phase 4: OAuth Integration
from authlib.integrations.starlette_client import OAuth
from authlib.jose import jwt
import httpx

ROOT_DIR = Path(__file__).parent
load_dotenv(ROOT_DIR / '.env')

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# MongoDB connection
client = AsyncIOMotorClient(os.environ.get('MONGO_URL', 'mongodb://localhost:27017'))
db = client[os.environ.get('DB_NAME', 'pharma_intelligence')]

# Create the main app without a prefix
app = FastAPI()

# Add session middleware for OAuth
app.add_middleware(SessionMiddleware, secret_key=os.environ.get('SESSION_SECRET_KEY', secrets.token_urlsafe(32)))

# Create a router with the /api prefix
api_router = APIRouter(prefix="/api")

# Phase 4: OAuth Configuration
oauth = OAuth()

# Configure Google OAuth
oauth.register(
    name='google',
    client_id=os.environ.get('GOOGLE_CLIENT_ID'),
    client_secret=os.environ.get('GOOGLE_CLIENT_SECRET'),
    server_metadata_url='https://accounts.google.com/.well-known/openid-configuration',
    client_kwargs={
        'scope': 'openid email profile'
    }
)

# Configure Apple OAuth (will need client credentials)
oauth.register(
    name='apple',
    client_id=os.environ.get('APPLE_CLIENT_ID'),
    client_secret=os.environ.get('APPLE_CLIENT_SECRET'),
    authorize_url='https://appleid.apple.com/auth/authorize',
    access_token_url='https://appleid.apple.com/auth/token',
    client_kwargs={
        'scope': 'name email'
    }
)

# Define Models
class StatusCheck(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    client_name: str
    timestamp: datetime = Field(default_factory=datetime.utcnow)

class StatusCheckCreate(BaseModel):
    client_name: str

class FinancialModelRequest(BaseModel):
    therapy_area: str
    product_name: Optional[str] = None
    analysis_id: str
    discount_rate: float = 0.12
    peak_sales_estimate: float = 1000  # millions
    patent_expiry_year: int = 2035
    launch_year: int = 2025
    ramp_up_years: int = 5
    monte_carlo_iterations: int = 1000
    api_key: str

class FinancialModel(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    therapy_area: str
    product_name: Optional[str]
    analysis_id: str
    npv_analysis: Dict[str, Any]
    irr_analysis: Dict[str, Any]
    monte_carlo_results: Dict[str, Any]
    peak_sales_distribution: Dict[str, Any]
    sensitivity_analysis: Dict[str, Any]
    financial_projections: List[Dict[str, Any]]
    risk_metrics: Dict[str, Any]
    visualization_data: Dict[str, Any]
    assumptions: Dict[str, Any]
    created_at: datetime = Field(default_factory=datetime.utcnow)

class TimelineRequest(BaseModel):
    therapy_area: str
    product_name: Optional[str] = None
    analysis_id: str
    include_competitive_milestones: bool = True
    api_key: str

class Timeline(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    therapy_area: str
    product_name: Optional[str]
    analysis_id: str
    milestones: List[Dict[str, Any]]
    competitive_milestones: List[Dict[str, Any]]
    regulatory_timeline: Dict[str, Any]
    visualization_data: Dict[str, Any]
    created_at: datetime = Field(default_factory=datetime.utcnow)

class TemplateRequest(BaseModel):
    template_type: str  # "therapy_specific", "regulatory", "kol_interview"
    therapy_area: Optional[str] = None
    region: Optional[str] = None
    api_key: str

class CustomTemplate(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    template_type: str
    therapy_area: Optional[str]
    region: Optional[str]
    template_data: Dict[str, Any]
    sections: List[Dict[str, Any]]
    customization_options: Dict[str, Any]
    created_at: datetime = Field(default_factory=datetime.utcnow)

class EnsembleAnalysisRequest(BaseModel):
    therapy_area: str
    product_name: Optional[str] = None
    analysis_type: str = "comprehensive"  # "competitive", "forecasting", "comprehensive"
    claude_api_key: str
    perplexity_api_key: str
    gemini_api_key: Optional[str] = None
    use_gemini: bool = False
    confidence_threshold: float = 0.7

class EnsembleResult(BaseModel):
    therapy_area: str
    product_name: Optional[str]
    claude_analysis: Dict[str, Any]
    perplexity_intelligence: Dict[str, Any]
    gemini_analysis: Optional[Dict[str, Any]] = None
    ensemble_synthesis: str
    confidence_scores: Dict[str, float]
    consensus_insights: List[str]
    conflicting_points: List[str]
    recommendation: str
    model_agreement_score: float
    sources: List[str]
    timestamp: datetime = Field(default_factory=datetime.utcnow)

class CompanyIntelRequest(BaseModel):
    product_name: str
    therapy_area: Optional[str] = None
    api_key: str
    include_competitors: bool = True

class CompanyIntelligence(BaseModel):
    product_name: str
    parent_company: str
    company_website: str
    market_class: str
    investor_data: Dict[str, Any]
    press_releases: List[Dict[str, str]]
    competitive_products: List[Dict[str, Any]]
    financial_metrics: Dict[str, Any]
    recent_developments: List[Dict[str, str]]
    sources_scraped: List[str]
    timestamp: datetime = Field(default_factory=datetime.utcnow)

class PerplexityRequest(BaseModel):
    query: str
    api_key: str
    search_focus: Optional[str] = "pharmaceutical"

class PerplexityResult(BaseModel):
    content: str
    citations: List[str]
    search_query: str
    timestamp: datetime = Field(default_factory=datetime.utcnow)

class TherapyAreaRequest(BaseModel):
    therapy_area: str
    product_name: Optional[str] = None
    api_key: str

class PatientFlowFunnelRequest(BaseModel):
    therapy_area: str
    analysis_id: str
    api_key: str

class CompetitiveAnalysisRequest(BaseModel):
    therapy_area: str
    analysis_id: str
    api_key: str

class ScenarioModelingRequest(BaseModel):
    therapy_area: str
    analysis_id: str
    scenarios: List[str] = ["optimistic", "realistic", "pessimistic"]
    api_key: str

class ExportRequest(BaseModel):
    analysis_id: str
    export_type: str  # "pdf", "excel", "pptx"

class TherapyAreaAnalysis(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    therapy_area: str
    product_name: Optional[str] = None
    disease_summary: str
    staging: str
    biomarkers: str
    treatment_algorithm: str
    patient_journey: str
    market_size_data: Optional[Dict[str, Any]] = None
    competitive_landscape: Optional[Dict[str, Any]] = None
    regulatory_intelligence: Optional[Dict[str, Any]] = None
    clinical_trials_data: Optional[List[Dict[str, Any]]] = None
    risk_assessment: Optional[Dict[str, Any]] = None
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)

class PatientFlowFunnel(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    therapy_area: str
    analysis_id: str
    funnel_stages: List[dict]
    total_addressable_population: str
    forecasting_notes: str
    scenario_models: Optional[Dict[str, Any]] = None
    visualization_data: Optional[Dict[str, Any]] = None
    created_at: datetime = Field(default_factory=datetime.utcnow)

class ResearchResult(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    query: str
    source: str
    results: Dict[str, Any]
    cached_at: datetime = Field(default_factory=datetime.utcnow)

# Phase 3 Models: Real-World Evidence Integration & Market Access Intelligence
class RWERequest(BaseModel):
    therapy_area: str
    product_name: Optional[str] = None
    analysis_type: str = "comprehensive"  # "effectiveness", "safety", "outcomes", "comprehensive"
    data_sources: List[str] = ["registries", "claims", "ehr", "patient_outcomes"]
    api_key: str

class RealWorldEvidence(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    therapy_area: str
    product_name: Optional[str]
    effectiveness_data: Dict[str, Any]
    safety_profile: Dict[str, Any]
    patient_outcomes: Dict[str, Any]
    real_world_performance: Dict[str, Any]
    comparative_effectiveness: List[Dict[str, Any]]
    cost_effectiveness: Dict[str, Any]
    adherence_patterns: Dict[str, Any]
    health_economics_data: Dict[str, Any]
    evidence_quality_score: float
    data_sources: List[str]
    study_populations: Dict[str, Any]
    limitations: List[str]
    recommendations: List[str]
    created_at: datetime = Field(default_factory=datetime.utcnow)

class MarketAccessRequest(BaseModel):
    therapy_area: str
    product_name: Optional[str] = None
    target_markets: List[str] = ["US", "EU5", "Japan"]
    analysis_depth: str = "comprehensive"  # "basic", "standard", "comprehensive"
    api_key: str

class MarketAccessIntelligence(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    therapy_area: str
    product_name: Optional[str]
    payer_landscape: Dict[str, Any]
    reimbursement_pathways: Dict[str, Any]
    pricing_analysis: Dict[str, Any]
    access_barriers: List[Dict[str, str]]
    heor_requirements: Dict[str, Any]
    regulatory_pathways: Dict[str, Any]
    approval_timelines: Dict[str, Any]
    formulary_placement: Dict[str, Any]
    budget_impact_models: Dict[str, Any]
    coverage_policies: List[Dict[str, Any]]
    stakeholder_mapping: Dict[str, Any]
    market_readiness_score: float
    recommendations: List[str]
    created_at: datetime = Field(default_factory=datetime.utcnow)

class PredictiveAnalyticsRequest(BaseModel):
    therapy_area: str
    product_name: Optional[str] = None
    forecast_horizon: int = 10  # years
    model_type: str = "ml_enhanced"  # "traditional", "ml_enhanced", "hybrid"
    include_rwe: bool = True
    api_key: str

class PredictiveAnalytics(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    therapy_area: str
    product_name: Optional[str]
    market_penetration_forecast: Dict[str, Any]
    competitive_response_modeling: Dict[str, Any]
    patient_flow_predictions: Dict[str, Any]
    revenue_forecasts: Dict[str, Any]
    risk_adjusted_projections: Dict[str, Any]
    scenario_probabilities: Dict[str, float]
    confidence_intervals: Dict[str, Any]
    key_assumptions: List[str]
    sensitivity_factors: Dict[str, float]
    model_performance_metrics: Dict[str, float]
    uncertainty_analysis: Dict[str, Any]
    recommendations: List[str]
    created_at: datetime = Field(default_factory=datetime.utcnow)

# Phase 4: User Management, Sessions & Payment Models
class UserRegistration(BaseModel):
    email: EmailStr
    password: str
    first_name: str
    last_name: str
    company: Optional[str] = None
    role: Optional[str] = None

class UserLogin(BaseModel):
    email: EmailStr
    password: str

class User(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    email: EmailStr
    first_name: str
    last_name: str
    company: Optional[str] = None
    role: Optional[str] = None
    subscription_tier: str = "free"  # free, basic, professional, enterprise
    subscription_status: str = "active"  # active, inactive, cancelled, expired
    api_usage: Dict[str, int] = Field(default_factory=dict)  # usage tracking
    created_at: datetime = Field(default_factory=datetime.utcnow)
    last_login: Optional[datetime] = None
    is_active: bool = True

class UserSession(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    user_id: str
    session_token: str
    expires_at: datetime
    created_at: datetime = Field(default_factory=datetime.utcnow)
    last_accessed: datetime = Field(default_factory=datetime.utcnow)
    ip_address: Optional[str] = None
    user_agent: Optional[str] = None

class PaymentTransaction(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    user_id: Optional[str] = None
    session_id: str
    stripe_session_id: str
    amount: float
    currency: str = "usd"
    subscription_tier: Optional[str] = None
    payment_status: str = "pending"  # pending, completed, failed, expired, cancelled
    stripe_payment_status: str = "unpaid"
    metadata: Dict[str, Any] = Field(default_factory=dict)
    created_at: datetime = Field(default_factory=datetime.utcnow)
    completed_at: Optional[datetime] = None

class SubscriptionPlan(BaseModel):
    id: str
    name: str
    price: float
    currency: str = "usd"
    stripe_price_id: Optional[str] = None
    features: List[str]
    api_limits: Dict[str, int]  # daily/monthly limits
    description: str
    is_active: bool = True

class AutomatedWorkflow(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    user_id: str
    name: str
    description: Optional[str] = None
    workflow_type: str  # "scheduled_analysis", "alert", "report_generation"
    schedule: Dict[str, Any]  # cron-like schedule
    parameters: Dict[str, Any]  # workflow specific parameters
    is_active: bool = True
    created_at: datetime = Field(default_factory=datetime.utcnow)
    last_run: Optional[datetime] = None
    next_run: Optional[datetime] = None

class WorkflowExecution(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    workflow_id: str
    user_id: str
    status: str  # "running", "completed", "failed"
    started_at: datetime = Field(default_factory=datetime.utcnow)
    completed_at: Optional[datetime] = None
    results: Dict[str, Any] = Field(default_factory=dict)
    error_message: Optional[str] = None

class AlertRule(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    user_id: str
    name: str
    description: Optional[str] = None
    rule_type: str  # "price_change", "competitor_activity", "regulatory_update"
    conditions: Dict[str, Any]
    notification_channels: List[str]  # ["email", "webhook"]
    is_active: bool = True
    created_at: datetime = Field(default_factory=datetime.utcnow)
    last_triggered: Optional[datetime] = None

class AnalysisTemplate(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    user_id: str
    name: str
    description: Optional[str] = None
    template_type: str  # "therapy_analysis", "market_access", "rwe"
    parameters: Dict[str, Any]
    is_public: bool = False
    created_at: datetime = Field(default_factory=datetime.utcnow)
    used_count: int = 0

class UserProfile(BaseModel):
    user_id: str
    preferences: Dict[str, Any] = Field(default_factory=dict)
    notification_settings: Dict[str, bool] = Field(default_factory=dict)
    api_keys: Dict[str, str] = Field(default_factory=dict)  # encrypted
    dashboard_layout: Dict[str, Any] = Field(default_factory=dict)
    recent_analyses: List[str] = Field(default_factory=list)
    favorite_analyses: List[str] = Field(default_factory=list)

# Phase 4: Configuration and Security
# Subscription Plans Configuration
SUBSCRIPTION_PLANS = {
    "basic": {
        "id": "basic",
        "name": "Basic Plan",
        "price": 29.00,
        "stripe_price_id": "price_basic_monthly",  # Replace with actual Stripe price ID
        "features": [
            "Core Therapy Analysis",
            "Patient Flow Funnel",
            "Basic Competitive Intelligence",
            "Export to PDF/Excel",
            "5 Analyses per month"
        ],
        "api_limits": {
            "monthly_analyses": 5,
            "daily_api_calls": 50
        },
        "description": "Perfect for individual researchers and small pharma teams"
    },
    "professional": {
        "id": "professional", 
        "name": "Professional Plan",
        "price": 99.00,
        "stripe_price_id": "price_pro_monthly",  # Replace with actual Stripe price ID
        "features": [
            "Everything in Basic",
            "Real-World Evidence Analysis",
            "Market Access Intelligence", 
            "Predictive Analytics",
            "Multi-Model AI Ensemble",
            "Advanced Financial Modeling",
            "Custom Templates",
            "25 Analyses per month",
            "Priority Support"
        ],
        "api_limits": {
            "monthly_analyses": 25,
            "daily_api_calls": 200
        },
        "description": "For pharma professionals requiring advanced analytics"
    },
    "enterprise": {
        "id": "enterprise",
        "name": "Enterprise Plan", 
        "price": 299.00,
        "stripe_price_id": "price_enterprise_monthly",  # Replace with actual Stripe price ID
        "features": [
            "Everything in Professional",
            "Automated Workflows",
            "Smart Alerts & Monitoring",
            "Portfolio Management",
            "API Access",
            "Custom Integrations",
            "Unlimited Analyses",
            "Priority Support",
            "Dedicated Account Manager"
        ],
        "api_limits": {
            "monthly_analyses": -1,  # Unlimited
            "daily_api_calls": -1    # Unlimited
        },
        "description": "For large pharma companies and enterprise teams"
    }
}

# Security Configuration
security = HTTPBearer()
JWT_SECRET_KEY = os.environ.get('JWT_SECRET_KEY', secrets.token_urlsafe(32))
JWT_ALGORITHM = "HS256"
SESSION_EXPIRE_HOURS = 24

# Password hashing utilities
def hash_password(password: str) -> str:
    """Hash password using SHA-256 with salt"""
    salt = secrets.token_hex(16)
    password_hash = hashlib.sha256((password + salt).encode()).hexdigest()
    return f"{salt}:{password_hash}"

def verify_password(password: str, password_hash: str) -> bool:
    """Verify password against hash"""
    try:
        salt, stored_hash = password_hash.split(':')
        password_hash_check = hashlib.sha256((password + salt).encode()).hexdigest()
        return stored_hash == password_hash_check
    except:
        return False

def generate_session_token() -> str:
    """Generate secure session token"""
    return secrets.token_urlsafe(32)

async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)) -> Optional[User]:
    """Get current user from session token"""
    try:
        session = await db.user_sessions.find_one({
            "session_token": credentials.credentials,
            "expires_at": {"$gt": datetime.utcnow()}
        })
        
        if not session:
            return None
            
        user = await db.users.find_one({"id": session["user_id"]})
        if not user:
            return None
            
        # Update last accessed
        await db.user_sessions.update_one(
            {"id": session["id"]},
            {"$set": {"last_accessed": datetime.utcnow()}}
        )
        
        return User(**user)
    except:
        return None

async def require_subscription(user: User, required_tier: str = "basic") -> bool:
    """Check if user has required subscription tier"""
    tier_hierarchy = {"free": 0, "basic": 1, "professional": 2, "enterprise": 3}
    user_level = tier_hierarchy.get(user.subscription_tier, 0)
    required_level = tier_hierarchy.get(required_tier, 1)
    
    return user_level >= required_level and user.subscription_status == "active"

# Initialize Stripe
stripe_api_key = os.environ.get('STRIPE_API_KEY')
if not stripe_api_key:
    logger.error("STRIPE_API_KEY not found in environment variables")
    raise ValueError("STRIPE_API_KEY is required")

# Utility functions
def create_funnel_chart(funnel_stages):
    """Create a funnel visualization chart"""
    if not funnel_stages:
        return None
        
    stages = [stage['stage'] for stage in funnel_stages]
    # Extract numeric values from percentages
    values = []
    for stage in funnel_stages:
        percentage_str = stage.get('percentage', '100%')
        try:
            # Extract number from percentage string
            numeric_val = float(percentage_str.replace('%', '').strip())
            values.append(numeric_val)
        except:
            values.append(100)  # Default fallback
    
    # Create funnel chart with Plotly
    fig = go.Figure(go.Funnel(
        y=stages,
        x=values,
        textposition="inside",
        texttemplate="%{y}<br>%{x}%",
        textfont=dict(color="white", size=14),
        connector={"line": {"color": "royalblue", "dash": "solid", "width": 3}},
        marker={"color": ["deepskyblue", "lightsalmon", "tan", "teal", "silver", "gold"][:len(stages)],
                "line": {"width": [4, 2, 2, 3, 1, 1][:len(stages)], "color": ["wheat", "wheat", "wheat", "wheat", "wheat", "wheat"][:len(stages)]}}
    ))
    
    fig.update_layout(
        title={
            'text': "Patient Flow Funnel - Treatment Journey",
            'x': 0.5,
            'xanchor': 'center',
            'font': {'size': 18, 'family': 'Arial, sans-serif'}
        },
        font=dict(size=12, family="Arial, sans-serif"),
        showlegend=False,
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)',
        height=500,
        margin=dict(t=80, b=50, l=50, r=50)
    )
    
    return json.dumps(fig, cls=PlotlyJSONEncoder)

def create_market_analysis_chart(competitive_data):
    """Create market share visualization"""
    if not competitive_data or 'competitors' not in competitive_data:
        return None
        
    competitors = competitive_data['competitors'][:10]  # Top 10
    names = [comp.get('name', 'Unknown') for comp in competitors]
    market_shares = [comp.get('market_share', 5) for comp in competitors]
    
    fig = px.pie(
        values=market_shares, 
        names=names, 
        title="Competitive Market Landscape"
    )
    
    return json.dumps(fig, cls=PlotlyJSONEncoder)

def create_scenario_comparison_chart(scenario_models, therapy_area="", product_name=""):
    """Create scenario comparison visualization"""
    if not scenario_models:
        return None
        
    scenarios = list(scenario_models.keys())
    years = list(range(2024, 2030))
    
    fig = go.Figure()
    
    colors = {
        'optimistic': '#10B981',    # Green
        'realistic': '#3B82F6',     # Blue  
        'pessimistic': '#EF4444',   # Red
        'best_case': '#10B981',
        'base_case': '#3B82F6',
        'worst_case': '#EF4444'
    }
    
    for scenario in scenarios:
        scenario_data = scenario_models[scenario]
        if 'projections' in scenario_data and scenario_data['projections']:
            projections = scenario_data['projections'][:6]  # 6 years
            
            # Ensure we have numeric values
            numeric_projections = []
            for proj in projections:
                try:
                    numeric_projections.append(float(proj))
                except:
                    numeric_projections.append(0)
            
            color = colors.get(scenario.lower(), '#6B7280')
            
            fig.add_trace(go.Scatter(
                x=years[:len(numeric_projections)],
                y=numeric_projections,
                mode='lines+markers',
                name=scenario.replace('_', ' ').title(),
                line=dict(color=color, width=3),
                marker=dict(size=8, color=color),
                hovertemplate=f'<b>{scenario.title()}</b><br>' +
                             'Year: %{x}<br>' +
                             'Revenue: $%{y:.0f}M<br>' +
                             '<extra></extra>'
            ))
    
    # Add title with product context
    title_text = f"Market Forecast Scenarios"
    if product_name:
        title_text += f" - {product_name}"
    if therapy_area:
        title_text += f" ({therapy_area})"
    
    fig.update_layout(
        title={
            'text': title_text,
            'x': 0.5,
            'xanchor': 'center',
            'font': {'size': 18, 'family': 'Arial, sans-serif'}
        },
        xaxis_title="Year",
        yaxis_title="Market Value ($ Millions)",
        hovermode='x unified',
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1
        ),
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)',
        height=400,
        margin=dict(t=100, b=50, l=60, r=50),
        font=dict(family="Arial, sans-serif", size=12)
    )
    
    # Style the axes
    fig.update_xaxes(
        showgrid=True, 
        gridwidth=1, 
        gridcolor='rgba(128,128,128,0.2)',
        showline=True,
        linewidth=2,
        linecolor='rgba(128,128,128,0.3)'
    )
    fig.update_yaxes(
        showgrid=True, 
        gridwidth=1, 
        gridcolor='rgba(128,128,128,0.2)',
        showline=True,
        linewidth=2,
        linecolor='rgba(128,128,128,0.3)'
    )
    
    return json.dumps(fig, cls=PlotlyJSONEncoder)

# Custom Templates System
async def generate_custom_template(template_type: str, therapy_area: str, region: str, api_key: str) -> CustomTemplate:
    """Generate custom templates for different use cases"""
    try:
        template_queries = {
            "therapy_specific": f"""
            Create a comprehensive therapy-specific analysis template for {therapy_area}:
            
            1. DISEASE OVERVIEW SECTION
            - Pathophysiology specific to {therapy_area}
            - Key biomarkers and diagnostic criteria
            - Disease staging and classification systems
            - Current standard of care protocols
            
            2. MARKET ANALYSIS FRAMEWORK  
            - Market segmentation approach
            - Key performance indicators (KPIs)
            - Competitive analysis structure
            - Pricing and reimbursement considerations
            
            3. CLINICAL DEVELOPMENT TEMPLATE
            - Phase I/II/III study design considerations
            - Relevant endpoints and biomarkers
            - Regulatory pathway specifics
            - Patient population definitions
            
            4. FORECASTING METHODOLOGY
            - Patient flow modeling approach
            - Market penetration assumptions
            - Scenario planning framework
            - Risk assessment criteria
            
            Provide structured template with customizable sections.
            """,
            
            "regulatory": f"""
            Create regulatory analysis template for {therapy_area} in {region}:
            
            1. REGULATORY PATHWAY ANALYSIS
            - Applicable regulatory guidelines
            - Submission requirements and timelines
            - Key regulatory precedents
            - Orphan drug/breakthrough therapy considerations
            
            2. CLINICAL TRIAL DESIGN REQUIREMENTS
            - Endpoint requirements by region
            - Patient population specifications
            - Comparator selection criteria
            - Post-market surveillance requirements
            
            3. MARKET ACCESS FRAMEWORK
            - HTA requirements by country
            - Pricing and reimbursement pathways
            - Evidence generation requirements
            - Real-world evidence considerations
            
            4. RISK MITIGATION STRATEGIES
            - Regulatory risk assessment
            - Contingency planning
            - Communication strategies
            - Timeline optimization
            
            Focus on {region}-specific requirements and best practices.
            """,
            
            "kol_interview": f"""
            Create KOL interview template for {therapy_area} research:
            
            1. BACKGROUND QUESTIONS
            - Experience with {therapy_area}
            - Current practice patterns
            - Patient population characteristics
            - Treatment decision factors
            
            2. CLINICAL PERSPECTIVES
            - Unmet medical needs assessment
            - Current therapy limitations
            - Ideal product profile definition
            - Clinical endpoint preferences
            
            3. MARKET INSIGHTS
            - Adoption barriers and drivers
            - Competitive positioning views
            - Pricing sensitivity analysis
            - Future treatment landscape
            
            4. PRODUCT-SPECIFIC QUESTIONS
            - Differentiation factors
            - Target patient populations
            - Expected market share
            - Implementation considerations
            
            Include follow-up questions and probing techniques.
            """
        }
        
        # Get template content using AI
        query = template_queries.get(template_type, f"Create analysis template for {therapy_area}")
        result = await search_with_perplexity(query, api_key, f"{template_type}_template")
        
        # Parse template into structured sections
        sections = parse_template_sections(result.content, template_type)
        
        # Create customization options
        customization_options = generate_customization_options(template_type, therapy_area, region)
        
        return CustomTemplate(
            template_type=template_type,
            therapy_area=therapy_area,
            region=region,
            template_data={
                "content": result.content,
                "generated_by": "ai_perplexity",
                "sources": result.citations,
                "word_count": len(result.content.split()),
                "sections_count": len(sections)
            },
            sections=sections,
            customization_options=customization_options
        )
        
    except Exception as e:
        logging.error(f"Template generation error: {str(e)}")
        return CustomTemplate(
            template_type=template_type,
            therapy_area=therapy_area or "Unknown",
            region=region or "Global",
            template_data={"error": str(e)},
            sections=[{"title": "Error", "content": str(e), "type": "error"}],
            customization_options={"error": str(e)}
        )

def parse_template_sections(content: str, template_type: str) -> List[Dict[str, Any]]:
    """Parse template content into structured sections"""
    try:
        sections = []
        lines = content.split('\n')
        current_section = None
        current_content = []
        
        section_markers = ['##', '1.', '2.', '3.', '4.', '5.', 'I.', 'II.', 'III.', 'IV.', 'V.']
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # Check if this is a section header
            is_header = any(line.startswith(marker) for marker in section_markers)
            is_header = is_header or (line.isupper() and len(line.split()) <= 5)
            
            if is_header:
                # Save previous section
                if current_section:
                    sections.append({
                        "title": current_section,
                        "content": '\n'.join(current_content),
                        "type": determine_section_type(current_section, template_type),
                        "customizable": True,
                        "required": determine_section_importance(current_section)
                    })
                
                # Start new section
                current_section = line.replace('#', '').replace('1.', '').replace('2.', '').replace('3.', '').replace('4.', '').replace('5.', '').strip()
                current_content = []
            else:
                current_content.append(line)
        
        # Add last section
        if current_section:
            sections.append({
                "title": current_section,
                "content": '\n'.join(current_content),
                "type": determine_section_type(current_section, template_type),
                "customizable": True,
                "required": determine_section_importance(current_section)
            })
        
        return sections
        
    except Exception as e:
        return [{
            "title": "Template Parsing Error",
            "content": f"Error parsing template: {str(e)}",
            "type": "error",
            "customizable": False,
            "required": True
        }]

def determine_section_type(section_title: str, template_type: str) -> str:
    """Determine section type based on title and template type"""
    title_lower = section_title.lower()
    
    if 'clinical' in title_lower or 'trial' in title_lower:
        return 'clinical'
    elif 'market' in title_lower or 'competitive' in title_lower:
        return 'market'
    elif 'regulatory' in title_lower or 'approval' in title_lower:
        return 'regulatory'
    elif 'financial' in title_lower or 'forecast' in title_lower:
        return 'financial'
    elif 'risk' in title_lower:
        return 'risk'
    else:
        return 'general'

def determine_section_importance(section_title: str) -> bool:
    """Determine if section is required or optional"""
    important_keywords = ['overview', 'analysis', 'framework', 'requirements', 'key', 'main']
    return any(keyword in section_title.lower() for keyword in important_keywords)

def generate_customization_options(template_type: str, therapy_area: str, region: str) -> Dict[str, Any]:
    """Generate customization options for templates"""
    try:
        base_options = {
            "editable_sections": True,
            "section_reordering": True,
            "custom_fields": True,
            "export_formats": ["pdf", "word", "powerpoint"],
            "collaboration": True
        }
        
        if template_type == "therapy_specific":
            base_options.update({
                "biomarker_customization": True,
                "treatment_pathway_editing": True,
                "kpi_selection": True,
                "competitor_focus": True
            })
        elif template_type == "regulatory":
            base_options.update({
                "region_specific_requirements": True,
                "timeline_customization": True,
                "endpoint_selection": True,
                "risk_weighting": True
            })
        elif template_type == "kol_interview":
            base_options.update({
                "question_branching": True,
                "interview_flow_editing": True,
                "specialty_focus": True,
                "follow_up_automation": True
            })
        
        return base_options
        
    except Exception as e:
        return {"error": f"Customization options generation failed: {str(e)}"}

# Advanced 2D Visualizations
def create_competitive_positioning_map(competitive_data: Dict[str, Any]) -> str:
    """Create 2D competitive positioning bubble chart"""
    try:
        competitors = competitive_data.get("competitors", [])
        if not competitors or len(competitors) < 2:
            return json.dumps({"error": "Insufficient competitive data"})
        
        # Extract positioning dimensions (simplified approach)
        x_values = []  # Market share or efficacy
        y_values = []  # Safety or innovation
        sizes = []    # Revenue or market size
        names = []
        
        for i, comp in enumerate(competitors[:10]):  # Limit to 10 competitors
            # Use market share for x-axis (with some randomization for demo)
            market_share = comp.get("market_share", 10 + i * 5)
            if isinstance(market_share, str):
                market_share = float(market_share.replace('%', '')) if '%' in market_share else 10
            x_values.append(market_share)
            
            # Use a derived metric for y-axis (innovation/safety score)
            innovation_score = 50 + (i * 10) % 40  # Simulated score 50-90
            y_values.append(innovation_score)
            
            # Size based on estimated revenue
            estimated_revenue = market_share * 20  # Simplified revenue estimation
            sizes.append(max(10, estimated_revenue))
            
            names.append(comp.get("name", f"Competitor {i+1}"))
        
        fig = go.Figure(data=go.Scatter(
            x=x_values,
            y=y_values,
            mode='markers+text',
            marker=dict(
                size=sizes,
                sizemode='diameter',
                sizeref=2. * max(sizes) / (40.**2),
                sizemin=4,
                color=x_values,
                colorscale='Viridis',
                showscale=True,
                colorbar=dict(title="Market Share (%)")
            ),
            text=names,
            textposition="top center",
            hovertemplate="<b>%{text}</b><br>" +
                         "Market Share: %{x}%<br>" +
                         "Innovation Score: %{y}<br>" +
                         "<extra></extra>"
        ))
        
        fig.update_layout(
            title="Competitive Positioning Map",
            xaxis_title="Market Share (%)",
            yaxis_title="Innovation/Safety Score",
            height=500,
            showlegend=False
        )
        
        return json.dumps(fig, cls=PlotlyJSONEncoder)
        
    except Exception as e:
        return json.dumps({"error": f"Positioning map creation failed: {str(e)}"})

def create_market_evolution_heatmap(therapy_area: str, time_periods: List[str] = None) -> str:
    """Create market evolution heatmap"""
    try:
        if not time_periods:
            time_periods = ["2020", "2021", "2022", "2023", "2024"]
        
        # Simulated market segments and their evolution
        segments = ["Newly Diagnosed", "Relapsed/Refractory", "Maintenance", "Palliative", "Prevention"]
        
        # Generate realistic market size data (in millions)
        np.random.seed(42)
        market_data = []
        for segment in segments:
            segment_data = []
            base_size = np.random.uniform(100, 1000)  # Base market size
            for i, period in enumerate(time_periods):
                # Add growth over time with some variability
                growth_factor = 1 + (i * 0.15) + np.random.uniform(-0.1, 0.1)
                market_size = base_size * growth_factor
                segment_data.append(market_size)
            market_data.append(segment_data)
        
        fig = go.Figure(data=go.Heatmap(
            z=market_data,
            x=time_periods,
            y=segments,
            colorscale='Blues',
            hovertemplate="<b>%{y}</b><br>" +
                         "Year: %{x}<br>" +
                         "Market Size: $%{z:.0f}M<br>" +
                         "<extra></extra>"
        ))
        
        fig.update_layout(
            title=f"Market Evolution Heatmap - {therapy_area}",
            xaxis_title="Year",
            yaxis_title="Market Segments",
            height=400
        )
        
        return json.dumps(fig, cls=PlotlyJSONEncoder)
        
    except Exception as e:
        return json.dumps({"error": f"Heatmap creation failed: {str(e)}"})

def create_risk_return_scatter(financial_data: Dict[str, Any]) -> str:
    """Create risk-return scatter plot for scenario analysis"""
    try:
        scenarios = financial_data.get("scenarios", {})
        if not scenarios:
            return json.dumps({"error": "No scenario data available"})
        
        x_values = []  # Risk (standard deviation)
        y_values = []  # Return (expected NPV)
        names = []
        colors = []
        
        for scenario_name, scenario_data in scenarios.items():
            if isinstance(scenario_data, dict):
                # Extract risk and return metrics
                projections = scenario_data.get("projections", [])
                if projections:
                    expected_return = np.mean(projections)
                    risk = np.std(projections)
                    
                    x_values.append(risk)
                    y_values.append(expected_return)
                    names.append(scenario_name.title())
                    
                    # Color coding
                    if 'optimistic' in scenario_name.lower():
                        colors.append('green')
                    elif 'pessimistic' in scenario_name.lower():
                        colors.append('red')
                    else:
                        colors.append('blue')
        
        if not x_values:
            return json.dumps({"error": "No valid scenario data for risk-return analysis"})
        
        fig = go.Figure(data=go.Scatter(
            x=x_values,
            y=y_values,
            mode='markers+text',
            marker=dict(
                size=15,
                color=colors,
                opacity=0.7
            ),
            text=names,
            textposition="top center",
            hovertemplate="<b>%{text}</b><br>" +
                         "Risk (StdDev): %{x:.1f}<br>" +
                         "Expected Return: $%{y:.0f}M<br>" +
                         "<extra></extra>"
        ))
        
        fig.update_layout(
            title="Risk-Return Analysis by Scenario",
            xaxis_title="Risk (Standard Deviation)",
            yaxis_title="Expected Return ($M)",
            height=400,
            showlegend=False
        )
        
        return json.dumps(fig, cls=PlotlyJSONEncoder)
        
    except Exception as e:
        return json.dumps({"error": f"Risk-return scatter creation failed: {str(e)}"})

# Interactive Timeline Functions
async def generate_timeline(therapy_area: str, product_name: str, analysis_id: str, 
                          include_competitive: bool, api_key: str) -> Timeline:
    """Generate interactive timeline with milestones"""
    try:
        # Get current analysis data
        analysis = await db.therapy_analyses.find_one({"id": analysis_id})
        if not analysis:
            raise Exception("Analysis not found")
        
        # Generate timeline milestones using AI
        timeline_query = f"""
        Create a comprehensive timeline of key milestones for {therapy_area}{f' - {product_name}' if product_name else ''}:
        
        Include:
        1. REGULATORY MILESTONES (FDA submissions, approvals, European approvals)
        2. CLINICAL DEVELOPMENT (Phase I/II/III start dates, data readouts, publications)
        3. COMMERCIAL MILESTONES (launch dates, market expansions, uptake milestones)
        4. COMPETITIVE EVENTS (competitor approvals, new entrants, patent expiries)
        5. MARKET ACCESS (reimbursement decisions, pricing announcements)
        
        Provide realistic dates based on current pharmaceutical development timelines.
        Format as structured data with dates, milestone types, and descriptions.
        """
        
        # Use Perplexity for real-time milestone data
        result = await search_with_perplexity(timeline_query, api_key, "timeline_milestones")
        
        # Parse timeline data
        milestones = parse_timeline_milestones(result.content, product_name or therapy_area)
        
        # Get competitive milestones if requested
        competitive_milestones = []
        if include_competitive:
            competitive_query = f"""
            Key competitive milestones and events in {therapy_area} over the next 5 years:
            - Competitor product launches and approvals
            - Patent expiries and generic entries  
            - New clinical trial initiations
            - Partnership announcements
            - Market access decisions
            """
            
            comp_result = await search_with_perplexity(competitive_query, api_key, "competitive_milestones")
            competitive_milestones = parse_timeline_milestones(comp_result.content, "competitive", is_competitive=True)
        
        # Generate regulatory timeline
        regulatory_timeline = generate_regulatory_timeline(therapy_area, product_name)
        
        # Create visualization data
        visualization_data = create_timeline_visualization(milestones, competitive_milestones)
        
        return Timeline(
            therapy_area=therapy_area,
            product_name=product_name,
            analysis_id=analysis_id,
            milestones=milestones,
            competitive_milestones=competitive_milestones,
            regulatory_timeline=regulatory_timeline,
            visualization_data=visualization_data
        )
        
    except Exception as e:
        logging.error(f"Timeline generation error: {str(e)}")
        return Timeline(
            therapy_area=therapy_area,
            product_name=product_name or "Unknown",
            analysis_id=analysis_id,
            milestones=[{"error": str(e), "date": "2024-01-01", "type": "error"}],
            competitive_milestones=[],
            regulatory_timeline={"error": str(e)},
            visualization_data={"error": str(e)}
        )

def parse_timeline_milestones(content: str, context: str, is_competitive: bool = False) -> List[Dict[str, Any]]:
    """Parse AI-generated content into structured timeline milestones"""
    try:
        milestones = []
        lines = content.split('\n')
        
        current_year = 2024
        milestone_types = {
            'clinical': ['phase', 'trial', 'study', 'data', 'results'],
            'regulatory': ['fda', 'approval', 'submission', 'filing', 'ema'],
            'commercial': ['launch', 'market', 'sales', 'revenue'],
            'competitive': ['competitor', 'generic', 'patent', 'expiry']
        }
        
        for line in lines:
            line = line.strip()
            if not line or len(line) < 10:
                continue
            
            # Extract date patterns
            import re
            date_patterns = [
                r'20[2-3][0-9]',  # Years 2020-2039
                r'[Qq][1-4]\s*20[2-3][0-9]',  # Q1 2024
                r'(January|February|March|April|May|June|July|August|September|October|November|December)\s*20[2-3][0-9]'
            ]
            
            extracted_date = None
            for pattern in date_patterns:
                match = re.search(pattern, line)
                if match:
                    date_str = match.group(0)
                    # Convert to standard format
                    if 'Q' in date_str:
                        year = re.search(r'20[2-3][0-9]', date_str).group(0)
                        quarter = re.search(r'[1-4]', date_str).group(0)
                        month = int(quarter) * 3
                        extracted_date = f"{year}-{month:02d}-01"
                    elif any(month in date_str for month in ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']):
                        # Handle month year format
                        year = re.search(r'20[2-3][0-9]', date_str).group(0)
                        month_names = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']
                        for i, month_name in enumerate(month_names):
                            if month_name in date_str:
                                extracted_date = f"{year}-{i+1:02d}-01"
                                break
                    else:
                        # Just year
                        extracted_date = f"{date_str}-01-01"
                    break
            
            # If no date found, estimate based on content
            if not extracted_date:
                if any(word in line.lower() for word in ['current', 'ongoing', 'now']):
                    extracted_date = "2024-01-01"
                elif any(word in line.lower() for word in ['next', 'upcoming', 'planned']):
                    extracted_date = "2025-01-01"
                else:
                    extracted_date = f"{current_year + len(milestones) // 2}-01-01"
            
            # Determine milestone type
            milestone_type = "general"
            for mtype, keywords in milestone_types.items():
                if any(keyword in line.lower() for keyword in keywords):
                    milestone_type = mtype
                    break
            
            # Extract importance/priority
            priority = "medium"
            if any(word in line.lower() for word in ['major', 'key', 'critical', 'important']):
                priority = "high"
            elif any(word in line.lower() for word in ['minor', 'small', 'routine']):
                priority = "low"
            
            milestones.append({
                "date": extracted_date,
                "title": line[:80] + "..." if len(line) > 80 else line,
                "description": line,
                "type": milestone_type,
                "priority": priority,
                "source": "ai_generated",
                "context": context,
                "is_competitive": is_competitive,
                "confidence": 0.7 if any(word in line.lower() for word in ['approved', 'announced', 'confirmed']) else 0.5
            })
        
        # Sort by date
        milestones.sort(key=lambda x: x["date"])
        
        return milestones[:20]  # Limit to top 20 milestones
        
    except Exception as e:
        return [{
            "date": "2024-01-01",
            "title": f"Timeline parsing error: {str(e)}",
            "description": str(e),
            "type": "error",
            "priority": "high",
            "source": "error"
        }]

def generate_regulatory_timeline(therapy_area: str, product_name: str) -> Dict[str, Any]:
    """Generate standard regulatory timeline"""
    try:
        # Standard pharmaceutical regulatory timeline
        current_year = 2024
        
        regulatory_phases = [
            {"phase": "IND Filing", "duration_months": 0, "description": "Investigational New Drug application"},
            {"phase": "Phase I", "duration_months": 12, "description": "Safety and dosage studies"},
            {"phase": "Phase II", "duration_months": 24, "description": "Efficacy and side effects"},
            {"phase": "Phase III", "duration_months": 36, "description": "Large-scale efficacy studies"},
            {"phase": "NDA/BLA Filing", "duration_months": 6, "description": "New Drug Application submission"},
            {"phase": "FDA Review", "duration_months": 12, "description": "Standard review process"},
            {"phase": "Approval", "duration_months": 0, "description": "FDA approval decision"},
            {"phase": "Launch", "duration_months": 6, "description": "Commercial launch"}
        ]
        
        timeline = []
        cumulative_months = 0
        
        for phase in regulatory_phases:
            start_date = f"{current_year + cumulative_months // 12}-{(cumulative_months % 12) + 1:02d}-01"
            cumulative_months += phase["duration_months"]
            end_date = f"{current_year + cumulative_months // 12}-{(cumulative_months % 12) + 1:02d}-01" if phase["duration_months"] > 0 else start_date
            
            timeline.append({
                "phase": phase["phase"],
                "start_date": start_date,
                "end_date": end_date,
                "duration_months": phase["duration_months"],
                "description": phase["description"],
                "critical_path": True if phase["phase"] in ["Phase III", "NDA/BLA Filing", "FDA Review"] else False
            })
        
        return {
            "timeline": timeline,
            "total_duration_years": cumulative_months / 12,
            "critical_path_phases": [p["phase"] for p in timeline if p.get("critical_path")],
            "estimated_launch_date": timeline[-1]["start_date"],
            "therapy_area": therapy_area,
            "product_name": product_name
        }
        
    except Exception as e:
        return {
            "timeline": [],
            "error": f"Regulatory timeline generation failed: {str(e)}",
            "total_duration_years": 0
        }

def create_timeline_visualization(milestones: List[Dict], competitive_milestones: List[Dict] = None) -> Dict[str, str]:
    """Create timeline visualization charts"""
    try:
        # Combine all milestones for visualization
        all_milestones = milestones.copy()
        if competitive_milestones:
            all_milestones.extend(competitive_milestones)
        
        if not all_milestones:
            return {"error": "No milestones to visualize"}
        
        # Sort by date
        all_milestones.sort(key=lambda x: x["date"])
        
        # Create Gantt-style timeline
        fig = go.Figure()
        
        # Color mapping for milestone types
        colors = {
            "clinical": "#3498db",
            "regulatory": "#e74c3c", 
            "commercial": "#2ecc71",
            "competitive": "#f39c12",
            "general": "#95a5a6"
        }
        
        for i, milestone in enumerate(all_milestones):
            milestone_type = milestone.get("type", "general")
            color = colors.get(milestone_type, "#95a5a6")
            
            # Create timeline bar
            fig.add_trace(go.Scatter(
                x=[milestone["date"], milestone["date"]],
                y=[i, i],
                mode='markers+text',
                marker=dict(
                    size=15,
                    color=color,
                    symbol='diamond' if milestone.get("is_competitive") else 'circle',
                    line=dict(width=2, color='white')
                ),
                text=milestone["title"][:30] + "..." if len(milestone["title"]) > 30 else milestone["title"],
                textposition="middle right",
                name=milestone_type.title(),
                hovertemplate=f"<b>{milestone['title']}</b><br>" +
                             f"Date: {milestone['date']}<br>" +
                             f"Type: {milestone_type}<br>" +
                             f"Priority: {milestone.get('priority', 'medium')}<br>" +
                             "<extra></extra>",
                showlegend=True if i == 0 or milestone_type not in [m.get("type") for m in all_milestones[:i]] else False
            ))
        
        fig.update_layout(
            title="Pharmaceutical Development Timeline",
            xaxis_title="Date",
            yaxis=dict(
                title="Milestones",
                showticklabels=False
            ),
            height=max(400, len(all_milestones) * 25),
            showlegend=True,
            legend=dict(x=1.05, y=1),
            hovermode='closest'
        )
        
        timeline_chart = json.dumps(fig, cls=PlotlyJSONEncoder)
        
        # Create milestone distribution chart
        type_counts = {}
        for milestone in all_milestones:
            mtype = milestone.get("type", "general")
            type_counts[mtype] = type_counts.get(mtype, 0) + 1
        
        fig2 = go.Figure(data=[
            go.Bar(
                x=list(type_counts.keys()),
                y=list(type_counts.values()),
                marker_color=[colors.get(t, "#95a5a6") for t in type_counts.keys()]
            )
        ])
        
        fig2.update_layout(
            title="Milestone Distribution by Type",
            xaxis_title="Milestone Type",
            yaxis_title="Count",
            height=300
        )
        
        distribution_chart = json.dumps(fig2, cls=PlotlyJSONEncoder)
        
        return {
            "timeline_chart": timeline_chart,
            "distribution_chart": distribution_chart,
            "milestone_count": len(all_milestones),
            "competitive_milestone_count": len([m for m in all_milestones if m.get("is_competitive")])
        }
        
    except Exception as e:
        return {"error": f"Timeline visualization creation failed: {str(e)}"}

# Advanced Financial Modeling Functions
import numpy as np
from scipy import stats
import pandas as pd

def calculate_npv(cash_flows: List[float], discount_rate: float) -> Dict[str, Any]:
    """Calculate Net Present Value with detailed analysis"""
    try:
        if not cash_flows or len(cash_flows) == 0:
            return {"npv": 0, "error": "No cash flows provided"}
        
        years = list(range(len(cash_flows)))
        discounted_flows = []
        
        for i, cash_flow in enumerate(cash_flows):
            pv = cash_flow / ((1 + discount_rate) ** i)
            discounted_flows.append(pv)
        
        npv = sum(discounted_flows)
        
        # Additional NPV metrics
        cumulative_pv = np.cumsum(discounted_flows)
        payback_period = None
        
        # Find payback period
        for i, cum_pv in enumerate(cumulative_pv):
            if cum_pv > 0:
                payback_period = i
                break
        
        return {
            "npv": round(npv, 2),
            "discount_rate": discount_rate,
            "cash_flows": cash_flows,
            "discounted_flows": [round(df, 2) for df in discounted_flows],
            "cumulative_pv": [round(cpv, 2) for cpv in cumulative_pv],
            "payback_period": payback_period,
            "total_revenue": sum([cf for cf in cash_flows if cf > 0]),
            "total_investment": abs(sum([cf for cf in cash_flows if cf < 0]))
        }
        
    except Exception as e:
        return {"npv": 0, "error": f"NPV calculation failed: {str(e)}"}

def calculate_irr(cash_flows: List[float]) -> Dict[str, Any]:
    """Calculate Internal Rate of Return using numerical methods"""
    try:
        if not cash_flows or len(cash_flows) < 2:
            return {"irr": 0, "error": "Insufficient cash flows for IRR"}
        
        # Simple IRR calculation using numpy
        def npv_at_rate(rate):
            return sum([cf / ((1 + rate) ** i) for i, cf in enumerate(cash_flows)])
        
        # Binary search for IRR
        low, high = -0.99, 5.0  # Search between -99% and 500%
        tolerance = 1e-6
        max_iterations = 1000
        
        for _ in range(max_iterations):
            mid = (low + high) / 2
            npv_mid = npv_at_rate(mid)
            
            if abs(npv_mid) < tolerance:
                irr = mid
                break
            elif npv_mid > 0:
                low = mid
            else:
                high = mid
        else:
            irr = None
        
        # IRR analysis
        if irr is not None:
            irr_percentage = irr * 100
            
            # IRR sensitivity
            irr_plus_1 = npv_at_rate(irr + 0.01) if irr else 0
            irr_minus_1 = npv_at_rate(irr - 0.01) if irr else 0
            
            return {
                "irr": round(irr_percentage, 2),
                "irr_decimal": round(irr, 4),
                "npv_at_irr": round(npv_at_rate(irr), 2) if irr else 0,
                "sensitivity_plus_1": round(irr_plus_1, 2),
                "sensitivity_minus_1": round(irr_minus_1, 2),
                "interpretation": "High return" if irr_percentage > 20 else "Moderate return" if irr_percentage > 12 else "Low return",
                "cash_flows_count": len(cash_flows)
            }
        else:
            return {
                "irr": None,
                "error": "IRR could not be calculated - no solution found",
                "cash_flows_count": len(cash_flows)
            }
            
    except Exception as e:
        return {"irr": None, "error": f"IRR calculation failed: {str(e)}"}

def run_monte_carlo_simulation(base_peak_sales: float, base_launch_year: int, 
                             iterations: int = 1000) -> Dict[str, Any]:
    """Run Monte Carlo simulation for peak sales and NPV distributions"""
    try:
        np.random.seed(42)  # For reproducible results
        
        results = {
            "peak_sales": [],
            "npv_values": [],
            "launch_delays": [],
            "market_penetrations": []
        }
        
        for i in range(iterations):
            # Random variables with realistic pharmaceutical distributions
            
            # Peak sales uncertainty (log-normal distribution)
            peak_sales_factor = np.random.lognormal(0, 0.3)  # 30% volatility
            simulated_peak_sales = base_peak_sales * peak_sales_factor
            
            # Launch delay (normal distribution, 0-3 years)
            launch_delay = max(0, np.random.normal(0, 1))
            
            # Market penetration rate (beta distribution)
            market_penetration = np.random.beta(2, 3)  # Skewed towards lower penetration
            
            # Generate cash flows for this iteration
            years = 15
            cash_flows = []
            
            # Initial R&D investment (negative)
            cash_flows.append(-500)  # $500M upfront investment
            
            # Revenue ramp-up
            for year in range(1, years):
                if year <= launch_delay:
                    cash_flows.append(-50)  # Continued investment
                else:
                    years_since_launch = year - launch_delay
                    if years_since_launch <= 5:  # Ramp-up period
                        revenue_factor = years_since_launch / 5
                    elif years_since_launch <= 10:  # Peak period
                        revenue_factor = 1.0
                    else:  # Decline period (patent expiry)
                        revenue_factor = max(0.1, 1.0 - (years_since_launch - 10) * 0.2)
                    
                    annual_revenue = simulated_peak_sales * revenue_factor * market_penetration
                    # Subtract costs (assume 70% margin)
                    net_cash_flow = annual_revenue * 0.7
                    cash_flows.append(net_cash_flow)
            
            # Calculate NPV for this iteration
            discount_rate = 0.12
            npv = sum([cf / ((1 + discount_rate) ** i) for i, cf in enumerate(cash_flows)])
            
            # Store results
            results["peak_sales"].append(simulated_peak_sales)
            results["npv_values"].append(npv)
            results["launch_delays"].append(launch_delay)
            results["market_penetrations"].append(market_penetration * 100)
        
        # Calculate statistics
        peak_sales_stats = {
            "mean": np.mean(results["peak_sales"]),
            "median": np.median(results["peak_sales"]),
            "std": np.std(results["peak_sales"]),
            "p10": np.percentile(results["peak_sales"], 10),
            "p90": np.percentile(results["peak_sales"], 90),
            "min": np.min(results["peak_sales"]),
            "max": np.max(results["peak_sales"])
        }
        
        npv_stats = {
            "mean": np.mean(results["npv_values"]),
            "median": np.median(results["npv_values"]),
            "std": np.std(results["npv_values"]),
            "p10": np.percentile(results["npv_values"], 10),
            "p90": np.percentile(results["npv_values"], 90),
            "positive_npv_probability": np.mean([npv > 0 for npv in results["npv_values"]]) * 100
        }
        
        return {
            "iterations": iterations,
            "peak_sales_distribution": peak_sales_stats,
            "npv_distribution": npv_stats,
            "launch_delay_stats": {
                "mean": np.mean(results["launch_delays"]),
                "std": np.std(results["launch_delays"])
            },
            "market_penetration_stats": {
                "mean": np.mean(results["market_penetrations"]),
                "std": np.std(results["market_penetrations"])
            },
            "risk_metrics": {
                "probability_of_success": npv_stats["positive_npv_probability"],
                "downside_risk": npv_stats["p10"],
                "upside_potential": npv_stats["p90"]
            },
            "simulation_data": results  # For visualization
        }
        
    except Exception as e:
        return {
            "iterations": 0,
            "error": f"Monte Carlo simulation failed: {str(e)}",
            "peak_sales_distribution": {},
            "npv_distribution": {},
            "risk_metrics": {}
        }

def perform_sensitivity_analysis(base_params: Dict[str, float]) -> Dict[str, Any]:
    """Perform sensitivity analysis on key parameters"""
    try:
        base_npv = base_params.get("base_npv", 1000)
        
        # Parameters to test
        sensitivity_params = {
            "peak_sales": {"base": 1000, "range": [-30, -20, -10, 0, 10, 20, 30]},
            "discount_rate": {"base": 0.12, "range": [-2, -1, -0.5, 0, 0.5, 1, 2]},
            "market_penetration": {"base": 0.25, "range": [-10, -5, -2.5, 0, 2.5, 5, 10]},
            "launch_delay": {"base": 0, "range": [0, 0.5, 1, 1.5, 2, 2.5, 3]}
        }
        
        sensitivity_results = {}
        
        for param_name, param_data in sensitivity_params.items():
            base_value = param_data["base"]
            param_results = []
            
            for change_pct in param_data["range"]:
                if param_name == "launch_delay":
                    new_value = base_value + change_pct
                else:
                    new_value = base_value * (1 + change_pct / 100)
                
                # Simplified NPV impact calculation
                if param_name == "peak_sales":
                    npv_impact = base_npv * (change_pct / 100) * 0.7  # Revenue impact
                elif param_name == "discount_rate":
                    npv_impact = base_npv * (-change_pct / 100) * 8  # Discount rate impact
                elif param_name == "market_penetration":
                    npv_impact = base_npv * (change_pct / 100) * 0.8
                elif param_name == "launch_delay":
                    npv_impact = base_npv * (-change_pct * 0.15)  # Each year delay
                else:
                    npv_impact = 0
                
                new_npv = base_npv + npv_impact
                
                param_results.append({
                    "parameter_change": change_pct,
                    "new_value": round(new_value, 4),
                    "npv_change": round(npv_impact, 2),
                    "new_npv": round(new_npv, 2),
                    "sensitivity": round(npv_impact / (base_npv * (change_pct / 100)) if change_pct != 0 else 0, 2)
                })
            
            sensitivity_results[param_name] = param_results
        
        # Tornado diagram data (sorted by impact)
        tornado_data = []
        for param_name, results in sensitivity_results.items():
            if len(results) >= 2:
                max_impact = max([abs(r["npv_change"]) for r in results])
                tornado_data.append({
                    "parameter": param_name,
                    "max_impact": max_impact,
                    "positive_impact": max([r["npv_change"] for r in results]),
                    "negative_impact": min([r["npv_change"] for r in results])
                })
        
        tornado_data.sort(key=lambda x: x["max_impact"], reverse=True)
        
        return {
            "sensitivity_results": sensitivity_results,
            "tornado_analysis": tornado_data,
            "key_drivers": [item["parameter"] for item in tornado_data[:3]],
            "analysis_type": "univariate_sensitivity"
        }
        
    except Exception as e:
        return {
            "sensitivity_results": {},
            "error": f"Sensitivity analysis failed: {str(e)}",
            "tornado_analysis": [],
            "key_drivers": []
        }

async def generate_financial_model(therapy_area: str, product_name: str, analysis_id: str, 
                                 params: Dict[str, Any], api_key: str) -> FinancialModel:
    """Generate comprehensive financial model"""
    try:
        # Extract parameters with defaults
        discount_rate = params.get("discount_rate", 0.12)
        peak_sales_estimate = params.get("peak_sales_estimate", 1000)
        launch_year = params.get("launch_year", 2025)
        patent_expiry_year = params.get("patent_expiry_year", 2035)
        ramp_up_years = params.get("ramp_up_years", 5)
        monte_carlo_iterations = params.get("monte_carlo_iterations", 1000)
        
        # Generate cash flow projections
        years = patent_expiry_year - launch_year + 5  # Include post-patent period
        cash_flows = []
        
        # Initial investment
        cash_flows.append(-500)  # $500M R&D investment
        
        # Revenue projections
        for year in range(1, years + 1):
            if year <= ramp_up_years:
                revenue_factor = year / ramp_up_years
            elif year <= (patent_expiry_year - launch_year):
                revenue_factor = 1.0
            else:
                # Post-patent decline
                years_post_patent = year - (patent_expiry_year - launch_year)
                revenue_factor = max(0.1, 1.0 - years_post_patent * 0.3)
            
            annual_revenue = peak_sales_estimate * revenue_factor
            net_cash_flow = annual_revenue * 0.7  # 70% margin
            cash_flows.append(net_cash_flow)
        
        # Perform financial analyses
        npv_analysis = calculate_npv(cash_flows, discount_rate)
        irr_analysis = calculate_irr(cash_flows)
        monte_carlo_results = run_monte_carlo_simulation(
            peak_sales_estimate, launch_year, monte_carlo_iterations
        )
        sensitivity_analysis = perform_sensitivity_analysis({
            "base_npv": npv_analysis.get("npv", 1000)
        })
        
        # Create financial projections
        financial_projections = []
        for i, cash_flow in enumerate(cash_flows):
            year = launch_year + i - 1 if i > 0 else launch_year - 1
            financial_projections.append({
                "year": year,
                "cash_flow": round(cash_flow, 2),
                "cumulative_cash_flow": round(sum(cash_flows[:i+1]), 2),
                "discounted_value": round(cash_flow / ((1 + discount_rate) ** i), 2)
            })
        
        # Generate visualization data
        visualization_data = {
            "cash_flow_chart": create_cash_flow_chart(financial_projections),
            "npv_sensitivity_chart": create_sensitivity_chart(sensitivity_analysis),
            "monte_carlo_chart": create_monte_carlo_chart(monte_carlo_results)
        }
        
        # Compile risk metrics
        risk_metrics = {
            "npv_risk": "High" if npv_analysis.get("npv", 0) < 500 else "Medium" if npv_analysis.get("npv", 0) < 1500 else "Low",
            "irr_vs_hurdle": irr_analysis.get("irr", 0) - (discount_rate * 100),
            "monte_carlo_success_rate": monte_carlo_results.get("risk_metrics", {}).get("probability_of_success", 50),
            "key_risk_factors": sensitivity_analysis.get("key_drivers", []),
            "payback_period": npv_analysis.get("payback_period", "Not calculated")
        }
        
        # Store assumptions
        assumptions = {
            "discount_rate": discount_rate,
            "peak_sales_estimate": peak_sales_estimate,
            "launch_year": launch_year,
            "patent_expiry_year": patent_expiry_year,
            "ramp_up_years": ramp_up_years,
            "profit_margin": 0.7,
            "monte_carlo_iterations": monte_carlo_iterations,
            "post_patent_decline_rate": 0.3
        }
        
        return FinancialModel(
            therapy_area=therapy_area,
            product_name=product_name,
            analysis_id=analysis_id,
            npv_analysis=npv_analysis,
            irr_analysis=irr_analysis,
            monte_carlo_results=monte_carlo_results,
            peak_sales_distribution=monte_carlo_results.get("peak_sales_distribution", {}),
            sensitivity_analysis=sensitivity_analysis,
            financial_projections=financial_projections,
            risk_metrics=risk_metrics,
            visualization_data=visualization_data,
            assumptions=assumptions
        )
        
    except Exception as e:
        logging.error(f"Financial model generation error: {str(e)}")
        return FinancialModel(
            therapy_area=therapy_area,
            product_name=product_name or "Unknown",
            analysis_id=analysis_id,
            npv_analysis={"error": str(e), "npv": 0},
            irr_analysis={"error": str(e), "irr": 0},
            monte_carlo_results={"error": str(e)},
            peak_sales_distribution={"error": str(e)},
            sensitivity_analysis={"error": str(e)},
            financial_projections=[],
            risk_metrics={"error": str(e)},
            visualization_data={"error": str(e)},
            assumptions={"error": str(e)}
        )

def create_cash_flow_chart(financial_projections: List[Dict]) -> str:
    """Create cash flow visualization"""
    try:
        years = [proj["year"] for proj in financial_projections]
        cash_flows = [proj["cash_flow"] for proj in financial_projections]
        cumulative = [proj["cumulative_cash_flow"] for proj in financial_projections]
        
        fig = go.Figure()
        
        # Cash flows bar chart
        fig.add_trace(go.Bar(
            x=years,
            y=cash_flows,
            name="Annual Cash Flow",
            marker_color=['red' if cf < 0 else 'green' for cf in cash_flows]
        ))
        
        # Cumulative line
        fig.add_trace(go.Scatter(
            x=years,
            y=cumulative,
            mode='lines+markers',
            name="Cumulative Cash Flow",
            yaxis='y2',
            line=dict(color='blue', width=3)
        ))
        
        fig.update_layout(
            title="Financial Projections - Cash Flow Analysis",
            xaxis_title="Year",
            yaxis_title="Annual Cash Flow ($M)",
            yaxis2=dict(title="Cumulative Cash Flow ($M)", overlaying='y', side='right'),
            hovermode='x unified',
            height=400
        )
        
        return json.dumps(fig, cls=PlotlyJSONEncoder)
        
    except Exception as e:
        return json.dumps({"error": f"Chart creation failed: {str(e)}"})

def create_sensitivity_chart(sensitivity_data: Dict) -> str:
    """Create tornado chart for sensitivity analysis"""
    try:
        tornado_data = sensitivity_data.get("tornado_analysis", [])
        
        if not tornado_data:
            return json.dumps({"error": "No sensitivity data available"})
        
        parameters = [item["parameter"] for item in tornado_data]
        positive_impacts = [item["positive_impact"] for item in tornado_data]
        negative_impacts = [item["negative_impact"] for item in tornado_data]
        
        fig = go.Figure()
        
        # Negative impacts (left side)
        fig.add_trace(go.Bar(
            y=parameters,
            x=negative_impacts,
            name="Negative Impact",
            orientation='h',
            marker_color='red',
            opacity=0.7
        ))
        
        # Positive impacts (right side)
        fig.add_trace(go.Bar(
            y=parameters,
            x=positive_impacts,
            name="Positive Impact", 
            orientation='h',
            marker_color='green',
            opacity=0.7
        ))
        
        fig.update_layout(
            title="Sensitivity Analysis - Tornado Chart",
            xaxis_title="NPV Impact ($M)",
            yaxis_title="Parameters",
            barmode='overlay',
            height=400
        )
        
        return json.dumps(fig, cls=PlotlyJSONEncoder)
        
    except Exception as e:
        return json.dumps({"error": f"Sensitivity chart creation failed: {str(e)}"})

def create_monte_carlo_chart(monte_carlo_data: Dict) -> str:
    """Create Monte Carlo distribution charts"""
    try:
        simulation_data = monte_carlo_data.get("simulation_data", {})
        npv_values = simulation_data.get("npv_values", [])
        
        if not npv_values:
            return json.dumps({"error": "No Monte Carlo data available"})
        
        fig = go.Figure()
        
        # NPV distribution histogram
        fig.add_trace(go.Histogram(
            x=npv_values,
            nbinsx=50,
            name="NPV Distribution",
            marker_color='blue',
            opacity=0.7
        ))
        
        # Add percentile lines
        p10 = np.percentile(npv_values, 10)
        p90 = np.percentile(npv_values, 90)
        median = np.median(npv_values)
        
        fig.add_vline(x=p10, line_dash="dash", line_color="red", annotation_text="P10")
        fig.add_vline(x=median, line_dash="solid", line_color="green", annotation_text="Median")
        fig.add_vline(x=p90, line_dash="dash", line_color="red", annotation_text="P90")
        
        fig.update_layout(
            title="Monte Carlo Simulation - NPV Distribution",
            xaxis_title="NPV ($M)",
            yaxis_title="Frequency",
            height=400
        )
        
        return json.dumps(fig, cls=PlotlyJSONEncoder)
        
    except Exception as e:
        return json.dumps({"error": f"Monte Carlo chart creation failed: {str(e)}"})

# Multi-Model AI Ensemble Functions
async def analyze_with_claude(therapy_area: str, product_name: str, analysis_type: str, claude_key: str) -> Dict[str, Any]:
    """Comprehensive analysis using Claude"""
    try:
        from emergentintegrations.llm.chat import LlmChat, UserMessage
        
        chat = LlmChat(
            api_key=claude_key,
            session_id=f"ensemble_claude_{uuid.uuid4()}",
            system_message=f"You are a world-class pharmaceutical analyst specializing in {analysis_type} analysis. Provide structured, quantitative insights with high confidence assessments."
        ).with_model("anthropic", "claude-sonnet-4-20250514").with_max_tokens(4096)
        
        if analysis_type == "comprehensive":
            prompt = f"""
            Provide a comprehensive pharmaceutical analysis for {therapy_area}{f' - {product_name}' if product_name else ''}:
            
            1. MARKET ANALYSIS (size, growth, key drivers)
            2. COMPETITIVE LANDSCAPE (major players, market shares, positioning)  
            3. CLINICAL LANDSCAPE (treatment pathways, unmet needs, emerging therapies)
            4. REGULATORY ENVIRONMENT (approval pathways, recent changes, requirements)
            5. FORECASTING OUTLOOK (3-5 year projections, key assumptions, risk factors)
            6. CONFIDENCE ASSESSMENT (rate confidence 1-10 for each section with justification)
            
            Provide specific numbers, percentages, and quantitative insights where possible.
            For each section, include a confidence score (1-10) based on available data quality.
            """
            
        elif analysis_type == "competitive":
            prompt = f"""
            Deep competitive intelligence analysis for {therapy_area}{f' focusing on {product_name}' if product_name else ''}:
            
            1. KEY COMPETITORS (names, products, market positions, strengths/weaknesses)
            2. MARKET SHARE ANALYSIS (current shares, trends, growth rates)
            3. PIPELINE THREATS (Phase 2/3 drugs, potential market disruptors)
            4. PRICING DYNAMICS (current pricing, pressure points, access challenges)
            5. DIFFERENTIATION FACTORS (unique selling points, competitive advantages)
            6. STRATEGIC POSITIONING (how competitors position vs. each other)
            
            Include confidence scores for each competitive assessment.
            """
            
        elif analysis_type == "forecasting":
            prompt = f"""
            Advanced forecasting analysis for {therapy_area}{f' - {product_name}' if product_name else ''}:
            
            1. MARKET SIZE PROJECTIONS (current and 5-year forecast with growth rates)
            2. PATIENT POPULATION TRENDS (epidemiology, diagnosis rates, treatment patterns)
            3. ADOPTION CURVES (uptake scenarios, penetration rates, time to peak)
            4. REVENUE MODELS (pricing assumptions, volume projections, peak sales)
            5. RISK SCENARIOS (best/worst case, key risk factors, probability assessments)
            6. SENSITIVITY ANALYSIS (key variables that impact forecasts)
            
            Provide specific numerical forecasts with confidence intervals.
            """
        
        response = await chat.send_message(UserMessage(text=prompt))
        
        # Extract confidence scores from response
        import re
        confidence_pattern = r'confidence[:\s]*([0-9](?:\.[0-9])?(?:/10)?)'
        confidence_matches = re.findall(confidence_pattern, response.lower())
        
        avg_confidence = 0.75  # Default
        if confidence_matches:
            scores = []
            for match in confidence_matches:
                try:
                    score = float(match.replace('/10', ''))
                    if score <= 1.0:
                        score *= 10  # Convert to 10-point scale if needed
                    scores.append(score / 10.0)  # Normalize to 0-1
                except:
                    continue
            if scores:
                avg_confidence = sum(scores) / len(scores)
        
        return {
            "analysis": response,
            "confidence_score": avg_confidence,
            "model": "claude-sonnet-4",
            "analysis_type": analysis_type,
            "word_count": len(response.split()),
            "timestamp": datetime.utcnow().isoformat(),
            "error": None
        }
        
    except Exception as e:
        return {
            "analysis": f"Claude analysis failed: {str(e)}",
            "confidence_score": 0.0,
            "model": "claude-sonnet-4",
            "analysis_type": analysis_type,
            "error": str(e),
            "timestamp": datetime.utcnow().isoformat()
        }

async def analyze_with_perplexity(therapy_area: str, product_name: str, analysis_type: str, perplexity_key: str) -> Dict[str, Any]:
    """Real-time market intelligence using Perplexity"""
    try:
        if analysis_type == "comprehensive":
            query = f"""
            Latest comprehensive market analysis for {therapy_area} pharmaceutical market 2024-2025:
            - Current market size and growth projections
            - Key pharmaceutical companies and their market positions  
            - Recent clinical trial results and FDA approvals
            - Market access and reimbursement trends
            - Competitive landscape and emerging threats
            {f'- Specific analysis of {product_name} and direct competitors' if product_name else ''}
            
            Include specific numbers, recent data, and quantitative metrics.
            """
            
        elif analysis_type == "competitive":
            query = f"""
            Real-time competitive intelligence for {therapy_area} market:
            - Current market leaders and their latest performance metrics
            - Recent partnership announcements and strategic moves
            - Latest clinical trial results and pipeline updates
            - Pricing changes and market access developments
            - Recent FDA approvals and regulatory decisions
            {f'- Latest news and developments for {product_name}' if product_name else ''}
            
            Focus on actionable competitive intelligence from the past 6 months.
            """
            
        elif analysis_type == "forecasting":
            query = f"""
            Latest forecasting data and market projections for {therapy_area}:
            - Recent analyst reports and revenue forecasts
            - Patient population studies and epidemiology updates
            - Treatment adoption trends and market penetration data
            - Pricing trends and reimbursement decisions
            - Market growth drivers and projected scenarios
            {f'- Specific forecasting data for {product_name}' if product_name else ''}
            
            Include quantitative projections and growth assumptions from recent sources.
            """
        
        result = await search_with_perplexity(query, perplexity_key, f"{analysis_type}_intelligence")
        
        # Assess data quality based on citation count and content length
        citation_count = len(result.citations)
        content_length = len(result.content)
        
        # Calculate confidence based on data quality indicators
        confidence_score = min(1.0, (citation_count * 0.1) + (content_length / 5000.0))
        confidence_score = max(0.3, confidence_score)  # Minimum 0.3 confidence
        
        return {
            "analysis": result.content,
            "confidence_score": confidence_score,
            "model": "perplexity-sonar-pro",
            "analysis_type": analysis_type,
            "citations": result.citations,
            "citation_count": citation_count,
            "search_query": result.search_query,
            "timestamp": datetime.utcnow().isoformat(),
            "error": None
        }
        
    except Exception as e:
        return {
            "analysis": f"Perplexity analysis failed: {str(e)}",
            "confidence_score": 0.0,
            "model": "perplexity-sonar-pro",
            "analysis_type": analysis_type,
            "citations": [],
            "error": str(e),
            "timestamp": datetime.utcnow().isoformat()
        }

async def analyze_with_gemini(therapy_area: str, product_name: str, analysis_type: str, gemini_key: str) -> Dict[str, Any]:
    """Optional Gemini analysis for ensemble"""
    try:
        from emergentintegrations.llm.chat import LlmChat, UserMessage
        
        chat = LlmChat(
            api_key=gemini_key,
            session_id=f"ensemble_gemini_{uuid.uuid4()}",
            system_message=f"You are a pharmaceutical market analyst. Provide {analysis_type} analysis with numerical insights and confidence assessments."
        ).with_model("gemini", "gemini-2.0-flash").with_max_tokens(3072)
        
        prompt = f"""
        Pharmaceutical {analysis_type} analysis for {therapy_area}{f' - {product_name}' if product_name else ''}:
        
        Focus on:
        1. Quantitative market metrics and data
        2. Specific numerical insights and projections  
        3. Evidence-based analysis with confidence levels
        4. Risk assessment and uncertainty factors
        5. Actionable strategic insights
        
        Rate your confidence level (1-10) for different aspects of the analysis.
        Provide specific numbers and quantitative insights where possible.
        """
        
        response = await chat.send_message(UserMessage(text=prompt))
        
        # Simple confidence extraction for Gemini
        confidence_score = 0.7  # Default for Gemini
        if any(word in response.lower() for word in ['confident', 'certain', 'definitive']):
            confidence_score = 0.85
        elif any(word in response.lower() for word in ['uncertain', 'limited', 'unclear']):
            confidence_score = 0.55
            
        return {
            "analysis": response,
            "confidence_score": confidence_score,
            "model": "gemini-2.0-flash",
            "analysis_type": analysis_type,
            "word_count": len(response.split()),
            "timestamp": datetime.utcnow().isoformat(),
            "error": None
        }
        
    except Exception as e:
        return {
            "analysis": f"Gemini analysis failed: {str(e)}",
            "confidence_score": 0.0,
            "model": "gemini-2.0-flash", 
            "analysis_type": analysis_type,
            "error": str(e),
            "timestamp": datetime.utcnow().isoformat()
        }

async def synthesize_ensemble_analysis(claude_result: Dict, perplexity_result: Dict, gemini_result: Dict = None) -> Dict[str, Any]:
    """Synthesize insights from multiple AI models"""
    try:
        models_used = ["Claude", "Perplexity"]
        analyses = [claude_result["analysis"], perplexity_result["analysis"]]
        confidences = [claude_result["confidence_score"], perplexity_result["confidence_score"]]
        
        if gemini_result and gemini_result.get("error") is None:
            models_used.append("Gemini")
            analyses.append(gemini_result["analysis"])
            confidences.append(gemini_result["confidence_score"])
        
        # Calculate model agreement score
        model_agreement_score = sum(confidences) / len(confidences)
        
        # Find consensus insights (common themes across models)
        consensus_insights = []
        conflicting_points = []
        
        # Simple keyword analysis for consensus (can be enhanced)
        common_keywords = []
        all_text = " ".join(analyses).lower()
        
        # Look for common pharmaceutical insights
        insight_patterns = [
            "market size", "growth rate", "competitive", "approval", "clinical trial", 
            "patient population", "treatment", "revenue", "forecast", "risk"
        ]
        
        for pattern in insight_patterns:
            if all(pattern in analysis.lower() for analysis in analyses):
                consensus_insights.append(f"All models identify {pattern} as key factor")
        
        # Look for conflicting viewpoints (different confidence levels or opposing statements)
        if max(confidences) - min(confidences) > 0.3:
            conflicting_points.append(f"Model confidence varies significantly ({min(confidences):.2f} to {max(confidences):.2f})")
        
        # Generate synthesis
        synthesis_prompt = f"""
        Based on analysis from {len(models_used)} AI models ({', '.join(models_used)}), provide a synthesized executive summary that:
        
        1. Highlights consensus insights where models agree
        2. Notes areas of uncertainty or disagreement  
        3. Provides weighted recommendations based on model confidence
        4. Identifies the most reliable insights and key uncertainties
        
        Model confidences: {[f'{models_used[i]}: {confidences[i]:.2f}' for i in range(len(models_used))]}
        
        Key findings to synthesize:
        {chr(10).join([f'{models_used[i]}: {analyses[i][:200]}...' for i in range(len(analyses))])}
        """
        
        # Use the highest confidence model for synthesis
        best_model_idx = confidences.index(max(confidences))
        if best_model_idx == 0 and claude_result.get("error") is None:
            # Use Claude for synthesis
            from emergentintegrations.llm.chat import LlmChat, UserMessage
            chat = LlmChat(
                api_key="temp",  # Would need to be passed
                session_id=f"synthesis_{uuid.uuid4()}",
                system_message="You are an expert at synthesizing multiple AI analyses into coherent insights."
            ).with_model("anthropic", "claude-sonnet-4-20250514")
            
            synthesis = f"Multi-model ensemble analysis combining insights from {', '.join(models_used)} with agreement score: {model_agreement_score:.2f}"
        else:
            synthesis = f"Ensemble analysis from {len(models_used)} models with weighted confidence: {model_agreement_score:.2f}"
        
        # Compile recommendation
        if model_agreement_score > 0.8:
            recommendation = "High confidence ensemble - proceed with insights"
        elif model_agreement_score > 0.6:
            recommendation = "Moderate confidence - validate key assumptions"
        else:
            recommendation = "Low agreement - require additional validation"
        
        return {
            "ensemble_synthesis": synthesis,
            "model_agreement_score": model_agreement_score,
            "consensus_insights": consensus_insights,
            "conflicting_points": conflicting_points,
            "recommendation": recommendation,
            "models_used": models_used,
            "confidence_scores": {models_used[i]: confidences[i] for i in range(len(models_used))},
            "synthesis_quality": "automated",
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        return {
            "ensemble_synthesis": f"Synthesis failed: {str(e)}",
            "model_agreement_score": 0.0,
            "consensus_insights": [],
            "conflicting_points": [f"Synthesis error: {str(e)}"],
            "recommendation": "Unable to provide recommendation due to synthesis error",
            "models_used": [],
            "confidence_scores": {},
            "error": str(e)
        }

async def run_ensemble_analysis(therapy_area: str, product_name: str, analysis_type: str, 
                              claude_key: str, perplexity_key: str, gemini_key: str = None, 
                              use_gemini: bool = False) -> EnsembleResult:
    """Run complete multi-model ensemble analysis"""
    try:
        # Run analyses in parallel for efficiency
        import asyncio
        
        tasks = []
        tasks.append(analyze_with_claude(therapy_area, product_name, analysis_type, claude_key))
        tasks.append(analyze_with_perplexity(therapy_area, product_name, analysis_type, perplexity_key))
        
        if use_gemini and gemini_key:
            tasks.append(analyze_with_gemini(therapy_area, product_name, analysis_type, gemini_key))
        
        # Execute analyses
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        claude_result = results[0] if not isinstance(results[0], Exception) else {"analysis": f"Claude failed: {results[0]}", "confidence_score": 0.0, "error": str(results[0])}
        perplexity_result = results[1] if not isinstance(results[1], Exception) else {"analysis": f"Perplexity failed: {results[1]}", "confidence_score": 0.0, "error": str(results[1])}
        gemini_result = None
        
        if use_gemini and len(results) > 2:
            gemini_result = results[2] if not isinstance(results[2], Exception) else {"analysis": f"Gemini failed: {results[2]}", "confidence_score": 0.0, "error": str(results[2])}
        
        # Synthesize results
        synthesis = await synthesize_ensemble_analysis(claude_result, perplexity_result, gemini_result)
        
        # Compile sources
        sources = []
        if perplexity_result.get("citations"):
            sources.extend(perplexity_result["citations"])
        
        # Create ensemble result
        ensemble_result = EnsembleResult(
            therapy_area=therapy_area,
            product_name=product_name,
            claude_analysis=claude_result,
            perplexity_intelligence=perplexity_result,
            gemini_analysis=gemini_result,
            ensemble_synthesis=synthesis["ensemble_synthesis"],
            confidence_scores=synthesis["confidence_scores"],
            consensus_insights=synthesis["consensus_insights"],
            conflicting_points=synthesis["conflicting_points"],
            recommendation=synthesis["recommendation"],
            model_agreement_score=synthesis["model_agreement_score"],
            sources=list(set(sources))  # Remove duplicates
        )
        
        return ensemble_result
        
    except Exception as e:
        # Return error ensemble result
        return EnsembleResult(
            therapy_area=therapy_area,
            product_name=product_name,
            claude_analysis={"analysis": f"Error: {str(e)}", "confidence_score": 0.0, "error": str(e)},
            perplexity_intelligence={"analysis": f"Error: {str(e)}", "confidence_score": 0.0, "error": str(e)},
            gemini_analysis=None,
            ensemble_synthesis=f"Ensemble analysis failed: {str(e)}",
            confidence_scores={},
            consensus_insights=[],
            conflicting_points=[f"Complete ensemble failure: {str(e)}"],
            recommendation="Unable to provide analysis due to system error",
            model_agreement_score=0.0,
            sources=[]
        )

# Company Intelligence Engine Functions
async def identify_parent_company(product_name: str, perplexity_key: str) -> Dict[str, str]:
    """Identify parent company and basic info from product name using Perplexity"""
    try:
        query = f"What company makes {product_name}? Provide the parent company name, official website, drug class, and therapeutic area. Include recent financial information and market position."
        
        result = await search_with_perplexity(query, perplexity_key, "company_identification")
        
        # Extract structured information from the response
        import re
        content = result.content.lower()
        
        # Common pharmaceutical company patterns
        company_patterns = [
            r'(novartis|pfizer|roche|bristol myers|merck|johnson \& johnson|abbvie|gilead|biogen|amgen|eli lilly|gsk|sanofi|astrazeneca|bayer|takeda|celgene|blueprint medicines|deciphera|exelixis)',
            r'([a-z\s&]+)\s+(?:inc|corp|ltd|pharmaceuticals|pharma|biopharm)',
            r'company:\s*([a-z\s&]+)',
            r'manufacturer:\s*([a-z\s&]+)',
            r'developed by\s+([a-z\s&]+)'
        ]
        
        company_name = "Unknown Company"
        for pattern in company_patterns:
            match = re.search(pattern, content)
            if match:
                company_name = match.group(1).title().strip()
                break
        
        # Extract website
        website_pattern = r'https?://(?:www\.)?([a-z0-9\-]+\.com)'
        website_match = re.search(website_pattern, result.content)
        website = website_match.group(0) if website_match else f"https://{company_name.lower().replace(' ', '').replace('&', '')}.com"
        
        # Extract drug class/therapeutic area
        class_patterns = [
            r'(kinase inhibitor|monoclonal antibody|checkpoint inhibitor|targeted therapy|immunotherapy|chemotherapy|tyrosine kinase inhibitor|small molecule)',
            r'class:\s*([a-z\s]+)',
            r'mechanism:\s*([a-z\s]+)'
        ]
        
        drug_class = "Unknown Class"
        for pattern in class_patterns:
            match = re.search(pattern, content)
            if match:
                drug_class = match.group(1).title().strip()
                break
        
        return {
            "company_name": company_name,
            "website": website,
            "drug_class": drug_class,
            "search_content": result.content[:500],
            "sources": result.citations
        }
        
    except Exception as e:
        logging.error(f"Company identification error: {str(e)}")
        return {
            "company_name": "Unknown Company",
            "website": "",
            "drug_class": "Unknown Class",
            "search_content": f"Error: {str(e)}",
            "sources": []
        }

async def scrape_investor_relations(company_name: str, company_website: str, perplexity_key: str) -> Dict[str, Any]:
    """Get investor relations data using Perplexity search - no direct scraping"""
    try:
        # Use Perplexity to gather comprehensive investor intelligence
        queries = [
            f"{company_name} latest financial results revenue earnings quarterly results 2024 2025",
            f"{company_name} investor presentation slides recent earnings call financial highlights",
            f"{company_name} press releases recent news corporate developments partnerships",
            f"{company_name} market capitalization stock performance financial metrics pipeline"
        ]
        
        scraped_data = {
            "financial_highlights": [],
            "recent_earnings": [],
            "pipeline_updates": [],
            "press_releases": [],
            "presentation_links": [],
            "sources_accessed": [],
            "error_log": []
        }
        
        for i, query in enumerate(queries):
            try:
                result = await search_with_perplexity(query, perplexity_key, f"investor_relations_{i+1}")
                
                # Extract financial metrics from content
                content = result.content
                scraped_data["sources_accessed"].extend(result.citations)
                
                # Parse financial information
                import re
                
                # Extract revenue/sales figures
                financial_patterns = [
                    r'\$([0-9.,]+)\s*(million|billion|M|B).*(?:revenue|sales|earnings)',
                    r'(?:revenue|sales|earnings).*?\$([0-9.,]+)\s*(million|billion|M|B)',
                    r'(?:quarterly|annual).*?\$([0-9.,]+)\s*(million|billion|M|B)'
                ]
                
                for pattern in financial_patterns:
                    matches = re.findall(pattern, content, re.IGNORECASE)
                    for match in matches[:3]:  # Limit results
                        scraped_data["financial_highlights"].append({
                            "metric": f"${match[0]} {match[1]}",
                            "source": f"perplexity_search_query_{i+1}",
                            "context": "revenue_sales_data",
                            "query_type": f"financial_search_{i+1}"
                        })
                
                # Extract press release information
                if "press release" in query.lower() or "news" in query.lower():
                    press_lines = content.split('\n')[:10]
                    for line in press_lines:
                        if len(line.strip()) > 20 and any(keyword in line.lower() for keyword in ['announced', 'reports', 'partnership', 'approval', 'results']):
                            scraped_data["press_releases"].append({
                                "title": line.strip()[:150],
                                "source": "perplexity_intelligence",
                                "type": "press_release_mention",
                                "query_context": query[:50] + "..."
                            })
                
                # Extract pipeline/development info
                if "pipeline" in query.lower() or "development" in query.lower():
                    pipeline_keywords = ['phase', 'trial', 'study', 'development', 'pipeline', 'candidate']
                    pipeline_lines = [line for line in content.split('\n') 
                                    if any(keyword in line.lower() for keyword in pipeline_keywords)]
                    
                    for line in pipeline_lines[:5]:
                        if len(line.strip()) > 15:
                            scraped_data["pipeline_updates"].append({
                                "update": line.strip()[:200],
                                "source": "perplexity_pipeline_search",
                                "extracted_from": f"query_{i+1}"
                            })
                
            except Exception as query_error:
                scraped_data["error_log"].append({
                    "query": query[:50] + "...",
                    "error": str(query_error),
                    "timestamp": datetime.utcnow().isoformat(),
                    "error_type": "perplexity_query_error"
                })
                continue
        
        # Remove duplicates and clean data
        scraped_data["sources_accessed"] = list(set(scraped_data["sources_accessed"]))
        
        return scraped_data
        
    except Exception as e:
        return {
            "financial_highlights": [],
            "recent_earnings": [],
            "pipeline_updates": [],
            "press_releases": [],
            "presentation_links": [],
            "sources_accessed": [],
            "error_log": [{
                "error": str(e),
                "error_type": "investor_relations_search_failure",
                "timestamp": datetime.utcnow().isoformat()
            }]
        }

async def find_competitive_products(drug_class: str, therapy_area: str, perplexity_key: str) -> List[Dict[str, Any]]:
    """Find competing products in the same therapeutic class using Perplexity"""
    try:
        query = f"""
        List all competing drugs and products in the {drug_class} class for {therapy_area} therapy area. 
        For each competitor, provide:
        1. Product name
        2. Parent company
        3. Approval status and dates
        4. Market share or sales figures if available
        5. Key differentiators
        Include both approved drugs and those in late-stage development (Phase 3).
        """
        
        result = await search_with_perplexity(query, perplexity_key, "competitive_products")
        
        # Parse the response to extract structured competitor data
        competitors = []
        content_lines = result.content.split('\n')
        
        current_product = {}
        for line in content_lines:
            line = line.strip()
            if not line:
                if current_product and current_product.get('name'):
                    competitors.append(current_product)
                    current_product = {}
                continue
            
            # Look for product names (usually at start of line or after numbers)
            import re
            product_pattern = r'^\d*\.?\s*([A-Z][a-zA-Z0-9\-\s]+(?:\([a-z]+\))?)'
            company_pattern = r'(?:by|from|manufacturer?:?)\s+([A-Z][a-zA-Z\s&]+)'
            
            product_match = re.search(product_pattern, line)
            company_match = re.search(company_pattern, line, re.IGNORECASE)
            
            if product_match and not current_product.get('name'):
                current_product['name'] = product_match.group(1).strip()
                current_product['description'] = line
            
            if company_match:
                current_product['company'] = company_match.group(1).strip()
            
            # Look for approval dates
            if 'approved' in line.lower() or 'fda' in line.lower():
                current_product['approval_status'] = line
            
            # Look for market share or sales
            if any(term in line.lower() for term in ['market share', 'sales', 'revenue', '%']):
                current_product['market_metrics'] = line
        
        # Add last product if exists
        if current_product and current_product.get('name'):
            competitors.append(current_product)
        
        # If parsing didn't work well, create fallback structure
        if len(competitors) < 2:
            # Extract product names using simpler method
            lines_with_drugs = [line for line in content_lines 
                              if any(term in line.lower() for term in ['drug', 'therapy', 'treatment', 'inhibitor', 'mab'])]
            
            for line in lines_with_drugs[:5]:  # Limit to 5
                competitors.append({
                    'name': line.strip()[:50],
                    'description': line.strip(),
                    'company': 'To be determined',
                    'source': 'perplexity_search'
                })
        
        # Add metadata
        for competitor in competitors:
            competitor['search_query'] = query
            competitor['therapeutic_area'] = therapy_area
            competitor['drug_class'] = drug_class
        
        return competitors[:10]  # Limit to top 10
        
    except Exception as e:
        logging.error(f"Competitive products search error: {str(e)}")
        return [{
            'name': 'Error in competitive search',
            'description': str(e),
            'company': 'Unknown',
            'search_query': query if 'query' in locals() else 'Unknown'
        }]

async def generate_company_intelligence(product_name: str, therapy_area: str, perplexity_key: str, include_competitors: bool = True) -> CompanyIntelligence:
    """Complete company intelligence pipeline using only Perplexity searches"""
    try:
        error_log = []
        
        # Step 1: Identify parent company using Perplexity
        try:
            company_info = await identify_parent_company(product_name, perplexity_key)
        except Exception as e:
            error_log.append({"step": "company_identification", "error": str(e)})
            company_info = {
                "company_name": "Unknown Company",
                "website": "",
                "drug_class": "Unknown Class",
                "search_content": f"Error: {str(e)}",
                "sources": []
            }
        
        # Step 2: Get investor relations data using Perplexity (no direct scraping)
        try:
            investor_data = await scrape_investor_relations(
                company_info["company_name"], 
                company_info["website"], 
                perplexity_key
            )
        except Exception as e:
            error_log.append({"step": "investor_relations", "error": str(e)})
            investor_data = {
                "financial_highlights": [],
                "recent_earnings": [],
                "pipeline_updates": [],
                "press_releases": [],
                "presentation_links": [],
                "sources_accessed": [],
                "error": str(e)
            }
        
        # Step 3: Find competitive products using Perplexity
        competitive_products = []
        if include_competitors:
            try:
                competitive_products = await find_competitive_products(
                    company_info["drug_class"], 
                    therapy_area or "general", 
                    perplexity_key
                )
            except Exception as e:
                error_log.append({"step": "competitive_products", "error": str(e)})
                competitive_products = [{
                    'name': 'Error in competitive search',
                    'description': str(e),
                    'company': 'Unknown',
                    'error_type': 'competitive_search_failure'
                }]
        
        # Step 4: Get recent developments using Perplexity
        recent_developments = []
        try:
            developments_query = f"""
            Latest news, developments, clinical updates, partnerships, and corporate announcements 
            for {product_name} and {company_info['company_name']} in the past 6 months. 
            Include FDA approvals, clinical trial results, market expansions, financial results.
            """
            developments_result = await search_with_perplexity(developments_query, perplexity_key, "recent_developments")
            
            dev_lines = developments_result.content.split('\n')
            for line in dev_lines:
                if line.strip() and len(line.strip()) > 20:
                    # Filter for meaningful updates
                    if any(keyword in line.lower() for keyword in ['approved', 'announced', 'reported', 'launched', 'partnership', 'results', 'fda', 'trial']):
                        recent_developments.append({
                            "update": line.strip()[:200],
                            "source": "perplexity_developments_search",
                            "timestamp": datetime.utcnow().isoformat(),
                            "relevance": "high" if any(term in line.lower() for term in [product_name.lower(), 'fda', 'approved']) else "medium"
                        })
                        
                if len(recent_developments) >= 10:  # Limit to 10 developments
                    break
                    
        except Exception as e:
            error_log.append({"step": "recent_developments", "error": str(e)})
            recent_developments = [{
                "update": f"Error retrieving recent developments: {str(e)}",
                "source": "error",
                "timestamp": datetime.utcnow().isoformat(),
                "error_type": "developments_search_failure"
            }]
        
        # Step 5: Enhanced financial metrics using Perplexity
        financial_metrics = {}
        try:
            financial_query = f"""
            {company_info['company_name']} latest financial performance, revenue, market capitalization, 
            stock performance, quarterly earnings, annual revenue, growth rates, market position in pharmaceutical industry.
            Include specific numbers and recent financial highlights.
            """
            financial_result = await search_with_perplexity(financial_query, perplexity_key, "financial_metrics")
            
            # Extract financial data from the search result
            import re
            content = financial_result.content
            
            # Look for various financial metrics
            revenue_patterns = [
                r'revenue.*?\$([0-9.,]+)\s*(million|billion|M|B)',
                r'\$([0-9.,]+)\s*(million|billion|M|B).*revenue',
                r'sales.*?\$([0-9.,]+)\s*(million|billion|M|B)'
            ]
            
            market_cap_patterns = [
                r'market cap.*?\$([0-9.,]+)\s*(million|billion|M|B)',
                r'market capitalization.*?\$([0-9.,]+)\s*(million|billion|M|B)'
            ]
            
            extracted_revenues = []
            extracted_market_caps = []
            
            for pattern in revenue_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                extracted_revenues.extend([f"${match[0]} {match[1]}" for match in matches[:2]])
                
            for pattern in market_cap_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                extracted_market_caps.extend([f"${match[0]} {match[1]}" for match in matches[:2]])
            
            financial_metrics = {
                "highlights": investor_data.get("financial_highlights", []),
                "extracted_revenues": extracted_revenues,
                "extracted_market_caps": extracted_market_caps,
                "market_position": content[:300] + "..." if content else "No financial data available",
                "growth_metrics": "See search results and investor data",
                "search_sources": financial_result.citations,
                "data_quality": "perplexity_extracted"
            }
            
        except Exception as e:
            error_log.append({"step": "financial_metrics", "error": str(e)})
            financial_metrics = {
                "highlights": investor_data.get("financial_highlights", []),
                "error": str(e),
                "error_type": "financial_search_failure"
            }
        
        # Step 6: Compile all sources
        all_sources = []
        all_sources.extend(company_info.get("sources", []))
        all_sources.extend(investor_data.get("sources_accessed", []))
        if 'search_sources' in financial_metrics:
            all_sources.extend(financial_metrics["search_sources"])
        # Remove duplicates
        all_sources = list(set(all_sources))
        
        # Step 7: Create comprehensive intelligence object
        intelligence = CompanyIntelligence(
            product_name=product_name,
            parent_company=company_info["company_name"],
            company_website=company_info["website"],
            market_class=company_info["drug_class"],
            investor_data=investor_data,
            press_releases=investor_data.get("press_releases", []),
            competitive_products=competitive_products,
            financial_metrics=financial_metrics,
            recent_developments=recent_developments,
            sources_scraped=all_sources
        )
        
        # Add error logging for debugging
        if error_log:
            intelligence.investor_data["error_log"] = error_log
        
        return intelligence
        
    except Exception as e:
        logging.error(f"Company intelligence generation error: {str(e)}")
        # Return comprehensive fallback intelligence with error tracking
        return CompanyIntelligence(
            product_name=product_name,
            parent_company="Error in Company Identification",
            company_website="",
            market_class="Error in Classification",
            investor_data={"error": str(e), "error_type": "complete_pipeline_failure"},
            press_releases=[],
            competitive_products=[{"name": "Error in competitive analysis", "error": str(e)}],
            financial_metrics={"error": str(e), "error_type": "complete_pipeline_failure"},
            recent_developments=[{"update": f"Error in intelligence generation: {str(e)}", "source": "error"}],
            sources_scraped=[]
        )

# Perplexity Integration Functions
async def search_with_perplexity(query: str, api_key: str, search_focus: str = "pharmaceutical") -> PerplexityResult:
    """Enhanced search using Perplexity API with citations"""
    try:
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        # Enhanced prompt for pharmaceutical intelligence
        enhanced_query = f"""
        {query}
        
        Please provide a comprehensive analysis focusing on:
        - Latest market data and quantitative metrics
        - Key pharmaceutical companies and their market positions
        - Recent clinical trial results and regulatory updates
        - Financial metrics (market size, growth rates, pricing)
        - Competitive landscape and market trends
        
        Provide specific numbers, percentages, and recent developments with dates.
        Focus on actionable pharmaceutical intelligence.
        """
        
        payload = {
            "model": "sonar-pro",
            "messages": [
                {
                    "role": "system", 
                    "content": f"You are an expert pharmaceutical intelligence analyst. Provide structured, quantitative analysis with specific data points, market metrics, and recent developments. Always cite reliable sources and include numerical data where available. Focus on {search_focus} intelligence."
                },
                {
                    "role": "user",
                    "content": enhanced_query
                }
            ],
            "search_recency_filter": "month",
            "return_citations": True,
            "search_domain_filter": ["clinicaltrials.gov", "fda.gov", "ema.europa.eu", "sec.gov", "pharmaceutical-journal.com", "biopharmadive.com", "fiercepharma.com"]
        }
        
        timeout = httpx.Timeout(45.0)
        async with httpx.AsyncClient(timeout=timeout) as client:
            response = await client.post(
                "https://api.perplexity.ai/chat/completions",
                json=payload,
                headers=headers
            )
            
            if response.status_code == 200:
                data = response.json()
                content = data["choices"][0]["message"]["content"]
                
                # Extract citations from the response
                citations = []
                if "citations" in data:
                    citations = data["citations"]
                else:
                    # Fallback: extract URLs from content
                    import re
                    url_pattern = r'https?://[^\s\])]+'
                    citations = re.findall(url_pattern, content)
                    citations = list(set(citations))  # Remove duplicates
                
                return PerplexityResult(
                    content=content,
                    citations=citations,
                    search_query=query
                )
            else:
                logging.error(f"Perplexity API error: {response.status_code} - {response.text}")
                return PerplexityResult(
                    content=f"Search failed with status {response.status_code}. Please check your Perplexity API key.",
                    citations=[],
                    search_query=query
                )
                
    except Exception as e:
        logging.error(f"Perplexity search error: {str(e)}")
        return PerplexityResult(
            content=f"Search error: {str(e)}. Please verify your Perplexity API key is valid.",
            citations=[],
            search_query=query
        )

async def enhanced_competitive_analysis_with_perplexity(therapy_area: str, product_name: str, perplexity_key: str, claude_key: str):
    """Combine Perplexity real-time search with Claude analysis for comprehensive competitive intelligence"""
    try:
        # First, get real-time market data from Perplexity
        market_query = f"Latest market analysis for {therapy_area} therapy area pharmaceutical market 2024-2025 including market size, key players, recent approvals, competitive landscape"
        if product_name:
            market_query += f" specifically focusing on {product_name} and competing products"
            
        perplexity_result = await search_with_perplexity(market_query, perplexity_key, "competitive_intelligence")
        
        # Then enhance with Claude's analytical capabilities
        from emergentintegrations.llm.chat import LlmChat, UserMessage
        
        chat = LlmChat(
            api_key=claude_key,
            session_id=f"enhanced_competitive_{uuid.uuid4()}",
            system_message="You are a pharmaceutical competitive intelligence expert. Analyze the provided real-time market data and enhance it with structured competitive analysis."
        ).with_model("anthropic", "claude-sonnet-4-20250514").with_max_tokens(3072)
        
        analysis_prompt = f"""
        Based on the following real-time market intelligence data, provide a comprehensive competitive analysis:
        
        REAL-TIME MARKET DATA:
        {perplexity_result.content}
        
        SOURCES: {', '.join(perplexity_result.citations)}
        
        Please structure your analysis with:
        1. KEY MARKET PLAYERS (with specific company names, products, market shares if available)
        2. MARKET DYNAMICS (size, growth rate, trends)
        3. COMPETITIVE POSITIONING (how players differentiate)
        4. RECENT DEVELOPMENTS (approvals, pipeline, partnerships)
        5. QUANTITATIVE METRICS (market size, pricing, growth rates)
        
        Focus on {therapy_area} therapy area.
        {f"Emphasize analysis of {product_name} and direct competitors." if product_name else ""}
        
        Provide specific company names, drug names, and quantitative data from the real-time sources.
        """
        
        claude_response = await chat.send_message(UserMessage(text=analysis_prompt))
        
        # Combine both analyses
        return {
            "real_time_intelligence": {
                "content": perplexity_result.content,
                "sources": perplexity_result.citations,
                "search_query": perplexity_result.search_query
            },
            "enhanced_analysis": claude_response,
            "combined_insights": f"REAL-TIME MARKET INTELLIGENCE:\n{perplexity_result.content}\n\nENHANCED COMPETITIVE ANALYSIS:\n{claude_response}",
            "total_sources": len(perplexity_result.citations),
            "analysis_type": "Perplexity + Claude Enhanced Intelligence"
        }
        
    except Exception as e:
        logging.error(f"Enhanced competitive analysis error: {str(e)}")
        return {
            "real_time_intelligence": {"content": f"Error: {str(e)}", "sources": [], "search_query": ""},
            "enhanced_analysis": "Analysis failed due to API issues",
            "combined_insights": f"Error in enhanced analysis: {str(e)}",
            "total_sources": 0,
            "analysis_type": "Error"
        }

# Web Research Functions
async def search_clinical_trials(therapy_area: str):
    """Search ClinicalTrials.gov for relevant trials"""
    try:
        url = "https://clinicaltrials.gov/api/v2/studies"
        params = {
            "query.cond": therapy_area.replace(" ", "+"),
            "pageSize": 20,
            "format": "json",
            "fields": "NCTId,BriefTitle,OverallStatus,Phase,Condition"
        }
        
        timeout = httpx.Timeout(30.0)
        async with httpx.AsyncClient(timeout=timeout) as client:
            response = await client.get(url, params=params)
            if response.status_code == 200:
                data = response.json()
                return data.get('studies', [])
    except Exception as e:
        logging.error(f"Clinical trials search error: {str(e)}")
    return []

async def search_regulatory_intelligence(therapy_area: str, api_key: str):
    """Generate regulatory intelligence using Claude"""
    try:
        from emergentintegrations.llm.chat import LlmChat, UserMessage
        
        chat = LlmChat(
            api_key=api_key,
            session_id=f"regulatory_{uuid.uuid4()}",
            system_message="You are a regulatory affairs expert specializing in pharmaceutical approvals and market access."
        ).with_model("anthropic", "claude-sonnet-4-20250514").with_max_tokens(2048)
        
        prompt = f"""
        Provide comprehensive regulatory intelligence for {therapy_area} including:
        
        1. Key regulatory pathways (FDA, EMA, other major markets)
        2. Recent approvals and rejections in this space
        3. Regulatory trends and guidance updates
        4. Timeline expectations for new therapies
        5. Market access considerations and reimbursement landscape
        
        Structure as JSON with these sections: pathways, recent_activity, trends, timelines, market_access
        """
        
        response = await chat.send_message(UserMessage(text=prompt))
        
        # Try to parse as JSON, fallback to structured text
        try:
            return json.loads(response)
        except:
            return {
                "pathways": "See full analysis",
                "recent_activity": "See full analysis", 
                "trends": "See full analysis",
                "timelines": "See full analysis",
                "market_access": response
            }
    except Exception as e:
        logging.error(f"Regulatory intelligence error: {str(e)}")
        return {}

async def generate_competitive_analysis(therapy_area: str, api_key: str):
    """Generate competitive landscape analysis using Claude"""
    try:
        from emergentintegrations.llm.chat import LlmChat, UserMessage
        
        chat = LlmChat(
            api_key=api_key,
            session_id=f"competitive_{uuid.uuid4()}",
            system_message="You are a pharmaceutical competitive intelligence analyst with expertise in market dynamics and competitive positioning."
        ).with_model("anthropic", "claude-sonnet-4-20250514").with_max_tokens(3072)
        
        prompt = f"""
        Conduct a comprehensive competitive analysis for {therapy_area} therapy area. 
        
        Please provide a structured analysis covering:
        
        1. MAJOR COMPETITORS: List the top 5-7 companies/products in this space with:
           - Company name
           - Key products/drugs 
           - Estimated market share
           - Main strengths
           - Key weaknesses
        
        2. MARKET DYNAMICS: Current market trends, growth drivers, challenges
        
        3. PIPELINE ANALYSIS: Key drugs in development (Phase II/III)
        
        4. COMPETITIVE POSITIONING: How different players differentiate
        
        5. UPCOMING CATALYSTS: Key events, approvals, patent expiries in next 2 years
        
        Be specific with actual company names, drug names, and real market data where possible.
        Focus on providing actionable competitive intelligence.
        """
        
        response = await chat.send_message(UserMessage(text=prompt))
        
        # Try to extract structured information from the response
        lines = response.split('\n')
        competitors = []
        market_dynamics = ""
        pipeline = ""
        positioning = ""
        catalysts = ""
        
        current_section = ""
        current_content = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            if any(keyword in line.upper() for keyword in ["COMPETITOR", "MAJOR", "KEY PLAYER"]):
                current_section = "competitors"
                current_content = []
            elif any(keyword in line.upper() for keyword in ["MARKET DYNAMIC", "MARKET TREND"]):
                current_section = "market_dynamics" 
                current_content = []
            elif any(keyword in line.upper() for keyword in ["PIPELINE", "DEVELOPMENT"]):
                current_section = "pipeline"
                current_content = []
            elif any(keyword in line.upper() for keyword in ["POSITIONING", "DIFFERENTIAT"]):
                current_section = "positioning"
                current_content = []
            elif any(keyword in line.upper() for keyword in ["CATALYST", "UPCOMING", "EVENTS"]):
                current_section = "catalysts"
                current_content = []
            else:
                current_content.append(line)
                
                # Process competitor lines
                if current_section == "competitors" and line:
                    # Try to extract company info from various formats
                    if any(char in line for char in ['-', '', '1.', '2.', '3.']):
                        parts = line.split(':', 1) if ':' in line else [line, ""]
                        company_part = parts[0].strip()
                        details_part = parts[1].strip() if len(parts) > 1 else ""
                        
                        # Clean company name
                        for prefix in ['1.', '2.', '3.', '4.', '5.', '6.', '7.', '-', '']:
                            company_part = company_part.replace(prefix, '').strip()
                        
                        if company_part and len(company_part) > 2:
                            # Extract market share if present
                            market_share = 25  # Default
                            if '%' in details_part:
                                import re
                                share_match = re.search(r'(\d+)%', details_part)
                                if share_match:
                                    market_share = int(share_match.group(1))
                            
                            competitors.append({
                                "name": company_part[:50],  # Limit length
                                "products": details_part[:100] if details_part else "Market presence",
                                "market_share": market_share,
                                "strengths": details_part[:100] if details_part else "Established player",
                                "weaknesses": "See analysis for details"
                            })
            
            # Collect content for other sections
            if current_section == "market_dynamics" and current_content:
                market_dynamics = '\n'.join(current_content[-10:])  # Last 10 lines
            elif current_section == "pipeline" and current_content:
                pipeline = '\n'.join(current_content[-10:])
            elif current_section == "positioning" and current_content:
                positioning = '\n'.join(current_content[-10:])
            elif current_section == "catalysts" and current_content:
                catalysts = '\n'.join(current_content[-10:])
        
        # Ensure we have some competitors
        if not competitors:
            # Extract from full response using basic parsing
            response_lines = response.split('\n')
            for line in response_lines:
                if any(company in line.upper() for company in ['NOVARTIS', 'PFIZER', 'ROCHE', 'BRISTOL', 'MERCK', 'JOHNSON', 'ABBVIE', 'GILEAD', 'BIOGEN', 'AMGEN']):
                    competitors.append({
                        "name": line.strip()[:30],
                        "products": "Multiple products in portfolio",
                        "market_share": 15,
                        "strengths": "Established pharmaceutical company",
                        "weaknesses": "High competition"
                    })
                if len(competitors) >= 5:
                    break
        
        # Ensure we have content for other sections
        if not market_dynamics:
            market_dynamics = response[:500] + "..."
        if not pipeline:
            pipeline = "Pipeline analysis included in full competitive analysis"
        if not catalysts:
            catalysts = "Key market catalysts and events detailed in comprehensive analysis"
        
        return {
            "competitors": competitors[:7],  # Top 7
            "market_dynamics": market_dynamics,
            "pipeline": pipeline,
            "positioning": positioning or "Competitive positioning varies by therapeutic focus and market presence",
            "catalysts": catalysts,
            "full_analysis": response
        }
        
    except Exception as e:
        logging.error(f"Competitive analysis error: {str(e)}")
        return {
            "competitors": [
                {"name": "Analysis Error", "market_share": 0, "strengths": "Please try again", "products": str(e)[:100]}
            ],
            "market_dynamics": f"Error generating analysis: {str(e)}",
            "pipeline": "Please regenerate analysis",
            "positioning": "Error in analysis generation",
            "catalysts": "Please try again with valid API key",
            "full_analysis": f"Error: {str(e)}"
        }

async def generate_risk_assessment(therapy_area: str, analysis_data: dict, api_key: str):
    """Generate comprehensive risk assessment"""
    try:
        from emergentintegrations.llm.chat import LlmChat, UserMessage
        
        chat = LlmChat(
            api_key=api_key,
            session_id=f"risk_{uuid.uuid4()}",
            system_message="You are a pharmaceutical risk assessment expert specializing in clinical, regulatory, and commercial risk analysis."
        ).with_model("anthropic", "claude-sonnet-4-20250514").with_max_tokens(2048)
        
        prompt = f"""
        Based on the therapy area analysis for {therapy_area}, assess key risks across:
        
        1. Clinical Risks (efficacy, safety, trial design, endpoints)
        2. Regulatory Risks (approval pathways, requirements, precedents)  
        3. Commercial Risks (competition, market access, pricing pressure)
        4. Operational Risks (manufacturing, supply chain, partnerships)
        5. Market Risks (market size, adoption, reimbursement)
        
        For each category, provide: high/medium/low risk level, key factors, mitigation strategies
        Structure as JSON with risk categories and overall risk score (1-10)
        """
        
        response = await chat.send_message(UserMessage(text=prompt))
        
        try:
            return json.loads(response)
        except:
            return {
                "clinical_risk": {"level": "Medium", "factors": ["See analysis"]},
                "regulatory_risk": {"level": "Medium", "factors": ["See analysis"]},
                "commercial_risk": {"level": "Medium", "factors": ["See analysis"]},
                "operational_risk": {"level": "Low", "factors": ["See analysis"]},
                "market_risk": {"level": "Medium", "factors": ["See analysis"]},
                "overall_score": 5,
                "full_assessment": response
            }
    except Exception as e:
        logging.error(f"Risk assessment error: {str(e)}")
        return {}

async def generate_scenario_models(therapy_area: str, analysis_data: dict, scenarios: List[str], api_key: str):
    """Generate multi-scenario forecasting models"""
    try:
        from emergentintegrations.llm.chat import LlmChat, UserMessage
        
        chat = LlmChat(
            api_key=api_key,
            session_id=f"scenarios_{uuid.uuid4()}",
            system_message="You are a pharmaceutical forecasting expert specializing in scenario modeling and market projections."
        ).with_model("anthropic", "claude-sonnet-4-20250514").with_max_tokens(3072)
        
        product_name = analysis_data.get('product_name', '')
        product_context = f" for {product_name}" if product_name else ""
        
        prompt = f"""
        Create detailed forecasting scenarios for {therapy_area}{product_context}.
        
        Generate realistic market forecasts for these scenarios: {', '.join(scenarios)}
        
        For each scenario, provide:
        1. Key market assumptions (penetration rate, pricing, competitive response)
        2. Annual revenue projections for 6 years (2024-2029) in millions USD
        3. Peak sales estimate and timing
        4. Market share trajectory over time
        5. Critical success or failure factors
        
        Consider these factors:
        - Current treatment landscape from analysis
        - Competitive environment
        - Regulatory pathway complexity
        - Market access challenges
        - Pricing pressures
        
        For revenue projections, consider:
        - Optimistic: Strong adoption, premium pricing, limited competition
        - Realistic: Moderate uptake, competitive pricing, some competition  
        - Pessimistic: Slow adoption, pricing pressure, strong competition
        
        Provide specific numbers - not ranges. Make projections realistic for {therapy_area}.
        """
        
        response = await chat.send_message(UserMessage(text=prompt))
        
        # Parse the response to extract scenario data
        scenarios_data = {}
        lines = response.split('\n')
        current_scenario = None
        current_section = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # Identify scenario sections
            for scenario in scenarios:
                if scenario.lower() in line.lower() and any(word in line.lower() for word in ['scenario', 'case', ':']):
                    current_scenario = scenario
                    if current_scenario not in scenarios_data:
                        scenarios_data[current_scenario] = {
                            "assumptions": [],
                            "projections": [],
                            "peak_sales": 500,
                            "market_share_trajectory": [2, 5, 8, 12, 15, 13],
                            "key_factors": []
                        }
                    break
            
            if current_scenario:
                # Extract projections (look for years and dollar amounts)
                if any(year in line for year in ['2024', '2025', '2026', '2027', '2028', '2029']):
                    import re
                    amounts = re.findall(r'\$?(\d+(?:\.\d+)?)\s*[mM]?', line)
                    for amount in amounts:
                        try:
                            val = float(amount)
                            if val > 0 and val < 10000:  # Reasonable range
                                scenarios_data[current_scenario]["projections"].append(int(val))
                        except:
                            continue
                
                # Extract key assumptions and factors
                if any(word in line.lower() for word in ['assumption', 'factor', 'driver', 'key']):
                    clean_line = line.replace('-', '').replace('', '').strip()
                    if len(clean_line) > 10:
                        if 'assumption' in line.lower():
                            scenarios_data[current_scenario]["assumptions"].append(clean_line[:100])
                        else:
                            scenarios_data[current_scenario]["key_factors"].append(clean_line[:100])
                
                # Extract peak sales
                if 'peak' in line.lower() and any(char in line for char in ['$', 'million', 'M']):
                    import re
                    peak_match = re.search(r'\$?(\d+(?:\.\d+)?)\s*[mM]?', line)
                    if peak_match:
                        try:
                            scenarios_data[current_scenario]["peak_sales"] = int(float(peak_match.group(1)))
                        except:
                            pass
        
        # Ensure we have data for all scenarios with realistic defaults
        base_projections = {
            'optimistic': [50, 150, 350, 600, 800, 750],
            'realistic': [25, 75, 200, 400, 500, 450],
            'pessimistic': [10, 30, 80, 150, 200, 180]
        }
        
        for i, scenario in enumerate(scenarios):
            if scenario not in scenarios_data:
                scenarios_data[scenario] = {}
            
            # Ensure projections exist
            if not scenarios_data[scenario].get("projections"):
                scenarios_data[scenario]["projections"] = base_projections.get(scenario, [100, 250, 400, 500, 450, 400])
            
            # Ensure we have 6 projections
            projections = scenarios_data[scenario]["projections"]
            while len(projections) < 6:
                if len(projections) == 0:
                    projections.append(base_projections.get(scenario, [100])[0])
                else:
                    # Extrapolate based on trend
                    if len(projections) >= 2:
                        growth = projections[-1] - projections[-2]
                        next_val = max(0, projections[-1] + growth * 0.8)  # Diminishing growth
                    else:
                        next_val = projections[-1] * 1.5
                    projections.append(int(next_val))
            
            # Limit to 6 years
            scenarios_data[scenario]["projections"] = projections[:6]
            
            # Set default values if missing
            if not scenarios_data[scenario].get("assumptions"):
                scenarios_data[scenario]["assumptions"] = [f"{scenario.title()} market conditions and adoption"]
            
            if not scenarios_data[scenario].get("key_factors"):
                scenarios_data[scenario]["key_factors"] = [f"Successful {scenario} execution"]
            
            if not scenarios_data[scenario].get("peak_sales"):
                scenarios_data[scenario]["peak_sales"] = max(scenarios_data[scenario]["projections"])
            
            if not scenarios_data[scenario].get("market_share_trajectory"):
                scenarios_data[scenario]["market_share_trajectory"] = [2, 5, 8, 12, 15, 13]
            
            # Add full analysis
            scenarios_data[scenario]["full_analysis"] = response
        
        return scenarios_data
        
    except Exception as e:
        logging.error(f"Scenario modeling error: {str(e)}")
        # Return fallback data
        fallback = {}
        base_projections = [100, 250, 500, 750, 900, 800]
        
        for i, scenario in enumerate(scenarios):
            multiplier = [0.6, 1.0, 1.8][min(i, 2)]  # pessimistic, realistic, optimistic
            fallback[scenario] = {
                "assumptions": [f"{scenario.title()} market scenario with moderate competition"],
                "projections": [int(p * multiplier) for p in base_projections],
                "peak_sales": int(900 * multiplier),
                "market_share_trajectory": [2, 5, 8, 12, 15, 13],
                "key_factors": [f"{scenario.title()} market execution and adoption"],
                "full_analysis": f"Error in analysis generation: {str(e)}"
            }
        return fallback

# Export Functions
def generate_pdf_report(analysis: dict, funnel: dict = None):
    """Generate comprehensive PDF report"""
    try:
        buffer = BytesIO()
        doc = SimpleDocTemplate(buffer, pagesize=A4)
        styles = getSampleStyleSheet()
        story = []
        
        # Title
        title_style = ParagraphStyle(
            'CustomTitle',
            parent=styles['Heading1'],
            fontSize=24,
            spaceAfter=30,
            textColor=colors.darkblue
        )
        story.append(Paragraph(f"Pharma Analysis Report: {analysis['therapy_area']}", title_style))
        story.append(Spacer(1, 20))
        
        # Executive Summary
        story.append(Paragraph("Executive Summary", styles['Heading2']))
        summary_text = analysis.get('disease_summary', '')[:500] + "..."
        story.append(Paragraph(summary_text, styles['Normal']))
        story.append(Spacer(1, 20))
        
        # Key Sections
        sections = [
            ("Disease Overview", analysis.get('disease_summary', '')),
            ("Staging Information", analysis.get('staging', '')),
            ("Biomarkers", analysis.get('biomarkers', '')),
            ("Treatment Algorithm", analysis.get('treatment_algorithm', '')),
            ("Patient Journey", analysis.get('patient_journey', ''))
        ]
        
        for section_title, content in sections:
            if content:
                story.append(Paragraph(section_title, styles['Heading3']))
                # Truncate content for PDF
                truncated_content = content[:1000] + "..." if len(content) > 1000 else content
                story.append(Paragraph(truncated_content, styles['Normal']))
                story.append(Spacer(1, 12))
        
        # Competitive Analysis
        if analysis.get('competitive_landscape'):
            story.append(Paragraph("Competitive Landscape", styles['Heading2']))
            comp_data = analysis['competitive_landscape']
            if isinstance(comp_data, dict) and 'competitors' in comp_data:
                for comp in comp_data['competitors'][:5]:  # Top 5
                    comp_text = f" {comp.get('name', 'Unknown')}: {comp.get('strengths', 'Market presence')}"
                    story.append(Paragraph(comp_text, styles['Normal']))
            story.append(Spacer(1, 20))
        
        # Risk Assessment
        if analysis.get('risk_assessment'):
            story.append(Paragraph("Risk Assessment", styles['Heading2']))
            risk_data = analysis['risk_assessment']
            if isinstance(risk_data, dict):
                for risk_type, risk_info in risk_data.items():
                    if isinstance(risk_info, dict) and 'level' in risk_info:
                        story.append(Paragraph(f" {risk_type.replace('_', ' ').title()}: {risk_info['level']}", styles['Normal']))
            story.append(Spacer(1, 20))
        
        doc.build(story)
        buffer.seek(0)
        return base64.b64encode(buffer.getvalue()).decode()
        
    except Exception as e:
        logging.error(f"PDF generation error: {str(e)}")
        return None

def generate_excel_export(analysis: dict, funnel: dict = None):
    """Generate Excel forecasting model"""
    try:
        buffer = BytesIO()
        wb = openpyxl.Workbook()
        
        # Analysis Summary Sheet
        ws1 = wb.active
        ws1.title = "Analysis Summary"
        
        # Headers
        header_font = Font(bold=True, size=14)
        ws1['A1'] = f"Therapy Area Analysis: {analysis['therapy_area']}"
        ws1['A1'].font = header_font
        
        row = 3
        sections = [
            ("Disease Summary", analysis.get('disease_summary', '')[:500]),
            ("Key Biomarkers", analysis.get('biomarkers', '')[:300]),
            ("Treatment Algorithm", analysis.get('treatment_algorithm', '')[:300])
        ]
        
        for title, content in sections:
            ws1[f'A{row}'] = title
            ws1[f'A{row}'].font = Font(bold=True)
            ws1[f'B{row}'] = content
            row += 2
        
        # Funnel Data Sheet
        if funnel and 'funnel_stages' in funnel:
            ws2 = wb.create_sheet("Patient Flow Funnel")
            ws2['A1'] = "Stage"
            ws2['B1'] = "Percentage"
            ws2['C1'] = "Description"
            
            for i, stage in enumerate(funnel['funnel_stages'], 2):
                ws2[f'A{i}'] = stage.get('stage', '')
                ws2[f'B{i}'] = stage.get('percentage', '')
                ws2[f'C{i}'] = stage.get('description', '')
        
        # Scenario Models Sheet
        if analysis.get('scenario_models'):
            ws3 = wb.create_sheet("Scenario Models")
            ws3['A1'] = "Scenario"
            for year in range(2024, 2030):
                ws3[f'{chr(66+year-2024)}1'] = str(year)
            
            row = 2
            for scenario, data in analysis['scenario_models'].items():
                ws3[f'A{row}'] = scenario.title()
                if 'projections' in data:
                    for i, projection in enumerate(data['projections'][:6]):
                        ws3[f'{chr(66+i)}{row}'] = projection
                row += 1
        
        wb.save(buffer)
        buffer.seek(0)
        return base64.b64encode(buffer.getvalue()).decode()
        
    except Exception as e:
        logging.error(f"Excel generation error: {str(e)}")
        return None

@api_router.post("/financial-model", response_model=FinancialModel)
async def create_financial_model(request: FinancialModelRequest):
    """Generate advanced financial model with NPV, IRR, and Monte Carlo analysis"""
    try:
        params = {
            "discount_rate": request.discount_rate,
            "peak_sales_estimate": request.peak_sales_estimate,
            "launch_year": request.launch_year,
            "patent_expiry_year": request.patent_expiry_year,
            "ramp_up_years": request.ramp_up_years,
            "monte_carlo_iterations": request.monte_carlo_iterations
        }
        
        financial_model = await generate_financial_model(
            therapy_area=request.therapy_area,
            product_name=request.product_name,
            analysis_id=request.analysis_id,
            params=params,
            api_key=request.api_key
        )
        
        # Store in database
        await db.financial_models.insert_one(financial_model.dict())
        
        return financial_model
        
    except Exception as e:
        logger.error(f"Financial model endpoint error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Financial modeling failed: {str(e)}")

@api_router.post("/timeline", response_model=Timeline)
async def create_timeline(request: TimelineRequest):
    """Generate interactive milestone timeline"""
    try:
        timeline = await generate_timeline(
            therapy_area=request.therapy_area,
            product_name=request.product_name,
            analysis_id=request.analysis_id,
            include_competitive=request.include_competitive_milestones,
            api_key=request.api_key
        )
        
        # Store in database
        await db.timelines.insert_one(timeline.dict())
        
        return timeline
        
    except Exception as e:
        logger.error(f"Timeline endpoint error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Timeline generation failed: {str(e)}")

@api_router.post("/custom-template", response_model=CustomTemplate)
async def create_custom_template(request: TemplateRequest):
    """Generate custom analysis templates"""
    try:
        template = await generate_custom_template(
            template_type=request.template_type,
            therapy_area=request.therapy_area or "General",
            region=request.region or "Global", 
            api_key=request.api_key
        )
        
        # Store in database
        await db.custom_templates.insert_one(template.dict())
        
        return template
        
    except Exception as e:
        logger.error(f"Template endpoint error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Template generation failed: {str(e)}")

@api_router.get("/templates")
async def get_available_templates(template_type: Optional[str] = None):
    """Get available custom templates"""
    try:
        query = {"template_type": template_type} if template_type else {}
        templates = await db.custom_templates.find(query).sort("created_at", -1).limit(20).to_list(20)
        
        return {
            "templates": [
                {
                    "id": str(template["_id"]),
                    "template_type": template["template_type"],
                    "therapy_area": template.get("therapy_area"),
                    "region": template.get("region"),
                    "sections_count": len(template.get("sections", [])),
                    "created_at": template["created_at"]
                } for template in templates
            ],
            "total_count": len(templates)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@api_router.get("/financial-model/{analysis_id}")
async def get_financial_model(analysis_id: str):
    """Retrieve financial model for analysis"""
    try:
        model = await db.financial_models.find_one({"analysis_id": analysis_id})
        if not model:
            raise HTTPException(status_code=404, detail="Financial model not found")
        return FinancialModel(**model)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@api_router.get("/timeline/{analysis_id}")
async def get_timeline(analysis_id: str):
    """Retrieve timeline for analysis"""
    try:
        timeline = await db.timelines.find_one({"analysis_id": analysis_id})
        if not timeline:
            raise HTTPException(status_code=404, detail="Timeline not found")
        return Timeline(**timeline)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@api_router.post("/advanced-visualization")
async def create_advanced_visualization(
    visualization_type: str,
    data_source: str,  # "competitive", "financial", "market"
    analysis_id: str
):
    """Generate advanced 2D visualizations"""
    try:
        if data_source == "competitive":
            # Get competitive data
            analysis = await db.therapy_analyses.find_one({"id": analysis_id})
            if not analysis or not analysis.get("competitive_landscape"):
                raise HTTPException(status_code=404, detail="Competitive data not found")
            
            if visualization_type == "positioning_map":
                chart_data = create_competitive_positioning_map(analysis["competitive_landscape"])
            else:
                chart_data = json.dumps({"error": "Unsupported visualization type"})
        
        elif data_source == "financial":
            # Get financial data
            financial_model = await db.financial_models.find_one({"analysis_id": analysis_id})
            if not financial_model:
                raise HTTPException(status_code=404, detail="Financial model not found")
            
            if visualization_type == "risk_return":
                # Get scenario data for risk-return analysis
                scenarios = financial_model.get("monte_carlo_results", {})
                chart_data = create_risk_return_scatter({"scenarios": scenarios})
            else:
                chart_data = json.dumps({"error": "Unsupported visualization type"})
        
        elif data_source == "market":
            # Get market data
            analysis = await db.therapy_analyses.find_one({"id": analysis_id})
            if not analysis:
                raise HTTPException(status_code=404, detail="Analysis not found")
            
            if visualization_type == "market_heatmap":
                chart_data = create_market_evolution_heatmap(analysis["therapy_area"])
            else:
                chart_data = json.dumps({"error": "Unsupported visualization type"})
        
        else:
            raise HTTPException(status_code=400, detail="Invalid data source")
        
        return {
            "visualization_type": visualization_type,
            "data_source": data_source,
            "chart_data": chart_data,
            "generated_at": datetime.utcnow()
        }
        
    except Exception as e:
        logger.error(f"Advanced visualization error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Visualization generation failed: {str(e)}")

@api_router.post("/ensemble-analysis", response_model=EnsembleResult)
async def ensemble_analysis_endpoint(request: EnsembleAnalysisRequest):
    """Advanced multi-model AI ensemble analysis"""
    try:
        ensemble_result = await run_ensemble_analysis(
            therapy_area=request.therapy_area,
            product_name=request.product_name,
            analysis_type=request.analysis_type,
            claude_key=request.claude_api_key,
            perplexity_key=request.perplexity_api_key,
            gemini_key=request.gemini_api_key,
            use_gemini=request.use_gemini
        )
        
        # Store ensemble result in database
        await db.ensemble_analyses.insert_one({
            "therapy_area": request.therapy_area,
            "product_name": request.product_name,
            "analysis_type": request.analysis_type,
            "ensemble_result": ensemble_result.dict(),
            "timestamp": datetime.utcnow(),
            "model_agreement_score": ensemble_result.model_agreement_score,
            "models_used": list(ensemble_result.confidence_scores.keys())
        })
        
        return ensemble_result
        
    except Exception as e:
        logger.error(f"Ensemble analysis endpoint error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Ensemble analysis failed: {str(e)}")

@api_router.get("/ensemble-history")
async def get_ensemble_history(limit: int = 10):
    """Retrieve ensemble analysis history"""
    try:
        results = await db.ensemble_analyses.find().sort("timestamp", -1).limit(limit).to_list(limit)
        return {
            "ensemble_analyses": [
                {
                    "id": str(result["_id"]),
                    "therapy_area": result["therapy_area"],
                    "product_name": result.get("product_name"),
                    "analysis_type": result["analysis_type"],
                    "model_agreement_score": result["model_agreement_score"],
                    "models_used": result["models_used"],
                    "timestamp": result["timestamp"]
                } for result in results
            ],
            "total_count": len(results)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@api_router.post("/company-intelligence", response_model=CompanyIntelligence)
async def company_intelligence_endpoint(request: CompanyIntelRequest):
    """Generate comprehensive company intelligence from product name"""
    try:
        intelligence = await generate_company_intelligence(
            product_name=request.product_name,
            therapy_area=request.therapy_area,
            perplexity_key=request.api_key,
            include_competitors=request.include_competitors
        )
        
        # Store intelligence in database
        await db.company_intelligence.insert_one({
            "product_name": request.product_name,
            "intelligence": intelligence.dict(),
            "timestamp": datetime.utcnow(),
            "therapy_area": request.therapy_area
        })
        
        return intelligence
        
    except Exception as e:
        logger.error(f"Company intelligence endpoint error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Company intelligence failed: {str(e)}")

@api_router.get("/company-intelligence/{product_name}")
async def get_company_intelligence(product_name: str):
    """Retrieve stored company intelligence"""
    try:
        result = await db.company_intelligence.find_one(
            {"product_name": {"$regex": product_name, "$options": "i"}},
            sort=[("timestamp", -1)]
        )
        
        if result:
            return CompanyIntelligence(**result["intelligence"])
        else:
            raise HTTPException(status_code=404, detail="Company intelligence not found")
            
    except Exception as e:
        logger.error(f"Company intelligence retrieval error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@api_router.post("/perplexity-search", response_model=PerplexityResult)
async def perplexity_search_endpoint(request: PerplexityRequest):
    """Real-time pharmaceutical intelligence search using Perplexity API"""
    try:
        result = await search_with_perplexity(
            query=request.query,
            api_key=request.api_key,
            search_focus=request.search_focus
        )
        
        # Store result in database for future reference
        await db.perplexity_searches.insert_one({
            "query": request.query,
            "result": result.dict(),
            "timestamp": datetime.utcnow(),
            "search_focus": request.search_focus
        })
        
        return result
        
    except Exception as e:
        logger.error(f"Perplexity search endpoint error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Search failed: {str(e)}")

@api_router.post("/enhanced-competitive-analysis")
async def enhanced_competitive_intel(request: CompetitiveAnalysisRequest):
    """Enhanced competitive analysis combining Perplexity real-time data with Claude insights"""
    try:
        analysis = await db.therapy_analyses.find_one({"id": request.analysis_id})
        if not analysis:
            raise HTTPException(status_code=404, detail="Analysis not found")
        
        # For this enhanced version, we need both Perplexity and Claude API keys
        # For now, using the same key - in production, you'd want separate keys
        enhanced_data = await enhanced_competitive_analysis_with_perplexity(
            therapy_area=request.therapy_area,
            product_name=analysis.get('product_name', ''),
            perplexity_key=request.api_key,  # Assuming same key for now
            claude_key=request.api_key
        )
        
        # Update analysis with enhanced competitive intelligence
        await db.therapy_analyses.update_one(
            {"id": request.analysis_id},
            {"$set": {
                "competitive_landscape": enhanced_data,
                "updated_at": datetime.utcnow()
            }}
        )
        
        return {
            "status": "success",
            "competitive_landscape": enhanced_data,
            "updated_at": datetime.utcnow(),
            "analysis_type": "Enhanced with Real-time Intelligence"
        }
        
    except Exception as e:
        logger.error(f"Enhanced competitive analysis error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Enhanced analysis failed: {str(e)}")

# Phase 3: Real-World Evidence Integration Functions
async def generate_real_world_evidence(therapy_area: str, product_name: str, analysis_type: str, data_sources: List[str], api_key: str) -> RealWorldEvidence:
    """Generate comprehensive Real-World Evidence analysis"""
    try:
        from emergentintegrations.llm.chat import LlmChat, UserMessage
        
        chat = LlmChat(
            api_key=api_key,
            session_id=f"rwe_{uuid.uuid4()}",
            system_message="You are a pharmaceutical real-world evidence expert specializing in effectiveness studies, safety monitoring, and health economics."
        ).with_model("anthropic", "claude-sonnet-4-20250514").with_max_tokens(4096)
        
        # Create data source specific prompts
        data_source_descriptions = {
            "registries": "patient registries and disease-specific databases",
            "claims": "insurance claims and administrative databases", 
            "ehr": "electronic health records and clinical documentation",
            "patient_outcomes": "patient-reported outcomes and quality of life studies"
        }
        
        sources_text = ", ".join([data_source_descriptions.get(ds, ds) for ds in data_sources])
        product_context = f" for {product_name}" if product_name else ""
        
        prompt = f"""
        Generate a comprehensive Real-World Evidence analysis for {therapy_area}{product_context} using {sources_text}.
        
        Analysis Type: {analysis_type}
        
        Provide detailed analysis for each section:
        
        1. EFFECTIVENESS DATA:
        - Real-world effectiveness vs clinical trial efficacy
        - Response rates in diverse populations  
        - Time to response and duration of treatment
        - Comparative effectiveness vs standard of care
        - Subgroup analyses (age, gender, comorbidities)
        
        2. SAFETY PROFILE:
        - Adverse event rates in real-world settings
        - Safety signals not seen in trials
        - Long-term safety monitoring data
        - Drug interactions and contraindications
        - Special population safety considerations
        
        3. PATIENT OUTCOMES:
        - Quality of life improvements
        - Functional status changes
        - Patient-reported outcome measures (PROMs)
        - Treatment satisfaction scores
        - Return to work/normal activities
        
        4. REAL-WORLD PERFORMANCE:
        - Treatment adherence and persistence rates
        - Dose modifications and discontinuation reasons  
        - Healthcare resource utilization
        - Hospital readmission rates
        - Emergency department visits
        
        5. COMPARATIVE EFFECTIVENESS:
        - Head-to-head comparisons with competitors
        - Relative effectiveness in matched cohorts
        - Time-to-treatment failure comparisons
        - Switching patterns between treatments
        
        6. COST-EFFECTIVENESS:
        - Real-world cost per QALY
        - Budget impact on healthcare systems
        - Direct and indirect cost comparisons
        - Value-based care metrics
        
        7. ADHERENCE PATTERNS:
        - Medication adherence rates over time
        - Factors affecting adherence
        - Impact of adherence on outcomes
        - Support program effectiveness
        
        8. HEALTH ECONOMICS DATA:
        - Healthcare cost offsets
        - Productivity gains
        - Caregiver burden reduction
        - Long-term economic impact
        
        Provide specific metrics, percentages, and quantitative data where possible. Include limitations and data quality considerations.
        Rate the overall evidence quality on a scale of 0-1.
        """
        
        response = await chat.send_message(UserMessage(text=prompt))
        
        # Parse response into structured data
        effectiveness_data = extract_section_data(response, "EFFECTIVENESS")
        safety_profile = extract_section_data(response, "SAFETY")
        patient_outcomes = extract_section_data(response, "PATIENT OUTCOMES")
        real_world_performance = extract_section_data(response, "REAL-WORLD PERFORMANCE")
        comparative_effectiveness = extract_comparative_data(response)
        cost_effectiveness = extract_section_data(response, "COST-EFFECTIVENESS")
        adherence_patterns = extract_section_data(response, "ADHERENCE")
        health_economics_data = extract_section_data(response, "HEALTH ECONOMICS")
        
        # Calculate evidence quality score
        evidence_quality_score = calculate_evidence_quality(response, data_sources)
        
        return RealWorldEvidence(
            therapy_area=therapy_area,
            product_name=product_name,
            effectiveness_data=effectiveness_data,
            safety_profile=safety_profile,
            patient_outcomes=patient_outcomes,
            real_world_performance=real_world_performance,
            comparative_effectiveness=comparative_effectiveness,
            cost_effectiveness=cost_effectiveness,
            adherence_patterns=adherence_patterns,
            health_economics_data=health_economics_data,
            evidence_quality_score=evidence_quality_score,
            data_sources=data_sources,
            study_populations=extract_population_data(response),
            limitations=extract_limitations(response),
            recommendations=extract_recommendations(response)
        )
        
    except Exception as e:
        logging.error(f"RWE generation error: {str(e)}")
        # Return minimal structure on error
        return RealWorldEvidence(
            therapy_area=therapy_area,
            product_name=product_name,
            effectiveness_data={"error": str(e)},
            safety_profile={"error": str(e)},
            patient_outcomes={"error": str(e)},
            real_world_performance={"error": str(e)},
            comparative_effectiveness=[],
            cost_effectiveness={"error": str(e)},
            adherence_patterns={"error": str(e)},
            health_economics_data={"error": str(e)},
            evidence_quality_score=0.0,
            data_sources=data_sources,
            study_populations={},
            limitations=["Data generation error"],
            recommendations=["Unable to generate recommendations due to error"]
        )

async def generate_market_access_intelligence(therapy_area: str, product_name: str, target_markets: List[str], analysis_depth: str, api_key: str) -> MarketAccessIntelligence:
    """Generate comprehensive Market Access Intelligence"""
    try:
        # First get real-time market intelligence via Perplexity
        perplexity_results = []
        for market in target_markets:
            market_query = f"{therapy_area} {product_name} market access reimbursement {market} payer landscape 2024"
            perplexity_result = await search_with_perplexity(market_query, api_key, "market_access")
            perplexity_results.append(perplexity_result)
        
        # Combine Perplexity intelligence
        real_time_data = "\n\n".join([result.content for result in perplexity_results])
        
        from emergentintegrations.llm.chat import LlmChat, UserMessage
        
        chat = LlmChat(
            api_key=api_key,
            session_id=f"market_access_{uuid.uuid4()}",
            system_message="You are a pharmaceutical market access expert specializing in reimbursement, pricing, and payer strategies."
        ).with_model("anthropic", "claude-sonnet-4-20250514").with_max_tokens(4096)
        
        markets_text = ", ".join(target_markets)
        product_context = f" for {product_name}" if product_name else ""
        
        prompt = f"""
        Generate comprehensive Market Access Intelligence for {therapy_area}{product_context} across {markets_text}.
        
        Analysis Depth: {analysis_depth}
        
        Use this real-time market intelligence:
        {real_time_data}
        
        Provide detailed analysis for each section:
        
        1. PAYER LANDSCAPE:
        - Key payers and market share by region
        - Decision-making processes and timelines
        - Key decision makers and influencers
        - Budget allocation and spending patterns
        - Reimbursement mechanisms (fee-for-service vs value-based)
        
        2. REIMBURSEMENT PATHWAYS:
        - Standard coverage determination processes
        - Prior authorization requirements
        - Step therapy and fail-first policies
        - Appeals processes and success rates
        - Coverage criteria and medical policies
        
        3. PRICING ANALYSIS:
        - Reference pricing systems
        - Price benchmarking across markets
        - Discount and rebate expectations
        - Value-based pricing opportunities
        - Health technology assessment requirements
        
        4. ACCESS BARRIERS:
        - Administrative barriers (prior auth, step therapy)
        - Clinical barriers (restricted indications)
        - Economic barriers (copay, coinsurance)
        - Infrastructure barriers (specialty pharmacy)
        - Provider barriers (lack of awareness, training)
        
        5. HEOR REQUIREMENTS:
        - Health economic evaluation standards
        - Required clinical endpoints
        - Budget impact model requirements
        - Cost-effectiveness thresholds
        - Real-world evidence expectations
        
        6. REGULATORY PATHWAYS:
        - Expedited review pathways available
        - Regulatory timeline expectations
        - Post-market commitments
        - Risk evaluation and mitigation strategies
        - Companion diagnostic requirements
        
        7. APPROVAL TIMELINES:
        - Average time from submission to decision
        - Expedited pathway timelines
        - Resubmission rates and reasons
        - Key regulatory milestones
        
        8. FORMULARY PLACEMENT:
        - Tier placement expectations
        - Formulary committee processes
        - Coverage policy development timelines
        - Preferred vs non-preferred status factors
        
        9. BUDGET IMPACT MODELS:
        - Budget impact thresholds
        - Model validation requirements
        - Sensitivity analysis expectations
        - Time horizon requirements
        
        10. STAKEHOLDER MAPPING:
        - Key opinion leaders and their influence
        - Patient advocacy groups
        - Professional medical societies
        - Regulatory agencies and their priorities
        
        Provide specific metrics, timelines, and actionable insights. Calculate a market readiness score (0-1).
        """
        
        response = await chat.send_message(UserMessage(text=prompt))
        
        # Parse response into structured data
        payer_landscape = extract_section_data(response, "PAYER LANDSCAPE")
        reimbursement_pathways = extract_section_data(response, "REIMBURSEMENT PATHWAYS")
        pricing_analysis = extract_section_data(response, "PRICING ANALYSIS")
        access_barriers = extract_barriers_data(response)
        heor_requirements = extract_section_data(response, "HEOR REQUIREMENTS")
        regulatory_pathways = extract_section_data(response, "REGULATORY PATHWAYS")
        approval_timelines = extract_timelines_data(response)
        formulary_placement = extract_section_data(response, "FORMULARY PLACEMENT")
        budget_impact_models = extract_section_data(response, "BUDGET IMPACT")
        coverage_policies = extract_policies_data(response)
        stakeholder_mapping = extract_section_data(response, "STAKEHOLDER MAPPING")
        
        # Calculate market readiness score
        market_readiness_score = calculate_market_readiness(response, target_markets)
        
        return MarketAccessIntelligence(
            therapy_area=therapy_area,
            product_name=product_name,
            payer_landscape=payer_landscape,
            reimbursement_pathways=reimbursement_pathways,
            pricing_analysis=pricing_analysis,
            access_barriers=access_barriers,
            heor_requirements=heor_requirements,
            regulatory_pathways=regulatory_pathways,
            approval_timelines=approval_timelines,
            formulary_placement=formulary_placement,
            budget_impact_models=budget_impact_models,
            coverage_policies=coverage_policies,
            stakeholder_mapping=stakeholder_mapping,
            market_readiness_score=market_readiness_score,
            recommendations=extract_recommendations(response)
        )
        
    except Exception as e:
        logging.error(f"Market access intelligence error: {str(e)}")
        # Return minimal structure on error
        return MarketAccessIntelligence(
            therapy_area=therapy_area,
            product_name=product_name,
            payer_landscape={"error": str(e)},
            reimbursement_pathways={"error": str(e)},
            pricing_analysis={"error": str(e)},
            access_barriers=[],
            heor_requirements={"error": str(e)},
            regulatory_pathways={"error": str(e)},
            approval_timelines={"error": str(e)},
            formulary_placement={"error": str(e)},
            budget_impact_models={"error": str(e)},
            coverage_policies=[],
            stakeholder_mapping={"error": str(e)},
            market_readiness_score=0.0,
            recommendations=["Unable to generate recommendations due to error"]
        )

async def generate_predictive_analytics(therapy_area: str, product_name: str, forecast_horizon: int, model_type: str, include_rwe: bool, api_key: str) -> PredictiveAnalytics:
    """Generate advanced Predictive Analytics with ML-enhanced forecasting"""
    try:
        from emergentintegrations.llm.chat import LlmChat, UserMessage
        
        chat = LlmChat(
            api_key=api_key,
            session_id=f"predictive_{uuid.uuid4()}",
            system_message="You are a pharmaceutical forecasting expert specializing in predictive analytics, machine learning models, and advanced market modeling."
        ).with_model("anthropic", "claude-sonnet-4-20250514").with_max_tokens(4096)
        
        product_context = f" for {product_name}" if product_name else ""
        rwe_context = " incorporating real-world evidence" if include_rwe else ""
        
        prompt = f"""
        Generate advanced Predictive Analytics for {therapy_area}{product_context} with {forecast_horizon}-year horizon using {model_type} modeling{rwe_context}.
        
        Provide detailed analysis for each section:
        
        1. MARKET PENETRATION FORECAST:
        - Year-over-year penetration rates (next {forecast_horizon} years)
        - Patient adoption curves and S-curve modeling
        - Geographic rollout predictions
        - Market saturation analysis
        - Penetration by patient segments
        
        2. COMPETITIVE RESPONSE MODELING:
        - Competitor launch impact predictions
        - Price erosion scenarios
        - Market share cannibalization models
        - Defensive strategy effectiveness
        - New entrant threat assessment
        
        3. PATIENT FLOW PREDICTIONS:
        - Treatment-naive patient flow forecasts
        - Switching behavior predictions
        - Treatment sequence modeling
        - Duration of therapy projections
        - Discontinuation rate forecasts
        
        4. REVENUE FORECASTS:
        - Annual revenue projections ({forecast_horizon} years)
        - Peak sales timing and magnitude
        - Price evolution modeling
        - Volume vs price contribution
        - Revenue by indication/geography
        
        5. RISK-ADJUSTED PROJECTIONS:
        - Monte Carlo simulation results (1000+ scenarios)
        - Confidence intervals (80%, 95%)
        - Downside/upside risk quantification
        - Stress testing scenarios
        - Value-at-risk calculations
        
        6. SCENARIO PROBABILITIES:
        - Bull case probability and drivers
        - Base case probability and assumptions
        - Bear case probability and risks
        - Black swan event impacts
        - Scenario transition probabilities
        
        7. KEY ASSUMPTIONS:
        - Clinical success probabilities
        - Regulatory approval assumptions
        - Pricing and reimbursement assumptions
        - Market access timeline assumptions
        - Competitive landscape assumptions
        
        8. SENSITIVITY FACTORS:
        - Price sensitivity analysis
        - Market size sensitivity
        - Penetration rate sensitivity
        - Launch timing sensitivity
        - Competition sensitivity
        
        9. MODEL PERFORMANCE METRICS:
        - Historical forecasting accuracy
        - Prediction intervals
        - Model calibration scores
        - Cross-validation results
        - Ensemble model weights
        
        10. UNCERTAINTY ANALYSIS:
        - Parameter uncertainty quantification
        - Model uncertainty assessment
        - Scenario uncertainty evaluation
        - Total forecast uncertainty
        - Risk decomposition analysis
        
        Provide specific numbers, percentages, and quantitative projections. Include confidence levels and uncertainty ranges.
        """
        
        response = await chat.send_message(UserMessage(text=prompt))
        
        # Parse response into structured data
        market_penetration_forecast = extract_forecast_data(response, "MARKET PENETRATION")
        competitive_response_modeling = extract_section_data(response, "COMPETITIVE RESPONSE")
        patient_flow_predictions = extract_section_data(response, "PATIENT FLOW")
        revenue_forecasts = extract_revenue_data(response, forecast_horizon)
        risk_adjusted_projections = extract_risk_data(response)
        scenario_probabilities = extract_scenario_probabilities(response)
        confidence_intervals = extract_confidence_intervals(response)
        key_assumptions = extract_assumptions(response)
        sensitivity_factors = extract_sensitivity_data(response)
        model_performance_metrics = extract_performance_metrics(response)
        uncertainty_analysis = extract_uncertainty_data(response)
        
        return PredictiveAnalytics(
            therapy_area=therapy_area,
            product_name=product_name,
            market_penetration_forecast=market_penetration_forecast,
            competitive_response_modeling=competitive_response_modeling,
            patient_flow_predictions=patient_flow_predictions,
            revenue_forecasts=revenue_forecasts,
            risk_adjusted_projections=risk_adjusted_projections,
            scenario_probabilities=scenario_probabilities,
            confidence_intervals=confidence_intervals,
            key_assumptions=key_assumptions,
            sensitivity_factors=sensitivity_factors,
            model_performance_metrics=model_performance_metrics,
            uncertainty_analysis=uncertainty_analysis,
            recommendations=extract_recommendations(response)
        )
        
    except Exception as e:
        logging.error(f"Predictive analytics error: {str(e)}")
        # Return minimal structure on error
        return PredictiveAnalytics(
            therapy_area=therapy_area,
            product_name=product_name,
            market_penetration_forecast={"error": str(e)},
            competitive_response_modeling={"error": str(e)},
            patient_flow_predictions={"error": str(e)},
            revenue_forecasts={"error": str(e)},
            risk_adjusted_projections={"error": str(e)},
            scenario_probabilities={},
            confidence_intervals={"error": str(e)},
            key_assumptions=["Error in data generation"],
            sensitivity_factors={},
            model_performance_metrics={},
            uncertainty_analysis={"error": str(e)},
            recommendations=["Unable to generate recommendations due to error"]
        )

# Helper functions for data extraction
def extract_section_data(response: str, section_name: str) -> Dict[str, Any]:
    """Extract structured data from a specific section"""
    lines = response.split('\n')
    section_started = False
    section_data = {"content": "", "key_points": []}
    
    for line in lines:
        if section_name.upper() in line.upper():
            section_started = True
            continue
        elif section_started and any(other_section in line.upper() for other_section in 
                                   ["EFFECTIVENESS", "SAFETY", "OUTCOMES", "PERFORMANCE", "COMPARATIVE", 
                                    "COST", "ADHERENCE", "ECONOMICS", "PAYER", "REIMBURSEMENT", 
                                    "PRICING", "ACCESS", "HEOR", "REGULATORY", "APPROVAL", "FORMULARY",
                                    "MARKET PENETRATION", "COMPETITIVE RESPONSE", "PATIENT FLOW", "REVENUE"]):
            break
        elif section_started:
            if line.strip():
                section_data["content"] += line + "\n"
                if line.strip().startswith(('-', '', '*')) or any(char.isdigit() for char in line[:3]):
                    section_data["key_points"].append(line.strip())
    
    return section_data

def extract_comparative_data(response: str) -> List[Dict[str, Any]]:
    """Extract comparative effectiveness data"""
    comparisons = []
    lines = response.split('\n')
    
    for line in lines:
        if 'vs' in line.lower() or 'compared to' in line.lower():
            comparisons.append({
                "comparison": line.strip(),
                "outcome": "Not specified",
                "confidence": 0.6
            })
    
    return comparisons[:10]  # Limit to top 10

def extract_barriers_data(response: str) -> List[Dict[str, str]]:
    """Extract access barriers data"""
    barriers = []
    lines = response.split('\n')
    barrier_started = False
    
    for line in lines:
        if "ACCESS BARRIERS" in line.upper() or "BARRIERS" in line.upper():
            barrier_started = True
            continue
        elif barrier_started and any(section in line.upper() for section in ["HEOR", "REGULATORY", "APPROVAL"]):
            break
        elif barrier_started and line.strip():
            if line.strip().startswith(('-', '', '*')):
                barrier_type = "administrative"
                if 'clinical' in line.lower():
                    barrier_type = "clinical"
                elif 'economic' in line.lower() or 'cost' in line.lower():
                    barrier_type = "economic"
                elif 'infrastructure' in line.lower():
                    barrier_type = "infrastructure"
                elif 'provider' in line.lower():
                    barrier_type = "provider"
                
                barriers.append({
                    "type": barrier_type,
                    "description": line.strip(),
                    "severity": "medium"
                })
    
    return barriers

def calculate_evidence_quality(response: str, data_sources: List[str]) -> float:
    """Calculate evidence quality score"""
    base_score = 0.6
    
    # Add points for data sources
    source_bonus = len(data_sources) * 0.1
    
    # Add points for specific metrics mentioned
    if any(term in response.lower() for term in ['randomized', 'controlled', 'matched']):
        base_score += 0.2
    
    if any(term in response.lower() for term in ['significant', 'p-value', 'confidence interval']):
        base_score += 0.1
    
    return min(base_score + source_bonus, 1.0)

def calculate_market_readiness(response: str, target_markets: List[str]) -> float:
    """Calculate market readiness score"""
    base_score = 0.5
    
    # Add points for each market analyzed
    market_bonus = len(target_markets) * 0.1
    
    # Add points for positive indicators
    if any(term in response.lower() for term in ['approved', 'favorable', 'accessible']):
        base_score += 0.2
    
    # Subtract points for barriers
    if any(term in response.lower() for term in ['barrier', 'restriction', 'limited']):
        base_score -= 0.1
    
    return max(min(base_score + market_bonus, 1.0), 0.0)

def extract_forecast_data(response: str, section_name: str) -> Dict[str, Any]:
    """Extract forecast data with numerical projections"""
    forecast_data = extract_section_data(response, section_name)
    
    # Try to extract numerical projections
    import re
    numbers = re.findall(r'(\d+(?:\.\d+)?)\s*%', response)
    years = re.findall(r'20[2-4][0-9]', response)
    
    forecast_data["projections"] = {}
    for i, year in enumerate(years[:10]):  # Limit to 10 years
        if i < len(numbers):
            try:
                forecast_data["projections"][year] = float(numbers[i])
            except:
                pass
    
    return forecast_data

def extract_revenue_data(response: str, horizon: int) -> Dict[str, Any]:
    """Extract revenue forecast data"""
    revenue_data = {"annual_projections": {}, "peak_sales": 0, "peak_year": 0}
    
    import re
    # Find revenue numbers
    revenue_matches = re.findall(r'\$(\d+(?:\.\d+)?)\s*[mMbB]?', response)
    year_matches = re.findall(r'20[2-4][0-9]', response)
    
    current_year = 2024
    for i in range(horizon):
        year = current_year + i
        if i < len(revenue_matches):
            try:
                value = float(revenue_matches[i])
                revenue_data["annual_projections"][str(year)] = value
                if value > revenue_data["peak_sales"]:
                    revenue_data["peak_sales"] = value
                    revenue_data["peak_year"] = year
            except:
                pass
    
    return revenue_data

def extract_assumptions(response: str) -> List[str]:
    """Extract key assumptions"""
    assumptions = []
    lines = response.split('\n')
    assumption_started = False
    
    for line in lines:
        if "ASSUMPTION" in line.upper() or "KEY ASSUMPTIONS" in line.upper():
            assumption_started = True
            continue
        elif assumption_started and any(section in line.upper() for section in ["SENSITIVITY", "MODEL", "UNCERTAINTY"]):
            break
        elif assumption_started and line.strip():
            if line.strip().startswith(('-', '', '*')):
                assumptions.append(line.strip())
    
    return assumptions

def extract_sensitivity_data(response: str) -> Dict[str, float]:
    """Extract sensitivity factors"""
    sensitivity = {}
    lines = response.split('\n')
    
    for line in lines:
        if 'sensitivity' in line.lower():
            import re
            # Look for percentage or factor values
            numbers = re.findall(r'(\d+(?:\.\d+)?)', line)
            if numbers:
                factor_name = line.split(':')[0].strip() if ':' in line else "unknown"
                try:
                    sensitivity[factor_name] = float(numbers[0])
                except:
                    pass
    
    return sensitivity

def extract_scenario_probabilities(response: str) -> Dict[str, float]:
    """Extract scenario probabilities"""
    probabilities = {}
    lines = response.split('\n')
    
    scenarios = ['bull', 'base', 'bear', 'optimistic', 'realistic', 'pessimistic']
    for line in lines:
        for scenario in scenarios:
            if scenario in line.lower() and 'probability' in line.lower():
                import re
                numbers = re.findall(r'(\d+(?:\.\d+)?)', line)
                if numbers:
                    try:
                        prob = float(numbers[0])
                        if prob > 1:  # Convert percentage to decimal
                            prob = prob / 100
                        probabilities[scenario] = prob
                    except:
                        pass
    
    return probabilities

def extract_confidence_intervals(response: str) -> Dict[str, Any]:
    """Extract confidence intervals"""
    intervals = {}
    lines = response.split('\n')
    
    for line in lines:
        if 'confidence' in line.lower() or 'interval' in line.lower():
            import re
            numbers = re.findall(r'(\d+(?:\.\d+)?)', line)
            if len(numbers) >= 2:
                intervals["lower_bound"] = float(numbers[0])
                intervals["upper_bound"] = float(numbers[1])
                break
    
    return intervals

def extract_performance_metrics(response: str) -> Dict[str, float]:
    """Extract model performance metrics"""
    metrics = {}
    lines = response.split('\n')
    
    metric_keywords = ['accuracy', 'precision', 'recall', 'rmse', 'mae', 'r-squared']
    for line in lines:
        for keyword in metric_keywords:
            if keyword in line.lower():
                import re
                numbers = re.findall(r'(\d+(?:\.\d+)?)', line)
                if numbers:
                    try:
                        metrics[keyword] = float(numbers[0])
                    except:
                        pass
    
    return metrics

def extract_risk_data(response: str) -> Dict[str, Any]:
    """Extract risk-adjusted projection data"""
    risk_data = {"monte_carlo_results": {}, "var_analysis": {}}
    
    if 'monte carlo' in response.lower():
        risk_data["monte_carlo_results"]["iterations"] = 1000
        risk_data["monte_carlo_results"]["mean"] = 500  # Default values
        risk_data["monte_carlo_results"]["std_dev"] = 100
    
    if 'value at risk' in response.lower() or 'var' in response.lower():
        risk_data["var_analysis"]["5_percentile"] = 200
        risk_data["var_analysis"]["95_percentile"] = 800
    
    return risk_data

def extract_uncertainty_data(response: str) -> Dict[str, Any]:
    """Extract uncertainty analysis data"""
    uncertainty_data = {"parameter_uncertainty": {}, "model_uncertainty": {}}
    
    # Default uncertainty values
    uncertainty_data["parameter_uncertainty"]["low"] = 0.1
    uncertainty_data["parameter_uncertainty"]["medium"] = 0.3
    uncertainty_data["parameter_uncertainty"]["high"] = 0.5
    
    uncertainty_data["model_uncertainty"]["forecast_error"] = 0.2
    uncertainty_data["model_uncertainty"]["confidence_level"] = 0.8
    
    return uncertainty_data

def extract_timelines_data(response: str) -> Dict[str, Any]:
    """Extract approval timelines data"""
    timelines = {}
    lines = response.split('\n')
    
    for line in lines:
        if 'timeline' in line.lower() or 'months' in line.lower() or 'years' in line.lower():
            import re
            numbers = re.findall(r'(\d+)', line)
            if numbers:
                if 'approval' in line.lower():
                    timelines["approval_timeline_months"] = int(numbers[0])
                elif 'review' in line.lower():
                    timelines["review_timeline_months"] = int(numbers[0])
    
    return timelines

def extract_policies_data(response: str) -> List[Dict[str, Any]]:
    """Extract coverage policies data"""
    policies = []
    lines = response.split('\n')
    
    for line in lines:
        if 'policy' in line.lower() or 'coverage' in line.lower():
            policies.append({
                "policy_type": "coverage",
                "description": line.strip(),
                "impact": "medium"
            })
    
    return policies[:10]

def extract_population_data(response: str) -> Dict[str, Any]:
    """Extract study population data"""
    population_data = {"demographics": {}, "sample_sizes": {}}
    
    # Look for demographic information
    if 'age' in response.lower():
        population_data["demographics"]["age_mentioned"] = True
    if 'gender' in response.lower():
        population_data["demographics"]["gender_mentioned"] = True
    
    # Look for sample sizes
    import re
    numbers = re.findall(r'(\d+,?\d*)\s*patients?', response.lower())
    if numbers:
        try:
            population_data["sample_sizes"]["total_patients"] = int(numbers[0].replace(',', ''))
        except:
            pass
    
    return population_data

def extract_limitations(response: str) -> List[str]:
    """Extract study limitations"""
    limitations = []
    lines = response.split('\n')
    limitation_started = False
    
    for line in lines:
        if "limitation" in line.lower():
            limitation_started = True
            limitations.append(line.strip())
        elif limitation_started and line.strip().startswith(('-', '', '*')):
            limitations.append(line.strip())
        elif limitation_started and not line.strip():
            break
    
    return limitations[:10]

def extract_recommendations(response: str) -> List[str]:
    """Extract recommendations"""
    recommendations = []
    lines = response.split('\n')
    recommendation_started = False
    
    for line in lines:
        if "recommendation" in line.lower():
            recommendation_started = True
            if ':' in line:
                recommendations.append(line.split(':')[1].strip())
            else:
                recommendations.append(line.strip())
        elif recommendation_started and line.strip().startswith(('-', '', '*')):
            recommendations.append(line.strip())
        elif recommendation_started and not line.strip():
            break
    
    return recommendations[:10]

# API Routes
@api_router.get("/")
async def root():
    return {"message": "Pharma Forecasting Consultant API v2.0"}

@api_router.post("/status", response_model=StatusCheck)
async def create_status_check(input: StatusCheckCreate):
    status_dict = input.dict()
    status_obj = StatusCheck(**status_dict)
    _ = await db.status_checks.insert_one(status_obj.dict())
    return status_obj

@api_router.get("/status", response_model=List[StatusCheck])
async def get_status_checks():
    status_checks = await db.status_checks.find().to_list(1000)
    return [StatusCheck(**status_check) for status_check in status_checks]

@api_router.post("/analyze-therapy", response_model=TherapyAreaAnalysis)
async def analyze_therapy_area(request: TherapyAreaRequest):
    try:
        from emergentintegrations.llm.chat import LlmChat, UserMessage
        
        # Basic analysis using Claude
        chat = LlmChat(
            api_key=request.api_key,
            session_id=f"therapy_analysis_{uuid.uuid4()}",
            system_message="""You are a world-class pharmaceutical consultant specializing in therapy area analysis and forecasting. 
            You have deep expertise in disease pathology, treatment algorithms, biomarkers, and patient journey mapping.
            Provide comprehensive, accurate, and structured analysis suitable for pharmaceutical forecasting models."""
        ).with_model("anthropic", "claude-sonnet-4-20250514").with_max_tokens(4096)
        
        product_info = f" for the product '{request.product_name}'" if request.product_name else ""
        prompt = f"""
        Please provide a comprehensive analysis of the {request.therapy_area} therapy area{product_info}. 
        
        Structure your response in exactly 5 sections with clear headers:
        
        ## DISEASE SUMMARY
        [Provide overview of the disease/condition, epidemiology, prevalence, and key clinical characteristics]
        
        ## STAGING
        [Detail the disease staging system, progression stages, and clinical classifications used]
        
        ## BIOMARKERS
        [List key biomarkers, diagnostic markers, prognostic indicators, and companion diagnostics]
        
        ## TREATMENT ALGORITHM
        [Describe current treatment pathways, standard of care, decision points, and treatment sequencing]
        
        ## PATIENT JOURNEY
        [Map the complete patient journey from symptoms to diagnosis to treatment and follow-up care]
        
        Focus on current medical standards and include relevant clinical data where appropriate.
        """
        
        user_message = UserMessage(text=prompt)
        response = await chat.send_message(user_message)
        
        # Parse the response into structured sections
        sections = response.split("## ")
        disease_summary = ""
        staging = ""
        biomarkers = ""
        treatment_algorithm = ""
        patient_journey = ""
        
        for section in sections[1:]:
            if section.startswith("DISEASE SUMMARY"):
                disease_summary = section.replace("DISEASE SUMMARY\n", "").strip()
            elif section.startswith("STAGING"):
                staging = section.replace("STAGING\n", "").strip()
            elif section.startswith("BIOMARKERS"):
                biomarkers = section.replace("BIOMARKERS\n", "").strip()
            elif section.startswith("TREATMENT ALGORITHM"):
                treatment_algorithm = section.replace("TREATMENT ALGORITHM\n", "").strip()
            elif section.startswith("PATIENT JOURNEY"):
                patient_journey = section.replace("PATIENT JOURNEY\n", "").strip()
        
        # Enhanced intelligence gathering (run in background)
        clinical_trials_data = await search_clinical_trials(request.therapy_area)
        competitive_landscape = await generate_competitive_analysis(request.therapy_area, request.api_key)
        regulatory_intelligence = await search_regulatory_intelligence(request.therapy_area, request.api_key)
        
        # Create analysis object with enhanced data
        analysis = TherapyAreaAnalysis(
            therapy_area=request.therapy_area,
            product_name=request.product_name,
            disease_summary=disease_summary,
            staging=staging,
            biomarkers=biomarkers,
            treatment_algorithm=treatment_algorithm,
            patient_journey=patient_journey,
            clinical_trials_data=clinical_trials_data[:10],  # Top 10 relevant trials
            competitive_landscape=competitive_landscape,
            regulatory_intelligence=regulatory_intelligence
        )
        
        # Generate risk assessment
        analysis_dict = analysis.dict()
        risk_assessment = await generate_risk_assessment(request.therapy_area, analysis_dict, request.api_key)
        analysis.risk_assessment = risk_assessment
        
        # Save to database
        await db.therapy_analyses.insert_one(analysis.dict())
        
        return analysis
        
    except Exception as e:
        logger.error(f"Error in therapy analysis: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

@api_router.post("/generate-funnel", response_model=PatientFlowFunnel)
async def generate_patient_flow_funnel(request: PatientFlowFunnelRequest):
    try:
        analysis = await db.therapy_analyses.find_one({"id": request.analysis_id})
        if not analysis:
            raise HTTPException(status_code=404, detail="Analysis not found")
        
        from emergentintegrations.llm.chat import LlmChat, UserMessage
        
        chat = LlmChat(
            api_key=request.api_key,
            session_id=f"funnel_generation_{uuid.uuid4()}",
            system_message="""You are a pharmaceutical forecasting expert specializing in patient flow modeling and market analysis.
            Create detailed patient flow funnels suitable for pharmaceutical forecasting models based on therapy area analysis."""
        ).with_model("anthropic", "claude-sonnet-4-20250514").with_max_tokens(4096)
        
        prompt = f"""
        Based on the following therapy area analysis for {request.therapy_area}, create a comprehensive patient flow funnel suitable for pharmaceutical forecasting:
        
        THERAPY AREA: {request.therapy_area}
        DISEASE SUMMARY: {analysis['disease_summary'][:500]}...
        TREATMENT ALGORITHM: {analysis['treatment_algorithm'][:500]}...
        PATIENT JOURNEY: {analysis['patient_journey'][:500]}...
        
        Please provide your response in exactly this JSON structure:
        
        {{
            "funnel_stages": [
                {{
                    "stage": "Total Population at Risk",
                    "description": "Overall population that could develop this condition",
                    "percentage": "100%",
                    "notes": "Base population estimates"
                }},
                {{
                    "stage": "Disease Incidence/Prevalence",
                    "description": "Population that develops or has the condition",
                    "percentage": "X%",
                    "notes": "Epidemiological data"
                }},
                {{
                    "stage": "Diagnosis Rate",
                    "description": "Patients who get properly diagnosed",
                    "percentage": "X%",
                    "notes": "Diagnosis challenges and rates"
                }},
                {{
                    "stage": "Treatment Eligible",
                    "description": "Diagnosed patients eligible for treatment",
                    "percentage": "X%",
                    "notes": "Contraindications and eligibility criteria"
                }},
                {{
                    "stage": "Treated Patients",
                    "description": "Patients actually receiving treatment",
                    "percentage": "X%",
                    "notes": "Treatment uptake and access"
                }},
                {{
                    "stage": "Target Patient Population",
                    "description": "Specific target for your therapy/product",
                    "percentage": "X%",
                    "notes": "Specific targeting criteria"
                }}
            ],
            "total_addressable_population": "Detailed TAM analysis with numbers and rationale",
            "forecasting_notes": "Key assumptions, market dynamics, competitive landscape considerations, and forecasting methodology recommendations"
        }}
        
        Fill in realistic percentages and detailed descriptions based on current medical literature and market data for {request.therapy_area}.
        """
        
        user_message = UserMessage(text=prompt)
        response = await chat.send_message(user_message)
        
        # Parse JSON response
        import json
        try:
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            json_str = response[json_start:json_end]
            parsed_response = json.loads(json_str)
        except:
            parsed_response = {
                "funnel_stages": [
                    {"stage": "Total Population", "description": "Analysis generated", "percentage": "100%", "notes": "See full response"},
                    {"stage": "Target Population", "description": "Detailed analysis provided", "percentage": "Variable", "notes": response[:200] + "..."}
                ],
                "total_addressable_population": "See full analysis response",
                "forecasting_notes": response
            }
        
        # Generate scenario models
        scenario_models = await generate_scenario_models(
            request.therapy_area, 
            analysis, 
            ["optimistic", "realistic", "pessimistic"], 
            request.api_key
        )
        
        # Create visualization data with context
        product_name = analysis.get('product_name', '')
        visualization_data = {
            "funnel_chart": create_funnel_chart(parsed_response.get("funnel_stages", [])),
            "scenario_chart": create_scenario_comparison_chart(scenario_models, request.therapy_area, product_name)
        }
        
        if analysis.get('competitive_landscape'):
            visualization_data["market_chart"] = create_market_analysis_chart(analysis['competitive_landscape'])
        
        funnel = PatientFlowFunnel(
            therapy_area=request.therapy_area,
            analysis_id=request.analysis_id,
            funnel_stages=parsed_response.get("funnel_stages", []),
            total_addressable_population=parsed_response.get("total_addressable_population", ""),
            forecasting_notes=parsed_response.get("forecasting_notes", ""),
            scenario_models=scenario_models,
            visualization_data=visualization_data
        )
        
        await db.patient_flow_funnels.insert_one(funnel.dict())
        
        return funnel
        
    except Exception as e:
        logger.error(f"Error in funnel generation: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Funnel generation failed: {str(e)}")

@api_router.post("/competitive-analysis")
async def generate_competitive_intel(request: CompetitiveAnalysisRequest):
    """Generate enhanced competitive intelligence"""
    try:
        analysis = await db.therapy_analyses.find_one({"id": request.analysis_id})
        if not analysis:
            raise HTTPException(status_code=404, detail="Analysis not found")
        
        # Enhanced competitive analysis with clinical trials data
        competitive_data = await generate_competitive_analysis(request.therapy_area, request.api_key)
        clinical_trials = await search_clinical_trials(request.therapy_area)
        
        # Update analysis with enhanced competitive intelligence
        await db.therapy_analyses.update_one(
            {"id": request.analysis_id},
            {"$set": {
                "competitive_landscape": competitive_data,
                "clinical_trials_data": clinical_trials[:15],
                "updated_at": datetime.utcnow()
            }}
        )
        
        return {
            "status": "success",
            "competitive_landscape": competitive_data,
            "clinical_trials_count": len(clinical_trials),
            "updated_at": datetime.utcnow()
        }
        
    except Exception as e:
        logger.error(f"Competitive analysis error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Competitive analysis failed: {str(e)}")

@api_router.post("/scenario-modeling")
async def generate_scenario_analysis(request: ScenarioModelingRequest):
    """Generate multi-scenario forecasting models"""
    try:
        analysis = await db.therapy_analyses.find_one({"id": request.analysis_id})
        if not analysis:
            raise HTTPException(status_code=404, detail="Analysis not found")
        
        scenario_models = await generate_scenario_models(
            request.therapy_area,
            analysis,
            request.scenarios,
            request.api_key
        )
        
        # Update analysis with scenario models
        await db.therapy_analyses.update_one(
            {"id": request.analysis_id},
            {"$set": {
                "scenario_models": scenario_models,
                "updated_at": datetime.utcnow()
            }}
        )
        
        # Generate visualization
        visualization_chart = create_scenario_comparison_chart(scenario_models)
        
        return {
            "status": "success",
            "scenario_models": scenario_models,
            "visualization": visualization_chart,
            "updated_at": datetime.utcnow()
        }
        
    except Exception as e:
        logger.error(f"Scenario modeling error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Scenario modeling failed: {str(e)}")

@api_router.post("/export")
async def export_analysis(request: ExportRequest):
    """Export analysis in various formats"""
    try:
        analysis = await db.therapy_analyses.find_one({"id": request.analysis_id})
        if not analysis:
            raise HTTPException(status_code=404, detail="Analysis not found")
        
        funnel = await db.patient_flow_funnels.find_one({"analysis_id": request.analysis_id})
        
        if request.export_type == "pdf":
            export_data = generate_pdf_report(analysis, funnel)
            if export_data:
                return {
                    "status": "success",
                    "export_type": "pdf",
                    "data": export_data,
                    "filename": f"{analysis['therapy_area'].replace(' ', '_')}_analysis.pdf"
                }
        
        elif request.export_type == "excel":
            export_data = generate_excel_export(analysis, funnel)
            if export_data:
                return {
                    "status": "success", 
                    "export_type": "excel",
                    "data": export_data,
                    "filename": f"{analysis['therapy_area'].replace(' ', '_')}_model.xlsx"
                }
        
        raise HTTPException(status_code=400, detail="Invalid export type or generation failed")
        
    except Exception as e:
        logger.error(f"Export error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Export failed: {str(e)}")

@api_router.get("/analyses", response_model=List[TherapyAreaAnalysis])
async def get_therapy_analyses():
    analyses = await db.therapy_analyses.find().sort("created_at", -1).to_list(50)
    return [TherapyAreaAnalysis(**analysis) for analysis in analyses]

@api_router.get("/analysis/{analysis_id}")
async def get_analysis_details(analysis_id: str):
    analysis = await db.therapy_analyses.find_one({"id": analysis_id})
    if not analysis:
        raise HTTPException(status_code=404, detail="Analysis not found")
    
    funnel = await db.patient_flow_funnels.find_one({"analysis_id": analysis_id})
    
    return {
        "analysis": TherapyAreaAnalysis(**analysis),
        "funnel": PatientFlowFunnel(**funnel) if funnel else None
    }

@api_router.get("/funnels/{analysis_id}")
async def get_funnel_by_analysis(analysis_id: str):
    funnel = await db.patient_flow_funnels.find_one({"analysis_id": analysis_id})
    if not funnel:
        return None
    return PatientFlowFunnel(**funnel)

@api_router.get("/search/clinical-trials")
async def search_trials_endpoint(therapy_area: str):
    """Search clinical trials endpoint"""
    trials = await search_clinical_trials(therapy_area)
    return {"trials": trials, "count": len(trials)}

# Phase 3: API Endpoints for Real-World Evidence Integration & Market Access Intelligence

@api_router.post("/real-world-evidence", response_model=RealWorldEvidence)
async def generate_rwe_analysis(request: RWERequest):
    """Generate Real-World Evidence analysis"""
    try:
        rwe_result = await generate_real_world_evidence(
            therapy_area=request.therapy_area,
            product_name=request.product_name,
            analysis_type=request.analysis_type,
            data_sources=request.data_sources,
            api_key=request.api_key
        )
        
        # Store RWE analysis in database
        await db.rwe_analyses.insert_one({
            "therapy_area": request.therapy_area,
            "product_name": request.product_name,
            "analysis_type": request.analysis_type,
            "rwe_data": rwe_result.dict(),
            "data_sources": request.data_sources,
            "evidence_quality_score": rwe_result.evidence_quality_score,
            "timestamp": datetime.utcnow()
        })
        
        return rwe_result
        
    except Exception as e:
        logger.error(f"RWE analysis error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"RWE analysis failed: {str(e)}")

@api_router.post("/market-access-intelligence", response_model=MarketAccessIntelligence)
async def generate_market_access_analysis(request: MarketAccessRequest):
    """Generate Market Access Intelligence analysis"""
    try:
        market_access_result = await generate_market_access_intelligence(
            therapy_area=request.therapy_area,
            product_name=request.product_name,
            target_markets=request.target_markets,
            analysis_depth=request.analysis_depth,
            api_key=request.api_key
        )
        
        # Store market access analysis in database
        await db.market_access_analyses.insert_one({
            "therapy_area": request.therapy_area,
            "product_name": request.product_name,
            "target_markets": request.target_markets,
            "analysis_depth": request.analysis_depth,
            "market_access_data": market_access_result.dict(),
            "market_readiness_score": market_access_result.market_readiness_score,
            "timestamp": datetime.utcnow()
        })
        
        return market_access_result
        
    except Exception as e:
        logger.error(f"Market access analysis error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Market access analysis failed: {str(e)}")

@api_router.post("/predictive-analytics", response_model=PredictiveAnalytics)
async def generate_predictive_analysis(request: PredictiveAnalyticsRequest):
    """Generate Predictive Analytics with ML-enhanced forecasting"""
    try:
        predictive_result = await generate_predictive_analytics(
            therapy_area=request.therapy_area,
            product_name=request.product_name,
            forecast_horizon=request.forecast_horizon,
            model_type=request.model_type,
            include_rwe=request.include_rwe,
            api_key=request.api_key
        )
        
        # Store predictive analysis in database
        await db.predictive_analyses.insert_one({
            "therapy_area": request.therapy_area,
            "product_name": request.product_name,
            "forecast_horizon": request.forecast_horizon,
            "model_type": request.model_type,
            "include_rwe": request.include_rwe,
            "predictive_data": predictive_result.dict(),
            "model_performance": predictive_result.model_performance_metrics,
            "timestamp": datetime.utcnow()
        })
        
        return predictive_result
        
    except Exception as e:
        logger.error(f"Predictive analytics error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Predictive analytics failed: {str(e)}")

@api_router.get("/real-world-evidence/{therapy_area}")
async def get_rwe_analysis(therapy_area: str, product_name: str = None):
    """Retrieve RWE analysis"""
    try:
        query = {"therapy_area": {"$regex": therapy_area, "$options": "i"}}
        if product_name:
            query["product_name"] = {"$regex": product_name, "$options": "i"}
        
        analysis = await db.rwe_analyses.find_one(
            query,
            sort=[("timestamp", -1)]  # Get most recent
        )
        
        if not analysis:
            raise HTTPException(status_code=404, detail="RWE analysis not found")
        
        # Convert ObjectIds and return the stored RWE data directly
        rwe_data = convert_objectid_to_str(analysis["rwe_data"])
        return rwe_data
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@api_router.get("/market-access-intelligence/{therapy_area}")
async def get_market_access_analysis(therapy_area: str, product_name: str = None):
    """Retrieve Market Access Intelligence analysis"""
    try:
        query = {"therapy_area": {"$regex": therapy_area, "$options": "i"}}
        if product_name:
            query["product_name"] = {"$regex": product_name, "$options": "i"}
        
        analysis = await db.market_access_analyses.find_one(
            query,
            sort=[("timestamp", -1)]  # Get most recent
        )
        
        if not analysis:
            raise HTTPException(status_code=404, detail="Market access analysis not found")
        
        # Convert ObjectIds and return the stored market access data directly
        market_access_data = convert_objectid_to_str(analysis["market_access_data"])
        return market_access_data
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@api_router.get("/predictive-analytics/{therapy_area}")
async def get_predictive_analysis(therapy_area: str, product_name: str = None):
    """Retrieve Predictive Analytics analysis"""
    try:
        query = {"therapy_area": {"$regex": therapy_area, "$options": "i"}}
        if product_name:
            query["product_name"] = {"$regex": product_name, "$options": "i"}
        
        analysis = await db.predictive_analyses.find_one(
            query,
            sort=[("timestamp", -1)]  # Get most recent
        )
        
        if not analysis:
            raise HTTPException(status_code=404, detail="Predictive analysis not found")
        
        # Convert ObjectIds and return the stored predictive data directly
        predictive_data = convert_objectid_to_str(analysis["predictive_data"])
        return predictive_data
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def convert_objectid_to_str(obj):
    """Convert MongoDB ObjectId to string for JSON serialization"""
    if isinstance(obj, dict):
        return {key: convert_objectid_to_str(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_objectid_to_str(item) for item in obj]
    elif hasattr(obj, '__dict__'):
        return convert_objectid_to_str(obj.__dict__)
    elif str(type(obj)) == "<class 'bson.objectid.ObjectId'>":
        return str(obj)
    else:
        return obj

@api_router.get("/phase3-dashboard")
async def get_phase3_dashboard():
    """Get Phase 3 analytics dashboard data"""
    try:
        # Get counts of each analysis type
        rwe_count = await db.rwe_analyses.count_documents({})
        market_access_count = await db.market_access_analyses.count_documents({})
        predictive_count = await db.predictive_analyses.count_documents({})
        
        # Get recent analyses (convert ObjectIds to strings)
        recent_rwe_raw = await db.rwe_analyses.find({}).sort("timestamp", -1).limit(5).to_list(5)
        recent_market_access_raw = await db.market_access_analyses.find({}).sort("timestamp", -1).limit(5).to_list(5)
        recent_predictive_raw = await db.predictive_analyses.find({}).sort("timestamp", -1).limit(5).to_list(5)
        
        # Convert ObjectIds to strings for JSON serialization
        recent_rwe = [convert_objectid_to_str(doc) for doc in recent_rwe_raw]
        recent_market_access = [convert_objectid_to_str(doc) for doc in recent_market_access_raw]
        recent_predictive = [convert_objectid_to_str(doc) for doc in recent_predictive_raw]
        
        return {
            "summary": {
                "rwe_analyses": rwe_count,
                "market_access_analyses": market_access_count,
                "predictive_analyses": predictive_count,
                "total_phase3_analyses": rwe_count + market_access_count + predictive_count
            },
            "recent_analyses": {
                "rwe": recent_rwe,
                "market_access": recent_market_access,
                "predictive": recent_predictive
            },
            "capabilities": {
                "real_world_evidence": [
                    "Effectiveness vs. efficacy analysis",
                    "Safety signal monitoring", 
                    "Patient outcome assessment",
                    "Adherence pattern analysis",
                    "Health economics evaluation",
                    "Comparative effectiveness research"
                ],
                "market_access": [
                    "Payer landscape mapping",
                    "Reimbursement pathway analysis",
                    "Pricing strategy guidance",
                    "Access barrier identification",
                    "HEOR requirement assessment",
                    "Regulatory timeline optimization"
                ],
                "predictive_analytics": [
                    "ML-enhanced forecasting",
                    "Monte Carlo simulations",
                    "Competitive response modeling",
                    "Risk-adjusted projections", 
                    "Scenario probability analysis",
                    "Uncertainty quantification"
                ]
            }
        }
        
    except Exception as e:
        logger.error(f"Phase 3 dashboard error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Dashboard data retrieval failed: {str(e)}")

# ========================================
# Phase 4: User Management & Authentication API Endpoints
# ========================================

@api_router.post("/auth/register")
async def register_user(user_data: UserRegistration):
    """Register a new user"""
    try:
        # Check if user already exists
        existing_user = await db.users.find_one({"email": user_data.email})
        if existing_user:
            raise HTTPException(status_code=400, detail="Email already registered")
        
        # Hash password
        password_hash = hash_password(user_data.password)
        
        # Create user
        user = User(
            email=user_data.email,
            first_name=user_data.first_name,
            last_name=user_data.last_name,
            company=user_data.company,
            role=user_data.role
        )
        
        # Store user in database
        await db.users.insert_one(user.dict())
        
        # Store password separately
        await db.user_passwords.insert_one({
            "user_id": user.id,
            "password_hash": password_hash,
            "created_at": datetime.utcnow()
        })
        
        # Create user profile
        profile = UserProfile(
            user_id=user.id,
            preferences={
                "theme": "light",
                "notifications_enabled": True,
                "auto_save": True
            },
            notification_settings={
                "email_alerts": True,
                "browser_notifications": False,
                "weekly_reports": True
            }
        )
        await db.user_profiles.insert_one(profile.dict())
        
        return {
            "message": "User registered successfully",
            "user_id": user.id,
            "email": user.email
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"User registration error: {str(e)}")
        raise HTTPException(status_code=500, detail="Registration failed")

@api_router.post("/auth/login")
async def login_user(login_data: UserLogin, request: Request):
    """Authenticate user and create session"""
    try:
        # Find user
        user = await db.users.find_one({"email": login_data.email})
        if not user:
            raise HTTPException(status_code=401, detail="Invalid email or password")
        
        # Get password hash
        password_data = await db.user_passwords.find_one({"user_id": user["id"]})
        if not password_data:
            raise HTTPException(status_code=401, detail="Invalid email or password")
        
        # Verify password
        if not verify_password(login_data.password, password_data["password_hash"]):
            raise HTTPException(status_code=401, detail="Invalid email or password")
        
        # Create session
        session_token = generate_session_token()
        expires_at = datetime.utcnow() + timedelta(hours=SESSION_EXPIRE_HOURS)
        
        session = UserSession(
            user_id=user["id"],
            session_token=session_token,
            expires_at=expires_at,
            ip_address=request.client.host if request.client else None,
            user_agent=request.headers.get("user-agent")
        )
        
        await db.user_sessions.insert_one(session.dict())
        
        # Update last login
        await db.users.update_one(
            {"id": user["id"]},
            {"$set": {"last_login": datetime.utcnow()}}
        )
        
        return {
            "access_token": session_token,
            "token_type": "bearer",
            "expires_in": SESSION_EXPIRE_HOURS * 3600,
            "user": {
                "id": user["id"],
                "email": user["email"],
                "first_name": user["first_name"],
                "last_name": user["last_name"],
                "subscription_tier": user.get("subscription_tier", "free")
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"User login error: {str(e)}")
        raise HTTPException(status_code=500, detail="Login failed")

@api_router.post("/auth/logout")
async def logout_user(current_user: User = Depends(get_current_user)):
    """Logout user and invalidate session"""
    try:
        if not current_user:
            raise HTTPException(status_code=401, detail="Not authenticated")
        
        # Find and delete current session
        await db.user_sessions.delete_many({"user_id": current_user.id})
        
        return {"message": "Logged out successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"User logout error: {str(e)}")
        raise HTTPException(status_code=500, detail="Logout failed")

# ========================================
# Phase 4: OAuth Authentication Endpoints
# ========================================

@api_router.get("/auth/google/login")
async def google_login(request: Request):
    """Initiate Google OAuth login"""
    try:
        # Build redirect URI
        redirect_uri = str(request.url_for('google_auth'))
        return await oauth.google.authorize_redirect(request, redirect_uri)
    except Exception as e:
        logger.error(f"Google OAuth initiation error: {str(e)}")
        raise HTTPException(status_code=500, detail="OAuth initiation failed")

@api_router.get("/auth/google/callback")
async def google_auth(request: Request):
    """Handle Google OAuth callback"""
    try:
        # Get token from Google
        token = await oauth.google.authorize_access_token(request)
        user_info = token.get('userinfo')
        
        if not user_info:
            raise HTTPException(status_code=400, detail="Failed to get user info from Google")
        
        # Check if user exists
        email = user_info.get('email')
        user = await db.users.find_one({"email": email})
        
        if not user:
            # Create new user
            new_user = User(
                email=email,
                first_name=user_info.get('given_name', ''),
                last_name=user_info.get('family_name', ''),
                subscription_tier="free"
            )
            
            # Store user in database
            await db.users.insert_one(new_user.dict())
            
            # Create user profile
            profile = UserProfile(
                user_id=new_user.id,
                preferences={"oauth_provider": "google"},
                notification_settings={
                    "email_alerts": True,
                    "browser_notifications": False,
                    "weekly_reports": True
                }
            )
            await db.user_profiles.insert_one(profile.dict())
            
            user = new_user
        else:
            user = User(**user)
        
        # Create session
        session_token = generate_session_token()
        expires_at = datetime.utcnow() + timedelta(hours=SESSION_EXPIRE_HOURS)
        
        session = UserSession(
            user_id=user.id,
            session_token=session_token,
            expires_at=expires_at,
            ip_address=request.client.host if request.client else None,
            user_agent=request.headers.get("user-agent")
        )
        
        await db.user_sessions.insert_one(session.dict())
        
        # Update last login
        await db.users.update_one(
            {"id": user.id},
            {"$set": {"last_login": datetime.utcnow()}}
        )
        
        return {
            "access_token": session_token,
            "token_type": "bearer",
            "expires_in": SESSION_EXPIRE_HOURS * 3600,
            "user": {
                "id": user.id,
                "email": user.email,
                "first_name": user.first_name,
                "last_name": user.last_name,
                "subscription_tier": user.subscription_tier
            },
            "oauth_provider": "google"
        }
        
    except Exception as e:
        logger.error(f"Google OAuth callback error: {str(e)}")
        raise HTTPException(status_code=500, detail="OAuth authentication failed")

@api_router.post("/auth/google/token")
async def google_token_auth(request: Request):
    """Handle Google OAuth token authentication from frontend"""
    try:
        body = await request.json()
        google_token = body.get('token')
        
        if not google_token:
            raise HTTPException(status_code=400, detail="Google token required")
        
        # Verify Google token
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"https://www.googleapis.com/oauth2/v1/userinfo?access_token={google_token}"
            )
            
            if response.status_code != 200:
                raise HTTPException(status_code=400, detail="Invalid Google token")
            
            user_info = response.json()
        
        # Check if user exists
        email = user_info.get('email')
        user = await db.users.find_one({"email": email})
        
        if not user:
            # Create new user
            new_user = User(
                email=email,
                first_name=user_info.get('given_name', ''),
                last_name=user_info.get('family_name', ''),
                subscription_tier="free"
            )
            
            await db.users.insert_one(new_user.dict())
            
            # Create user profile
            profile = UserProfile(
                user_id=new_user.id,
                preferences={"oauth_provider": "google"},
                notification_settings={
                    "email_alerts": True,
                    "browser_notifications": False,
                    "weekly_reports": True
                }
            )
            await db.user_profiles.insert_one(profile.dict())
            
            user = new_user
        else:
            user = User(**user)
        
        # Create session
        session_token = generate_session_token()
        expires_at = datetime.utcnow() + timedelta(hours=SESSION_EXPIRE_HOURS)
        
        session = UserSession(
            user_id=user.id,
            session_token=session_token,
            expires_at=expires_at,
            ip_address=request.client.host if request.client else None,
            user_agent=request.headers.get("user-agent")
        )
        
        await db.user_sessions.insert_one(session.dict())
        
        # Update last login
        await db.users.update_one(
            {"id": user.id},
            {"$set": {"last_login": datetime.utcnow()}}
        )
        
        return {
            "access_token": session_token,
            "token_type": "bearer", 
            "expires_in": SESSION_EXPIRE_HOURS * 3600,
            "user": {
                "id": user.id,
                "email": user.email,
                "first_name": user.first_name,
                "last_name": user.last_name,
                "subscription_tier": user.subscription_tier
            },
            "oauth_provider": "google"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Google token authentication error: {str(e)}")
        raise HTTPException(status_code=500, detail="Token authentication failed")

@api_router.post("/auth/apple/token")
async def apple_token_auth(request: Request):
    """Handle Apple ID OAuth token authentication from frontend"""
    try:
        body = await request.json()
        apple_code = body.get('code')
        apple_id_token = body.get('id_token')
        
        if not apple_code and not apple_id_token:
            raise HTTPException(status_code=400, detail="Apple authorization code or ID token required")
        
        user_info = {}
        
        if apple_id_token:
            # Decode Apple ID token (in production, verify signature with Apple's public keys)
            try:
                # For development, we decode without verification
                # In production, implement proper signature verification
                decoded_token = jwt.decode(apple_id_token, options={"verify_signature": False})
                user_info = {
                    'email': decoded_token.get('email'),
                    'sub': decoded_token.get('sub'),  # Apple user ID
                    'email_verified': decoded_token.get('email_verified', False)
                }
            except Exception as e:
                logger.error(f"Apple ID token decode error: {str(e)}")
                raise HTTPException(status_code=400, detail="Invalid Apple ID token")
        
        # Get name from request if provided (Apple only sends this on first login)
        if 'name' in body:
            name_data = body['name']
            user_info['given_name'] = name_data.get('firstName', '')
            user_info['family_name'] = name_data.get('lastName', '')
        
        email = user_info.get('email')
        if not email:
            raise HTTPException(status_code=400, detail="Email not provided by Apple")
        
        # Check if user exists
        user = await db.users.find_one({"email": email})
        
        if not user:
            # Create new user
            new_user = User(
                email=email,
                first_name=user_info.get('given_name', ''),
                last_name=user_info.get('family_name', ''),
                subscription_tier="free"
            )
            
            await db.users.insert_one(new_user.dict())
            
            # Store Apple-specific data
            await db.user_oauth.insert_one({
                "user_id": new_user.id,
                "provider": "apple",
                "provider_user_id": user_info.get('sub'),
                "created_at": datetime.utcnow()
            })
            
            # Create user profile
            profile = UserProfile(
                user_id=new_user.id,
                preferences={"oauth_provider": "apple"},
                notification_settings={
                    "email_alerts": True,
                    "browser_notifications": False,
                    "weekly_reports": True
                }
            )
            await db.user_profiles.insert_one(profile.dict())
            
            user = new_user
        else:
            user = User(**user)
        
        # Create session
        session_token = generate_session_token()
        expires_at = datetime.utcnow() + timedelta(hours=SESSION_EXPIRE_HOURS)
        
        session = UserSession(
            user_id=user.id,
            session_token=session_token,
            expires_at=expires_at,
            ip_address=request.client.host if request.client else None,
            user_agent=request.headers.get("user-agent")
        )
        
        await db.user_sessions.insert_one(session.dict())
        
        # Update last login
        await db.users.update_one(
            {"id": user.id},
            {"$set": {"last_login": datetime.utcnow()}}
        )
        
        return {
            "access_token": session_token,
            "token_type": "bearer",
            "expires_in": SESSION_EXPIRE_HOURS * 3600,
            "user": {
                "id": user.id,
                "email": user.email,
                "first_name": user.first_name,
                "last_name": user.last_name,
                "subscription_tier": user.subscription_tier
            },
            "oauth_provider": "apple"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Apple token authentication error: {str(e)}")
        raise HTTPException(status_code=500, detail="Apple authentication failed")

@api_router.get("/auth/profile")
async def get_user_profile(current_user: User = Depends(get_current_user)):
    """Get current user profile"""
    try:
        if not current_user:
            raise HTTPException(status_code=401, detail="Not authenticated")
        
        # Get user profile
        profile = await db.user_profiles.find_one({"user_id": current_user.id})
        if not profile:
            # Create default profile if doesn't exist
            profile = UserProfile(user_id=current_user.id)
            await db.user_profiles.insert_one(profile.dict())
            profile = profile.dict()
        else:
            # Remove MongoDB ObjectId and convert to dict
            if "_id" in profile:
                del profile["_id"]
        
        return {
            "user": current_user.dict(),
            "profile": profile
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Get user profile error: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to get profile")

# ========================================
# Phase 4: Payment & Subscription API Endpoints
# ========================================

@api_router.get("/subscriptions/plans")
async def get_subscription_plans():
    """Get available subscription plans"""
    return {"plans": list(SUBSCRIPTION_PLANS.values())}

@api_router.post("/payments/checkout/session")
async def create_checkout_session(request: Request):
    """Create Stripe checkout session"""
    try:
        request_data = await request.json()
        
        # Get the origin URL from request
        origin_url = request.headers.get("origin") or "http://localhost:3000"
        
        # Validate package selection
        package_id = request_data.get("package_id")
        if not package_id or package_id not in SUBSCRIPTION_PLANS:
            raise HTTPException(status_code=400, detail="Invalid subscription package")
        
        plan = SUBSCRIPTION_PLANS[package_id]
        
        # Initialize Stripe checkout
        webhook_url = f"{origin_url}/api/webhook/stripe"
        stripe_checkout = StripeCheckout(api_key=stripe_api_key, webhook_url=webhook_url)
        
        # Build success and cancel URLs
        success_url = f"{origin_url}/payment/success?session_id={{CHECKOUT_SESSION_ID}}"
        cancel_url = f"{origin_url}/payment/cancelled"
        
        # Create checkout session request
        checkout_request = CheckoutSessionRequest(
            amount=plan["price"],
            currency="usd",
            success_url=success_url,
            cancel_url=cancel_url,
            metadata={
                "package_id": package_id,
                "package_name": plan["name"],
                "source": "web_checkout"
            }
        )
        
        # Create checkout session
        session: CheckoutSessionResponse = await stripe_checkout.create_checkout_session(checkout_request)
        
        # Store payment transaction
        payment_transaction = PaymentTransaction(
            stripe_session_id=session.session_id,
            amount=plan["price"],
            currency="usd",
            subscription_tier=package_id,
            metadata={
                "package_id": package_id,
                "package_name": plan["name"],
                "origin_url": origin_url
            }
        )
        
        await db.payment_transactions.insert_one(payment_transaction.dict())
        
        return {
            "url": session.url,
            "session_id": session.session_id
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Create checkout session error: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to create checkout session")

@api_router.get("/payments/checkout/status/{session_id}")
async def get_checkout_status(session_id: str):
    """Get checkout session status"""
    try:
        # Initialize Stripe checkout
        stripe_checkout = StripeCheckout(api_key=stripe_api_key, webhook_url="")
        
        # Get checkout status
        checkout_status: CheckoutStatusResponse = await stripe_checkout.get_checkout_status(session_id)
        
        # Find payment transaction
        payment = await db.payment_transactions.find_one({"stripe_session_id": session_id})
        if not payment:
            raise HTTPException(status_code=404, detail="Payment transaction not found")
        
        # Update payment status if changed
        if payment["stripe_payment_status"] != checkout_status.payment_status:
            await db.payment_transactions.update_one(
                {"stripe_session_id": session_id},
                {
                    "$set": {
                        "stripe_payment_status": checkout_status.payment_status,
                        "payment_status": "completed" if checkout_status.payment_status == "paid" else "pending"
                    }
                }
            )
            
            # If payment completed, upgrade user subscription
            if checkout_status.payment_status == "paid" and payment["payment_status"] != "completed":
                package_id = payment["metadata"].get("package_id")
                if package_id:
                    # Find user and update subscription (placeholder - would need user context)
                    # For now, we'll handle this in the frontend after successful payment
                    await db.payment_transactions.update_one(
                        {"stripe_session_id": session_id},
                        {"$set": {"completed_at": datetime.utcnow()}}
                    )
        
        return {
            "status": checkout_status.status,
            "payment_status": checkout_status.payment_status,
            "amount_total": checkout_status.amount_total,
            "currency": checkout_status.currency,
            "metadata": checkout_status.metadata
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Get checkout status error: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to get checkout status")

@api_router.post("/webhook/stripe")
async def stripe_webhook(request: Request):
    """Handle Stripe webhooks"""
    try:
        body = await request.body()
        stripe_signature = request.headers.get("stripe-signature")
        
        # Initialize Stripe checkout
        stripe_checkout = StripeCheckout(api_key=stripe_api_key, webhook_url="")
        
        # Handle webhook
        webhook_response = await stripe_checkout.handle_webhook(body, stripe_signature)
        
        if webhook_response.event_type == "checkout.session.completed":
            # Update payment transaction
            await db.payment_transactions.update_one(
                {"stripe_session_id": webhook_response.session_id},
                {
                    "$set": {
                        "payment_status": "completed",
                        "stripe_payment_status": webhook_response.payment_status,
                        "completed_at": datetime.utcnow()
                    }
                }
            )
        
        return {"received": True}
        
    except Exception as e:
        logger.error(f"Stripe webhook error: {str(e)}")
        raise HTTPException(status_code=500, detail="Webhook processing failed")

# ========================================
# Phase 4: Automated Workflows API Endpoints
# ========================================

@api_router.post("/workflows", response_model=AutomatedWorkflow)
async def create_workflow(workflow_data: dict, current_user: User = Depends(get_current_user)):
    """Create automated workflow"""
    try:
        if not current_user:
            raise HTTPException(status_code=401, detail="Not authenticated")
        
        if not await require_subscription(current_user, "professional"):
            raise HTTPException(status_code=403, detail="Professional subscription required")
        
        workflow = AutomatedWorkflow(
            user_id=current_user.id,
            name=workflow_data["name"],
            description=workflow_data.get("description"),
            workflow_type=workflow_data["workflow_type"],
            schedule=workflow_data["schedule"],
            parameters=workflow_data.get("parameters", {})
        )
        
        await db.automated_workflows.insert_one(workflow.dict())
        
        return workflow
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Create workflow error: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to create workflow")

@api_router.get("/workflows")
async def get_user_workflows(current_user: User = Depends(get_current_user)):
    """Get user's automated workflows"""
    try:
        if not current_user:
            raise HTTPException(status_code=401, detail="Not authenticated")
        
        workflows = await db.automated_workflows.find(
            {"user_id": current_user.id}
        ).sort("created_at", -1).to_list(50)
        
        return {"workflows": workflows}
        
    except Exception as e:
        logger.error(f"Get workflows error: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to get workflows")

@api_router.post("/alerts", response_model=AlertRule)
async def create_alert_rule(alert_data: dict, current_user: User = Depends(get_current_user)):
    """Create alert rule"""
    try:
        if not current_user:
            raise HTTPException(status_code=401, detail="Not authenticated")
        
        if not await require_subscription(current_user, "professional"):
            raise HTTPException(status_code=403, detail="Professional subscription required")
        
        alert = AlertRule(
            user_id=current_user.id,
            name=alert_data["name"],
            description=alert_data.get("description"),
            rule_type=alert_data["rule_type"],
            conditions=alert_data["conditions"],
            notification_channels=alert_data.get("notification_channels", ["email"])
        )
        
        await db.alert_rules.insert_one(alert.dict())
        
        return alert
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Create alert error: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to create alert")

@api_router.get("/dashboard/executive")
async def get_executive_dashboard(current_user: User = Depends(get_current_user)):
    """Get executive dashboard with KPIs and strategic insights"""
    try:
        if not current_user:
            raise HTTPException(status_code=401, detail="Not authenticated")
        
        if not await require_subscription(current_user, "enterprise"):
            raise HTTPException(status_code=403, detail="Enterprise subscription required")
        
        # Get user's analysis summary
        total_analyses = await db.therapy_analyses.count_documents({"user_id": current_user.id})
        rwe_analyses = await db.rwe_analyses.count_documents({"user_id": current_user.id})
        market_access_analyses = await db.market_access_analyses.count_documents({"user_id": current_user.id})
        predictive_analyses = await db.predictive_analyses.count_documents({"user_id": current_user.id})
        
        # Get recent activity
        recent_analyses = await db.therapy_analyses.find(
            {"user_id": current_user.id}
        ).sort("created_at", -1).limit(10).to_list(10)
        
        # Calculate trends (placeholder - would implement proper trending)
        trends = {
            "analyses_growth": 15.2,  # % growth
            "success_rate": 92.5,     # % success rate
            "avg_confidence": 87.3,   # Average confidence score
            "time_savings": 78.9      # % time savings
        }
        
        return {
            "summary": {
                "total_analyses": total_analyses,
                "rwe_analyses": rwe_analyses,
                "market_access_analyses": market_access_analyses,
                "predictive_analyses": predictive_analyses
            },
            "recent_activity": recent_analyses,
            "trends": trends,
            "subscription": {
                "tier": current_user.subscription_tier,
                "status": current_user.subscription_status
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Executive dashboard error: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to get executive dashboard")

# Include the router in the main app
app.include_router(api_router)

app.add_middleware(
    CORSMiddleware,
    allow_credentials=True,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("shutdown")
async def shutdown_db_client():
    client.close()